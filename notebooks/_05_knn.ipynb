{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repository_root_directory:\t /teamspace/studios/this_studio/csc_461_fp\n",
      "repository_root_directory:\t added to path\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "repository_root_directory = os.path.dirname(os.getcwd())\n",
    "rrd = \"repository_root_directory:\\t\"\n",
    "print(rrd, repository_root_directory)\n",
    "\n",
    "if repository_root_directory not in sys.path:\n",
    "    sys.path.append(repository_root_directory)\n",
    "    print(rrd, \"added to path\")\n",
    "else:  \n",
    "    print(rrd, \"already in path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'n_neighbors': 10, 'weights': 'distance'}\n",
      "Best Cross validation training score:  0.5125\n",
      "Test Accuracy:  0.51\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.20      0.33        20\n",
      "           1       0.94      0.85      0.89        20\n",
      "           2       0.50      0.30      0.38        20\n",
      "           3       0.47      0.35      0.40        20\n",
      "           4       0.53      0.40      0.46        20\n",
      "           5       0.33      0.75      0.46        20\n",
      "           6       0.53      0.90      0.67        20\n",
      "           7       0.48      0.70      0.57        20\n",
      "           8       0.44      0.20      0.28        20\n",
      "           9       0.47      0.45      0.46        20\n",
      "\n",
      "    accuracy                           0.51       200\n",
      "   macro avg       0.57      0.51      0.49       200\n",
      "weighted avg       0.57      0.51      0.49       200\n",
      "\n",
      "[[ 4  0  2  1  0  7  2  0  1  3]\n",
      " [ 0 17  0  0  0  2  0  0  1  0]\n",
      " [ 0  0  6  1  0  8  0  2  1  2]\n",
      " [ 0  1  1  7  3  0  5  2  0  1]\n",
      " [ 0  0  1  0  8  1  6  3  1  0]\n",
      " [ 0  0  0  0  0 15  0  2  0  3]\n",
      " [ 0  0  0  0  1  0 18  0  1  0]\n",
      " [ 0  0  0  5  0  1  0 14  0  0]\n",
      " [ 0  0  1  0  3  7  0  4  4  1]\n",
      " [ 0  0  1  1  0  4  3  2  0  9]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    }
   ],
   "source": [
    "# Reload the dataset and redo the optimization process\n",
    "import pandas as pd\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Reload dataset\n",
    "file_path = '../_03_data_gtzan_features_labels_pca.xlsx'\n",
    "dataset = pd.read_excel(file_path)\n",
    "\n",
    "# Extract features and target\n",
    "X = dataset.drop(columns=[\"Genre\"])\n",
    "y = dataset[\"Genre\"]\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define a range of values for k and weights\n",
    "param_grid = {'n_neighbors': range(1, 21), 'weights': ['uniform', 'distance']}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Train optimized KNN model\n",
    "optimized_knn = grid_search.best_estimator_\n",
    "optimized_knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the optimized model\n",
    "y_pred_optimized = optimized_knn.predict(X_test)\n",
    "optimized_accuracy = accuracy_score(y_test, y_pred_optimized)\n",
    "optimized_classification_rep = classification_report(y_test, y_pred_optimized)\n",
    "optimized_conf_matrix = confusion_matrix(y_test, y_pred_optimized)\n",
    "\n",
    "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Optimized Confusion Matrix\", dataframe=pd.DataFrame(optimized_conf_matrix))\n",
    "\n",
    "print(\"Best parameters: \", best_params)\n",
    "print(\"Best Cross validation training score: \", best_score)\n",
    "print(\"Test Accuracy: \", optimized_accuracy) \n",
    "print(optimized_classification_rep)\n",
    "print(optimized_conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10, 'solver': 'saga'}\n",
      "0.6287499999999999\n",
      "0.675\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.75      0.73        20\n",
      "           1       0.86      0.90      0.88        20\n",
      "           2       0.40      0.30      0.34        20\n",
      "           3       0.73      0.55      0.63        20\n",
      "           4       0.77      0.85      0.81        20\n",
      "           5       0.79      0.75      0.77        20\n",
      "           6       0.78      0.90      0.84        20\n",
      "           7       0.59      0.85      0.69        20\n",
      "           8       0.48      0.60      0.53        20\n",
      "           9       0.60      0.30      0.40        20\n",
      "\n",
      "    accuracy                           0.68       200\n",
      "   macro avg       0.67      0.67      0.66       200\n",
      "weighted avg       0.67      0.68      0.66       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength (inverse of regularization parameter)\n",
    "    'solver': ['lbfgs', 'newton-cg', 'saga']  # Different solvers for optimization\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search_logreg = GridSearchCV(\n",
    "    LogisticRegression(multi_class='multinomial', max_iter=1000, random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search_logreg.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and corresponding score\n",
    "best_params_logreg = grid_search_logreg.best_params_\n",
    "best_score_logreg = grid_search_logreg.best_score_\n",
    "\n",
    "# Train the optimized model on the training set\n",
    "optimized_logreg = grid_search_logreg.best_estimator_\n",
    "optimized_logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set with the optimized model\n",
    "y_pred_optimized_logreg = optimized_logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the optimized model\n",
    "optimized_accuracy_logreg = accuracy_score(y_test, y_pred_optimized_logreg)\n",
    "optimized_classification_rep_logreg = classification_report(y_test, y_pred_optimized_logreg)\n",
    "optimized_conf_matrix_logreg = confusion_matrix(y_test, y_pred_optimized_logreg)\n",
    "\n",
    "# tools.display_dataframe_to_user(\n",
    "#     name=\"Optimized Logistic Regression Confusion Matrix\",\n",
    "#     dataframe=pd.DataFrame(optimized_conf_matrix_logreg)\n",
    "# )\n",
    "\n",
    "print(best_params_logreg)\n",
    "print(best_score_logreg)\n",
    "print(optimized_accuracy_logreg)\n",
    "print(optimized_classification_rep_logreg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4000], Loss: 2.2972\n",
      "Epoch [2/4000], Loss: 2.3029\n",
      "Epoch [3/4000], Loss: 2.2803\n",
      "Epoch [4/4000], Loss: 2.2984\n",
      "Epoch [5/4000], Loss: 2.2813\n",
      "Epoch [6/4000], Loss: 2.2784\n",
      "Epoch [7/4000], Loss: 2.2711\n",
      "Epoch [8/4000], Loss: 2.2879\n",
      "Epoch [9/4000], Loss: 2.2675\n",
      "Epoch [10/4000], Loss: 2.2597\n",
      "Epoch [11/4000], Loss: 2.2442\n",
      "Epoch [12/4000], Loss: 2.2488\n",
      "Epoch [13/4000], Loss: 2.2384\n",
      "Epoch [14/4000], Loss: 2.2500\n",
      "Epoch [15/4000], Loss: 2.2504\n",
      "Epoch [16/4000], Loss: 2.2003\n",
      "Epoch [17/4000], Loss: 2.2291\n",
      "Epoch [18/4000], Loss: 2.1909\n",
      "Epoch [19/4000], Loss: 2.2070\n",
      "Epoch [20/4000], Loss: 2.1722\n",
      "Epoch [21/4000], Loss: 2.1860\n",
      "Epoch [22/4000], Loss: 2.1564\n",
      "Epoch [23/4000], Loss: 2.1233\n",
      "Epoch [24/4000], Loss: 2.1871\n",
      "Epoch [25/4000], Loss: 2.0800\n",
      "Epoch [26/4000], Loss: 2.1250\n",
      "Epoch [27/4000], Loss: 2.1343\n",
      "Epoch [28/4000], Loss: 2.0847\n",
      "Epoch [29/4000], Loss: 2.0837\n",
      "Epoch [30/4000], Loss: 2.1375\n",
      "Epoch [31/4000], Loss: 2.0687\n",
      "Epoch [32/4000], Loss: 2.0549\n",
      "Epoch [33/4000], Loss: 2.0638\n",
      "Epoch [34/4000], Loss: 2.0501\n",
      "Epoch [35/4000], Loss: 2.0669\n",
      "Epoch [36/4000], Loss: 2.0804\n",
      "Epoch [37/4000], Loss: 2.0662\n",
      "Epoch [38/4000], Loss: 2.0680\n",
      "Epoch [39/4000], Loss: 2.0679\n",
      "Epoch [40/4000], Loss: 2.0081\n",
      "Epoch [41/4000], Loss: 1.9368\n",
      "Epoch [42/4000], Loss: 1.9830\n",
      "Epoch [43/4000], Loss: 1.9152\n",
      "Epoch [44/4000], Loss: 2.0356\n",
      "Epoch [45/4000], Loss: 1.8175\n",
      "Epoch [46/4000], Loss: 1.9516\n",
      "Epoch [47/4000], Loss: 2.0474\n",
      "Epoch [48/4000], Loss: 1.9073\n",
      "Epoch [49/4000], Loss: 1.8265\n",
      "Epoch [50/4000], Loss: 1.8744\n",
      "Epoch [51/4000], Loss: 1.8899\n",
      "Epoch [52/4000], Loss: 1.9720\n",
      "Epoch [53/4000], Loss: 1.8604\n",
      "Epoch [54/4000], Loss: 1.9375\n",
      "Epoch [55/4000], Loss: 1.8818\n",
      "Epoch [56/4000], Loss: 1.8022\n",
      "Epoch [57/4000], Loss: 1.8394\n",
      "Epoch [58/4000], Loss: 1.8766\n",
      "Epoch [59/4000], Loss: 1.8832\n",
      "Epoch [60/4000], Loss: 1.8057\n",
      "Epoch [61/4000], Loss: 1.8358\n",
      "Epoch [62/4000], Loss: 1.8271\n",
      "Epoch [63/4000], Loss: 1.8407\n",
      "Epoch [64/4000], Loss: 1.7282\n",
      "Epoch [65/4000], Loss: 1.8693\n",
      "Epoch [66/4000], Loss: 1.8266\n",
      "Epoch [67/4000], Loss: 1.8017\n",
      "Epoch [68/4000], Loss: 1.7743\n",
      "Epoch [69/4000], Loss: 1.8340\n",
      "Epoch [70/4000], Loss: 1.8437\n",
      "Epoch [71/4000], Loss: 1.8479\n",
      "Epoch [72/4000], Loss: 1.7034\n",
      "Epoch [73/4000], Loss: 1.7429\n",
      "Epoch [74/4000], Loss: 1.8530\n",
      "Epoch [75/4000], Loss: 1.5500\n",
      "Epoch [76/4000], Loss: 1.8248\n",
      "Epoch [77/4000], Loss: 1.7773\n",
      "Epoch [78/4000], Loss: 1.6361\n",
      "Epoch [79/4000], Loss: 1.6081\n",
      "Epoch [80/4000], Loss: 1.7531\n",
      "Epoch [81/4000], Loss: 1.5738\n",
      "Epoch [82/4000], Loss: 1.5654\n",
      "Epoch [83/4000], Loss: 1.6598\n",
      "Epoch [84/4000], Loss: 1.7133\n",
      "Epoch [85/4000], Loss: 1.7084\n",
      "Epoch [86/4000], Loss: 1.8143\n",
      "Epoch [87/4000], Loss: 1.6604\n",
      "Epoch [88/4000], Loss: 1.7088\n",
      "Epoch [89/4000], Loss: 1.5997\n",
      "Epoch [90/4000], Loss: 1.8601\n",
      "Epoch [91/4000], Loss: 1.5332\n",
      "Epoch [92/4000], Loss: 1.4173\n",
      "Epoch [93/4000], Loss: 1.7771\n",
      "Epoch [94/4000], Loss: 1.5778\n",
      "Epoch [95/4000], Loss: 1.6201\n",
      "Epoch [96/4000], Loss: 1.6006\n",
      "Epoch [97/4000], Loss: 1.7751\n",
      "Epoch [98/4000], Loss: 1.6409\n",
      "Epoch [99/4000], Loss: 1.4924\n",
      "Epoch [100/4000], Loss: 1.7524\n",
      "Epoch [101/4000], Loss: 1.6516\n",
      "Epoch [102/4000], Loss: 1.4723\n",
      "Epoch [103/4000], Loss: 1.4359\n",
      "Epoch [104/4000], Loss: 1.6207\n",
      "Epoch [105/4000], Loss: 1.5116\n",
      "Epoch [106/4000], Loss: 1.3304\n",
      "Epoch [107/4000], Loss: 1.5138\n",
      "Epoch [108/4000], Loss: 1.5877\n",
      "Epoch [109/4000], Loss: 1.6202\n",
      "Epoch [110/4000], Loss: 1.6950\n",
      "Epoch [111/4000], Loss: 1.6107\n",
      "Epoch [112/4000], Loss: 1.5601\n",
      "Epoch [113/4000], Loss: 1.6204\n",
      "Epoch [114/4000], Loss: 1.5994\n",
      "Epoch [115/4000], Loss: 1.5766\n",
      "Epoch [116/4000], Loss: 1.5483\n",
      "Epoch [117/4000], Loss: 1.6358\n",
      "Epoch [118/4000], Loss: 1.4146\n",
      "Epoch [119/4000], Loss: 1.4098\n",
      "Epoch [120/4000], Loss: 1.6889\n",
      "Epoch [121/4000], Loss: 1.5202\n",
      "Epoch [122/4000], Loss: 1.5778\n",
      "Epoch [123/4000], Loss: 1.4596\n",
      "Epoch [124/4000], Loss: 1.4200\n",
      "Epoch [125/4000], Loss: 1.5254\n",
      "Epoch [126/4000], Loss: 1.4478\n",
      "Epoch [127/4000], Loss: 1.5431\n",
      "Epoch [128/4000], Loss: 1.4744\n",
      "Epoch [129/4000], Loss: 1.3745\n",
      "Epoch [130/4000], Loss: 1.3479\n",
      "Epoch [131/4000], Loss: 1.4135\n",
      "Epoch [132/4000], Loss: 1.4525\n",
      "Epoch [133/4000], Loss: 1.5145\n",
      "Epoch [134/4000], Loss: 1.4840\n",
      "Epoch [135/4000], Loss: 1.4840\n",
      "Epoch [136/4000], Loss: 1.4534\n",
      "Epoch [137/4000], Loss: 1.5700\n",
      "Epoch [138/4000], Loss: 1.5716\n",
      "Epoch [139/4000], Loss: 1.4398\n",
      "Epoch [140/4000], Loss: 1.5952\n",
      "Epoch [141/4000], Loss: 1.4513\n",
      "Epoch [142/4000], Loss: 1.5008\n",
      "Epoch [143/4000], Loss: 1.4235\n",
      "Epoch [144/4000], Loss: 1.6472\n",
      "Epoch [145/4000], Loss: 1.5869\n",
      "Epoch [146/4000], Loss: 1.4058\n",
      "Epoch [147/4000], Loss: 1.3835\n",
      "Epoch [148/4000], Loss: 1.4753\n",
      "Epoch [149/4000], Loss: 1.4807\n",
      "Epoch [150/4000], Loss: 1.2802\n",
      "Epoch [151/4000], Loss: 1.4481\n",
      "Epoch [152/4000], Loss: 1.3554\n",
      "Epoch [153/4000], Loss: 1.2725\n",
      "Epoch [154/4000], Loss: 1.6318\n",
      "Epoch [155/4000], Loss: 1.3126\n",
      "Epoch [156/4000], Loss: 1.5039\n",
      "Epoch [157/4000], Loss: 1.4084\n",
      "Epoch [158/4000], Loss: 1.3567\n",
      "Epoch [159/4000], Loss: 1.5589\n",
      "Epoch [160/4000], Loss: 1.1972\n",
      "Epoch [161/4000], Loss: 1.3199\n",
      "Epoch [162/4000], Loss: 1.5885\n",
      "Epoch [163/4000], Loss: 1.4559\n",
      "Epoch [164/4000], Loss: 1.3945\n",
      "Epoch [165/4000], Loss: 1.3488\n",
      "Epoch [166/4000], Loss: 1.3182\n",
      "Epoch [167/4000], Loss: 1.4267\n",
      "Epoch [168/4000], Loss: 1.4187\n",
      "Epoch [169/4000], Loss: 1.7831\n",
      "Epoch [170/4000], Loss: 1.2967\n",
      "Epoch [171/4000], Loss: 1.3051\n",
      "Epoch [172/4000], Loss: 1.3950\n",
      "Epoch [173/4000], Loss: 1.4148\n",
      "Epoch [174/4000], Loss: 1.3652\n",
      "Epoch [175/4000], Loss: 1.5619\n",
      "Epoch [176/4000], Loss: 1.4704\n",
      "Epoch [177/4000], Loss: 1.3572\n",
      "Epoch [178/4000], Loss: 1.5027\n",
      "Epoch [179/4000], Loss: 1.4194\n",
      "Epoch [180/4000], Loss: 1.3784\n",
      "Epoch [181/4000], Loss: 1.0504\n",
      "Epoch [182/4000], Loss: 1.3047\n",
      "Epoch [183/4000], Loss: 1.3221\n",
      "Epoch [184/4000], Loss: 1.3788\n",
      "Epoch [185/4000], Loss: 1.4678\n",
      "Epoch [186/4000], Loss: 1.2921\n",
      "Epoch [187/4000], Loss: 1.2688\n",
      "Epoch [188/4000], Loss: 1.2220\n",
      "Epoch [189/4000], Loss: 1.2371\n",
      "Epoch [190/4000], Loss: 1.2071\n",
      "Epoch [191/4000], Loss: 1.1314\n",
      "Epoch [192/4000], Loss: 1.2836\n",
      "Epoch [193/4000], Loss: 1.3215\n",
      "Epoch [194/4000], Loss: 1.6191\n",
      "Epoch [195/4000], Loss: 1.1884\n",
      "Epoch [196/4000], Loss: 1.5546\n",
      "Epoch [197/4000], Loss: 1.3074\n",
      "Epoch [198/4000], Loss: 1.2890\n",
      "Epoch [199/4000], Loss: 1.2921\n",
      "Epoch [200/4000], Loss: 1.3581\n",
      "Epoch [201/4000], Loss: 1.2048\n",
      "Epoch [202/4000], Loss: 1.2871\n",
      "Epoch [203/4000], Loss: 1.4161\n",
      "Epoch [204/4000], Loss: 1.5136\n",
      "Epoch [205/4000], Loss: 1.2498\n",
      "Epoch [206/4000], Loss: 1.3357\n",
      "Epoch [207/4000], Loss: 1.3983\n",
      "Epoch [208/4000], Loss: 1.1606\n",
      "Epoch [209/4000], Loss: 1.3920\n",
      "Epoch [210/4000], Loss: 1.1763\n",
      "Epoch [211/4000], Loss: 1.3873\n",
      "Epoch [212/4000], Loss: 1.1940\n",
      "Epoch [213/4000], Loss: 1.3729\n",
      "Epoch [214/4000], Loss: 1.2364\n",
      "Epoch [215/4000], Loss: 1.3396\n",
      "Epoch [216/4000], Loss: 1.2313\n",
      "Epoch [217/4000], Loss: 1.4324\n",
      "Epoch [218/4000], Loss: 1.2550\n",
      "Epoch [219/4000], Loss: 1.2945\n",
      "Epoch [220/4000], Loss: 1.1941\n",
      "Epoch [221/4000], Loss: 1.1514\n",
      "Epoch [222/4000], Loss: 1.3325\n",
      "Epoch [223/4000], Loss: 1.0579\n",
      "Epoch [224/4000], Loss: 1.4947\n",
      "Epoch [225/4000], Loss: 1.4153\n",
      "Epoch [226/4000], Loss: 1.2987\n",
      "Epoch [227/4000], Loss: 1.3071\n",
      "Epoch [228/4000], Loss: 1.2532\n",
      "Epoch [229/4000], Loss: 1.6206\n",
      "Epoch [230/4000], Loss: 1.3031\n",
      "Epoch [231/4000], Loss: 1.3419\n",
      "Epoch [232/4000], Loss: 1.1408\n",
      "Epoch [233/4000], Loss: 1.3178\n",
      "Epoch [234/4000], Loss: 1.4550\n",
      "Epoch [235/4000], Loss: 1.3010\n",
      "Epoch [236/4000], Loss: 1.0763\n",
      "Epoch [237/4000], Loss: 1.1796\n",
      "Epoch [238/4000], Loss: 1.1867\n",
      "Epoch [239/4000], Loss: 1.2541\n",
      "Epoch [240/4000], Loss: 1.2310\n",
      "Epoch [241/4000], Loss: 1.2512\n",
      "Epoch [242/4000], Loss: 1.4440\n",
      "Epoch [243/4000], Loss: 1.4167\n",
      "Epoch [244/4000], Loss: 1.3900\n",
      "Epoch [245/4000], Loss: 1.3898\n",
      "Epoch [246/4000], Loss: 1.3431\n",
      "Epoch [247/4000], Loss: 1.2597\n",
      "Epoch [248/4000], Loss: 1.3819\n",
      "Epoch [249/4000], Loss: 1.2301\n",
      "Epoch [250/4000], Loss: 1.1848\n",
      "Epoch [251/4000], Loss: 1.2181\n",
      "Epoch [252/4000], Loss: 1.3275\n",
      "Epoch [253/4000], Loss: 1.1291\n",
      "Epoch [254/4000], Loss: 1.3379\n",
      "Epoch [255/4000], Loss: 1.2750\n",
      "Epoch [256/4000], Loss: 1.3687\n",
      "Epoch [257/4000], Loss: 1.1280\n",
      "Epoch [258/4000], Loss: 1.2539\n",
      "Epoch [259/4000], Loss: 1.3176\n",
      "Epoch [260/4000], Loss: 1.2380\n",
      "Epoch [261/4000], Loss: 1.6203\n",
      "Epoch [262/4000], Loss: 1.2856\n",
      "Epoch [263/4000], Loss: 1.3779\n",
      "Epoch [264/4000], Loss: 1.3803\n",
      "Epoch [265/4000], Loss: 1.3323\n",
      "Epoch [266/4000], Loss: 1.5147\n",
      "Epoch [267/4000], Loss: 1.2081\n",
      "Epoch [268/4000], Loss: 1.3982\n",
      "Epoch [269/4000], Loss: 1.1446\n",
      "Epoch [270/4000], Loss: 1.1361\n",
      "Epoch [271/4000], Loss: 1.1682\n",
      "Epoch [272/4000], Loss: 1.4002\n",
      "Epoch [273/4000], Loss: 1.2525\n",
      "Epoch [274/4000], Loss: 1.2735\n",
      "Epoch [275/4000], Loss: 1.2727\n",
      "Epoch [276/4000], Loss: 1.2225\n",
      "Epoch [277/4000], Loss: 1.5977\n",
      "Epoch [278/4000], Loss: 1.2168\n",
      "Epoch [279/4000], Loss: 1.2268\n",
      "Epoch [280/4000], Loss: 1.1698\n",
      "Epoch [281/4000], Loss: 1.0564\n",
      "Epoch [282/4000], Loss: 1.1328\n",
      "Epoch [283/4000], Loss: 1.0601\n",
      "Epoch [284/4000], Loss: 1.0625\n",
      "Epoch [285/4000], Loss: 1.2104\n",
      "Epoch [286/4000], Loss: 1.1212\n",
      "Epoch [287/4000], Loss: 1.4529\n",
      "Epoch [288/4000], Loss: 1.0182\n",
      "Epoch [289/4000], Loss: 1.1597\n",
      "Epoch [290/4000], Loss: 0.9584\n",
      "Epoch [291/4000], Loss: 1.0808\n",
      "Epoch [292/4000], Loss: 1.1454\n",
      "Epoch [293/4000], Loss: 1.3835\n",
      "Epoch [294/4000], Loss: 1.2144\n",
      "Epoch [295/4000], Loss: 1.4266\n",
      "Epoch [296/4000], Loss: 1.2559\n",
      "Epoch [297/4000], Loss: 0.8833\n",
      "Epoch [298/4000], Loss: 1.2842\n",
      "Epoch [299/4000], Loss: 1.2135\n",
      "Epoch [300/4000], Loss: 1.1630\n",
      "Epoch [301/4000], Loss: 1.1722\n",
      "Epoch [302/4000], Loss: 1.5962\n",
      "Epoch [303/4000], Loss: 1.2389\n",
      "Epoch [304/4000], Loss: 1.2789\n",
      "Epoch [305/4000], Loss: 1.0656\n",
      "Epoch [306/4000], Loss: 0.9951\n",
      "Epoch [307/4000], Loss: 1.0803\n",
      "Epoch [308/4000], Loss: 1.1921\n",
      "Epoch [309/4000], Loss: 1.0575\n",
      "Epoch [310/4000], Loss: 1.0959\n",
      "Epoch [311/4000], Loss: 1.2363\n",
      "Epoch [312/4000], Loss: 1.2518\n",
      "Epoch [313/4000], Loss: 1.2197\n",
      "Epoch [314/4000], Loss: 1.2839\n",
      "Epoch [315/4000], Loss: 1.5035\n",
      "Epoch [316/4000], Loss: 1.4943\n",
      "Epoch [317/4000], Loss: 1.1932\n",
      "Epoch [318/4000], Loss: 1.2415\n",
      "Epoch [319/4000], Loss: 0.9883\n",
      "Epoch [320/4000], Loss: 1.3818\n",
      "Epoch [321/4000], Loss: 1.2400\n",
      "Epoch [322/4000], Loss: 1.1125\n",
      "Epoch [323/4000], Loss: 1.0886\n",
      "Epoch [324/4000], Loss: 0.9643\n",
      "Epoch [325/4000], Loss: 1.0819\n",
      "Epoch [326/4000], Loss: 1.2354\n",
      "Epoch [327/4000], Loss: 1.6183\n",
      "Epoch [328/4000], Loss: 1.1660\n",
      "Epoch [329/4000], Loss: 1.1446\n",
      "Epoch [330/4000], Loss: 1.0181\n",
      "Epoch [331/4000], Loss: 1.0542\n",
      "Epoch [332/4000], Loss: 1.2286\n",
      "Epoch [333/4000], Loss: 1.2628\n",
      "Epoch [334/4000], Loss: 1.0634\n",
      "Epoch [335/4000], Loss: 1.3337\n",
      "Epoch [336/4000], Loss: 1.2697\n",
      "Epoch [337/4000], Loss: 1.0826\n",
      "Epoch [338/4000], Loss: 1.2384\n",
      "Epoch [339/4000], Loss: 1.1276\n",
      "Epoch [340/4000], Loss: 1.0668\n",
      "Epoch [341/4000], Loss: 1.1623\n",
      "Epoch [342/4000], Loss: 1.1730\n",
      "Epoch [343/4000], Loss: 1.0204\n",
      "Epoch [344/4000], Loss: 1.0485\n",
      "Epoch [345/4000], Loss: 1.1585\n",
      "Epoch [346/4000], Loss: 0.9571\n",
      "Epoch [347/4000], Loss: 1.1516\n",
      "Epoch [348/4000], Loss: 1.0417\n",
      "Epoch [349/4000], Loss: 1.1411\n",
      "Epoch [350/4000], Loss: 1.0914\n",
      "Epoch [351/4000], Loss: 1.3297\n",
      "Epoch [352/4000], Loss: 1.0030\n",
      "Epoch [353/4000], Loss: 1.2820\n",
      "Epoch [354/4000], Loss: 1.2817\n",
      "Epoch [355/4000], Loss: 1.3711\n",
      "Epoch [356/4000], Loss: 1.0757\n",
      "Epoch [357/4000], Loss: 1.0236\n",
      "Epoch [358/4000], Loss: 1.2212\n",
      "Epoch [359/4000], Loss: 0.8884\n",
      "Epoch [360/4000], Loss: 0.9141\n",
      "Epoch [361/4000], Loss: 1.4337\n",
      "Epoch [362/4000], Loss: 1.1806\n",
      "Epoch [363/4000], Loss: 1.2506\n",
      "Epoch [364/4000], Loss: 0.9783\n",
      "Epoch [365/4000], Loss: 1.4069\n",
      "Epoch [366/4000], Loss: 1.3551\n",
      "Epoch [367/4000], Loss: 1.2849\n",
      "Epoch [368/4000], Loss: 1.3368\n",
      "Epoch [369/4000], Loss: 1.1506\n",
      "Epoch [370/4000], Loss: 1.1728\n",
      "Epoch [371/4000], Loss: 1.1196\n",
      "Epoch [372/4000], Loss: 1.2190\n",
      "Epoch [373/4000], Loss: 1.1101\n",
      "Epoch [374/4000], Loss: 1.0595\n",
      "Epoch [375/4000], Loss: 1.1080\n",
      "Epoch [376/4000], Loss: 1.2390\n",
      "Epoch [377/4000], Loss: 1.1147\n",
      "Epoch [378/4000], Loss: 1.1757\n",
      "Epoch [379/4000], Loss: 1.2073\n",
      "Epoch [380/4000], Loss: 1.0060\n",
      "Epoch [381/4000], Loss: 1.1402\n",
      "Epoch [382/4000], Loss: 1.0516\n",
      "Epoch [383/4000], Loss: 1.2526\n",
      "Epoch [384/4000], Loss: 1.1064\n",
      "Epoch [385/4000], Loss: 1.1379\n",
      "Epoch [386/4000], Loss: 1.0495\n",
      "Epoch [387/4000], Loss: 1.2663\n",
      "Epoch [388/4000], Loss: 1.1942\n",
      "Epoch [389/4000], Loss: 1.0081\n",
      "Epoch [390/4000], Loss: 0.9398\n",
      "Epoch [391/4000], Loss: 1.0464\n",
      "Epoch [392/4000], Loss: 1.1370\n",
      "Epoch [393/4000], Loss: 1.5312\n",
      "Epoch [394/4000], Loss: 1.0551\n",
      "Epoch [395/4000], Loss: 0.9977\n",
      "Epoch [396/4000], Loss: 0.9278\n",
      "Epoch [397/4000], Loss: 1.0821\n",
      "Epoch [398/4000], Loss: 1.1818\n",
      "Epoch [399/4000], Loss: 1.3831\n",
      "Epoch [400/4000], Loss: 0.9231\n",
      "Epoch [401/4000], Loss: 1.1610\n",
      "Epoch [402/4000], Loss: 1.1325\n",
      "Epoch [403/4000], Loss: 1.2265\n",
      "Epoch [404/4000], Loss: 0.9152\n",
      "Epoch [405/4000], Loss: 1.2519\n",
      "Epoch [406/4000], Loss: 0.8220\n",
      "Epoch [407/4000], Loss: 1.1899\n",
      "Epoch [408/4000], Loss: 0.9256\n",
      "Epoch [409/4000], Loss: 0.8448\n",
      "Epoch [410/4000], Loss: 0.8919\n",
      "Epoch [411/4000], Loss: 0.9752\n",
      "Epoch [412/4000], Loss: 1.0978\n",
      "Epoch [413/4000], Loss: 0.9956\n",
      "Epoch [414/4000], Loss: 1.1148\n",
      "Epoch [415/4000], Loss: 0.9145\n",
      "Epoch [416/4000], Loss: 1.2096\n",
      "Epoch [417/4000], Loss: 1.0405\n",
      "Epoch [418/4000], Loss: 1.0708\n",
      "Epoch [419/4000], Loss: 1.2868\n",
      "Epoch [420/4000], Loss: 1.0359\n",
      "Epoch [421/4000], Loss: 1.3082\n",
      "Epoch [422/4000], Loss: 1.0970\n",
      "Epoch [423/4000], Loss: 1.1112\n",
      "Epoch [424/4000], Loss: 1.0296\n",
      "Epoch [425/4000], Loss: 0.9697\n",
      "Epoch [426/4000], Loss: 0.9528\n",
      "Epoch [427/4000], Loss: 0.9699\n",
      "Epoch [428/4000], Loss: 1.0647\n",
      "Epoch [429/4000], Loss: 0.9657\n",
      "Epoch [430/4000], Loss: 1.0278\n",
      "Epoch [431/4000], Loss: 1.0839\n",
      "Epoch [432/4000], Loss: 0.9988\n",
      "Epoch [433/4000], Loss: 1.0088\n",
      "Epoch [434/4000], Loss: 1.1770\n",
      "Epoch [435/4000], Loss: 1.2614\n",
      "Epoch [436/4000], Loss: 1.1137\n",
      "Epoch [437/4000], Loss: 0.9328\n",
      "Epoch [438/4000], Loss: 1.1867\n",
      "Epoch [439/4000], Loss: 1.2086\n",
      "Epoch [440/4000], Loss: 0.9764\n",
      "Epoch [441/4000], Loss: 1.2505\n",
      "Epoch [442/4000], Loss: 1.1630\n",
      "Epoch [443/4000], Loss: 0.9988\n",
      "Epoch [444/4000], Loss: 1.0795\n",
      "Epoch [445/4000], Loss: 0.9637\n",
      "Epoch [446/4000], Loss: 1.2381\n",
      "Epoch [447/4000], Loss: 1.0728\n",
      "Epoch [448/4000], Loss: 1.0805\n",
      "Epoch [449/4000], Loss: 1.0965\n",
      "Epoch [450/4000], Loss: 1.1012\n",
      "Epoch [451/4000], Loss: 1.0277\n",
      "Epoch [452/4000], Loss: 0.9701\n",
      "Epoch [453/4000], Loss: 0.9801\n",
      "Epoch [454/4000], Loss: 0.9100\n",
      "Epoch [455/4000], Loss: 1.0193\n",
      "Epoch [456/4000], Loss: 1.1625\n",
      "Epoch [457/4000], Loss: 1.1813\n",
      "Epoch [458/4000], Loss: 0.8553\n",
      "Epoch [459/4000], Loss: 1.3381\n",
      "Epoch [460/4000], Loss: 1.0476\n",
      "Epoch [461/4000], Loss: 0.9570\n",
      "Epoch [462/4000], Loss: 1.1302\n",
      "Epoch [463/4000], Loss: 1.0807\n",
      "Epoch [464/4000], Loss: 1.3126\n",
      "Epoch [465/4000], Loss: 1.0084\n",
      "Epoch [466/4000], Loss: 0.9279\n",
      "Epoch [467/4000], Loss: 1.2213\n",
      "Epoch [468/4000], Loss: 1.0307\n",
      "Epoch [469/4000], Loss: 0.8565\n",
      "Epoch [470/4000], Loss: 0.9279\n",
      "Epoch [471/4000], Loss: 1.0135\n",
      "Epoch [472/4000], Loss: 1.0271\n",
      "Epoch [473/4000], Loss: 0.9541\n",
      "Epoch [474/4000], Loss: 0.9853\n",
      "Epoch [475/4000], Loss: 1.1457\n",
      "Epoch [476/4000], Loss: 1.1843\n",
      "Epoch [477/4000], Loss: 1.3347\n",
      "Epoch [478/4000], Loss: 1.0305\n",
      "Epoch [479/4000], Loss: 0.9350\n",
      "Epoch [480/4000], Loss: 0.8520\n",
      "Epoch [481/4000], Loss: 0.9204\n",
      "Epoch [482/4000], Loss: 1.0011\n",
      "Epoch [483/4000], Loss: 0.8875\n",
      "Epoch [484/4000], Loss: 0.9747\n",
      "Epoch [485/4000], Loss: 0.9418\n",
      "Epoch [486/4000], Loss: 1.2770\n",
      "Epoch [487/4000], Loss: 1.0931\n",
      "Epoch [488/4000], Loss: 0.9525\n",
      "Epoch [489/4000], Loss: 1.0310\n",
      "Epoch [490/4000], Loss: 1.2445\n",
      "Epoch [491/4000], Loss: 1.0431\n",
      "Epoch [492/4000], Loss: 1.1430\n",
      "Epoch [493/4000], Loss: 1.1353\n",
      "Epoch [494/4000], Loss: 1.1278\n",
      "Epoch [495/4000], Loss: 1.0357\n",
      "Epoch [496/4000], Loss: 1.1133\n",
      "Epoch [497/4000], Loss: 1.0729\n",
      "Epoch [498/4000], Loss: 0.9244\n",
      "Epoch [499/4000], Loss: 1.0166\n",
      "Epoch [500/4000], Loss: 1.0164\n",
      "Epoch [501/4000], Loss: 1.1713\n",
      "Epoch [502/4000], Loss: 1.2496\n",
      "Epoch [503/4000], Loss: 0.8734\n",
      "Epoch [504/4000], Loss: 1.0666\n",
      "Epoch [505/4000], Loss: 1.1311\n",
      "Epoch [506/4000], Loss: 0.8669\n",
      "Epoch [507/4000], Loss: 1.1888\n",
      "Epoch [508/4000], Loss: 0.9666\n",
      "Epoch [509/4000], Loss: 0.9199\n",
      "Epoch [510/4000], Loss: 1.0612\n",
      "Epoch [511/4000], Loss: 0.9910\n",
      "Epoch [512/4000], Loss: 1.1503\n",
      "Epoch [513/4000], Loss: 1.1191\n",
      "Epoch [514/4000], Loss: 0.9231\n",
      "Epoch [515/4000], Loss: 0.9226\n",
      "Epoch [516/4000], Loss: 1.2014\n",
      "Epoch [517/4000], Loss: 0.8851\n",
      "Epoch [518/4000], Loss: 1.0275\n",
      "Epoch [519/4000], Loss: 1.2826\n",
      "Epoch [520/4000], Loss: 0.9219\n",
      "Epoch [521/4000], Loss: 1.0704\n",
      "Epoch [522/4000], Loss: 0.8478\n",
      "Epoch [523/4000], Loss: 0.9826\n",
      "Epoch [524/4000], Loss: 0.8543\n",
      "Epoch [525/4000], Loss: 0.8436\n",
      "Epoch [526/4000], Loss: 0.9124\n",
      "Epoch [527/4000], Loss: 0.9714\n",
      "Epoch [528/4000], Loss: 0.9753\n",
      "Epoch [529/4000], Loss: 1.0623\n",
      "Epoch [530/4000], Loss: 0.7491\n",
      "Epoch [531/4000], Loss: 1.1675\n",
      "Epoch [532/4000], Loss: 1.0771\n",
      "Epoch [533/4000], Loss: 1.0647\n",
      "Epoch [534/4000], Loss: 1.1235\n",
      "Epoch [535/4000], Loss: 0.9622\n",
      "Epoch [536/4000], Loss: 0.9443\n",
      "Epoch [537/4000], Loss: 1.1293\n",
      "Epoch [538/4000], Loss: 0.9703\n",
      "Epoch [539/4000], Loss: 1.1101\n",
      "Epoch [540/4000], Loss: 0.7975\n",
      "Epoch [541/4000], Loss: 1.0360\n",
      "Epoch [542/4000], Loss: 1.1373\n",
      "Epoch [543/4000], Loss: 1.0405\n",
      "Epoch [544/4000], Loss: 1.0334\n",
      "Epoch [545/4000], Loss: 0.9219\n",
      "Epoch [546/4000], Loss: 0.7960\n",
      "Epoch [547/4000], Loss: 0.9327\n",
      "Epoch [548/4000], Loss: 0.9880\n",
      "Epoch [549/4000], Loss: 1.0379\n",
      "Epoch [550/4000], Loss: 1.1694\n",
      "Epoch [551/4000], Loss: 1.0019\n",
      "Epoch [552/4000], Loss: 1.0594\n",
      "Epoch [553/4000], Loss: 0.9491\n",
      "Epoch [554/4000], Loss: 0.9997\n",
      "Epoch [555/4000], Loss: 1.0341\n",
      "Epoch [556/4000], Loss: 0.8550\n",
      "Epoch [557/4000], Loss: 1.0503\n",
      "Epoch [558/4000], Loss: 1.0477\n",
      "Epoch [559/4000], Loss: 0.8907\n",
      "Epoch [560/4000], Loss: 0.7995\n",
      "Epoch [561/4000], Loss: 0.7804\n",
      "Epoch [562/4000], Loss: 1.0502\n",
      "Epoch [563/4000], Loss: 0.9071\n",
      "Epoch [564/4000], Loss: 1.1568\n",
      "Epoch [565/4000], Loss: 1.0155\n",
      "Epoch [566/4000], Loss: 0.7637\n",
      "Epoch [567/4000], Loss: 0.9911\n",
      "Epoch [568/4000], Loss: 0.7365\n",
      "Epoch [569/4000], Loss: 1.1297\n",
      "Epoch [570/4000], Loss: 1.0125\n",
      "Epoch [571/4000], Loss: 1.0154\n",
      "Epoch [572/4000], Loss: 1.0971\n",
      "Epoch [573/4000], Loss: 0.9088\n",
      "Epoch [574/4000], Loss: 1.0497\n",
      "Epoch [575/4000], Loss: 1.0684\n",
      "Epoch [576/4000], Loss: 0.9171\n",
      "Epoch [577/4000], Loss: 1.1215\n",
      "Epoch [578/4000], Loss: 0.7259\n",
      "Epoch [579/4000], Loss: 0.9520\n",
      "Epoch [580/4000], Loss: 1.0206\n",
      "Epoch [581/4000], Loss: 0.9070\n",
      "Epoch [582/4000], Loss: 0.7666\n",
      "Epoch [583/4000], Loss: 0.9543\n",
      "Epoch [584/4000], Loss: 0.9912\n",
      "Epoch [585/4000], Loss: 0.9460\n",
      "Epoch [586/4000], Loss: 0.7023\n",
      "Epoch [587/4000], Loss: 0.7952\n",
      "Epoch [588/4000], Loss: 1.1317\n",
      "Epoch [589/4000], Loss: 0.8796\n",
      "Epoch [590/4000], Loss: 0.8666\n",
      "Epoch [591/4000], Loss: 0.7523\n",
      "Epoch [592/4000], Loss: 0.9639\n",
      "Epoch [593/4000], Loss: 1.2134\n",
      "Epoch [594/4000], Loss: 0.9551\n",
      "Epoch [595/4000], Loss: 1.0804\n",
      "Epoch [596/4000], Loss: 0.7720\n",
      "Epoch [597/4000], Loss: 1.0814\n",
      "Epoch [598/4000], Loss: 0.9615\n",
      "Epoch [599/4000], Loss: 0.8405\n",
      "Epoch [600/4000], Loss: 0.9511\n",
      "Epoch [601/4000], Loss: 0.8254\n",
      "Epoch [602/4000], Loss: 1.0484\n",
      "Epoch [603/4000], Loss: 1.0485\n",
      "Epoch [604/4000], Loss: 1.0057\n",
      "Epoch [605/4000], Loss: 0.7636\n",
      "Epoch [606/4000], Loss: 0.9681\n",
      "Epoch [607/4000], Loss: 0.6173\n",
      "Epoch [608/4000], Loss: 0.8730\n",
      "Epoch [609/4000], Loss: 0.9694\n",
      "Epoch [610/4000], Loss: 1.0259\n",
      "Epoch [611/4000], Loss: 1.0002\n",
      "Epoch [612/4000], Loss: 0.9029\n",
      "Epoch [613/4000], Loss: 0.9251\n",
      "Epoch [614/4000], Loss: 0.8509\n",
      "Epoch [615/4000], Loss: 0.8624\n",
      "Epoch [616/4000], Loss: 0.6855\n",
      "Epoch [617/4000], Loss: 0.8233\n",
      "Epoch [618/4000], Loss: 0.8951\n",
      "Epoch [619/4000], Loss: 0.8917\n",
      "Epoch [620/4000], Loss: 0.8345\n",
      "Epoch [621/4000], Loss: 0.7745\n",
      "Epoch [622/4000], Loss: 0.6775\n",
      "Epoch [623/4000], Loss: 0.9234\n",
      "Epoch [624/4000], Loss: 0.8752\n",
      "Epoch [625/4000], Loss: 0.9016\n",
      "Epoch [626/4000], Loss: 0.8912\n",
      "Epoch [627/4000], Loss: 0.8367\n",
      "Epoch [628/4000], Loss: 1.1053\n",
      "Epoch [629/4000], Loss: 0.7117\n",
      "Epoch [630/4000], Loss: 0.8907\n",
      "Epoch [631/4000], Loss: 0.8775\n",
      "Epoch [632/4000], Loss: 1.0085\n",
      "Epoch [633/4000], Loss: 0.8550\n",
      "Epoch [634/4000], Loss: 0.9685\n",
      "Epoch [635/4000], Loss: 1.0082\n",
      "Epoch [636/4000], Loss: 0.9559\n",
      "Epoch [637/4000], Loss: 0.8313\n",
      "Epoch [638/4000], Loss: 1.0049\n",
      "Epoch [639/4000], Loss: 0.8177\n",
      "Epoch [640/4000], Loss: 0.9524\n",
      "Epoch [641/4000], Loss: 0.7611\n",
      "Epoch [642/4000], Loss: 0.7129\n",
      "Epoch [643/4000], Loss: 0.8779\n",
      "Epoch [644/4000], Loss: 0.9797\n",
      "Epoch [645/4000], Loss: 0.9088\n",
      "Epoch [646/4000], Loss: 0.8703\n",
      "Epoch [647/4000], Loss: 0.8323\n",
      "Epoch [648/4000], Loss: 0.7627\n",
      "Epoch [649/4000], Loss: 0.9798\n",
      "Epoch [650/4000], Loss: 0.8169\n",
      "Epoch [651/4000], Loss: 0.7541\n",
      "Epoch [652/4000], Loss: 1.1564\n",
      "Epoch [653/4000], Loss: 0.8477\n",
      "Epoch [654/4000], Loss: 0.8933\n",
      "Epoch [655/4000], Loss: 1.0350\n",
      "Epoch [656/4000], Loss: 0.9581\n",
      "Epoch [657/4000], Loss: 0.7043\n",
      "Epoch [658/4000], Loss: 0.8337\n",
      "Epoch [659/4000], Loss: 1.2214\n",
      "Epoch [660/4000], Loss: 0.7908\n",
      "Epoch [661/4000], Loss: 0.7054\n",
      "Epoch [662/4000], Loss: 1.0038\n",
      "Epoch [663/4000], Loss: 0.8240\n",
      "Epoch [664/4000], Loss: 1.0207\n",
      "Epoch [665/4000], Loss: 0.6577\n",
      "Epoch [666/4000], Loss: 0.5828\n",
      "Epoch [667/4000], Loss: 0.8498\n",
      "Epoch [668/4000], Loss: 0.7632\n",
      "Epoch [669/4000], Loss: 0.8236\n",
      "Epoch [670/4000], Loss: 0.8326\n",
      "Epoch [671/4000], Loss: 0.9235\n",
      "Epoch [672/4000], Loss: 0.9775\n",
      "Epoch [673/4000], Loss: 1.0070\n",
      "Epoch [674/4000], Loss: 0.9041\n",
      "Epoch [675/4000], Loss: 0.7177\n",
      "Epoch [676/4000], Loss: 0.7998\n",
      "Epoch [677/4000], Loss: 1.0100\n",
      "Epoch [678/4000], Loss: 0.6848\n",
      "Epoch [679/4000], Loss: 0.8611\n",
      "Epoch [680/4000], Loss: 1.0762\n",
      "Epoch [681/4000], Loss: 0.9712\n",
      "Epoch [682/4000], Loss: 0.9732\n",
      "Epoch [683/4000], Loss: 0.7800\n",
      "Epoch [684/4000], Loss: 0.7298\n",
      "Epoch [685/4000], Loss: 1.0216\n",
      "Epoch [686/4000], Loss: 0.8320\n",
      "Epoch [687/4000], Loss: 0.8576\n",
      "Epoch [688/4000], Loss: 0.8222\n",
      "Epoch [689/4000], Loss: 1.1257\n",
      "Epoch [690/4000], Loss: 0.7664\n",
      "Epoch [691/4000], Loss: 1.0551\n",
      "Epoch [692/4000], Loss: 0.9655\n",
      "Epoch [693/4000], Loss: 0.8334\n",
      "Epoch [694/4000], Loss: 0.9365\n",
      "Epoch [695/4000], Loss: 0.7905\n",
      "Epoch [696/4000], Loss: 0.8748\n",
      "Epoch [697/4000], Loss: 0.8923\n",
      "Epoch [698/4000], Loss: 0.8288\n",
      "Epoch [699/4000], Loss: 0.8139\n",
      "Epoch [700/4000], Loss: 0.9702\n",
      "Epoch [701/4000], Loss: 0.9832\n",
      "Epoch [702/4000], Loss: 0.7045\n",
      "Epoch [703/4000], Loss: 0.6451\n",
      "Epoch [704/4000], Loss: 0.8087\n",
      "Epoch [705/4000], Loss: 0.8526\n",
      "Epoch [706/4000], Loss: 0.9170\n",
      "Epoch [707/4000], Loss: 0.8720\n",
      "Epoch [708/4000], Loss: 0.6786\n",
      "Epoch [709/4000], Loss: 0.8213\n",
      "Epoch [710/4000], Loss: 0.9684\n",
      "Epoch [711/4000], Loss: 0.7408\n",
      "Epoch [712/4000], Loss: 0.8962\n",
      "Epoch [713/4000], Loss: 0.7791\n",
      "Epoch [714/4000], Loss: 0.9459\n",
      "Epoch [715/4000], Loss: 0.8052\n",
      "Epoch [716/4000], Loss: 0.7741\n",
      "Epoch [717/4000], Loss: 0.6737\n",
      "Epoch [718/4000], Loss: 0.8190\n",
      "Epoch [719/4000], Loss: 0.8794\n",
      "Epoch [720/4000], Loss: 0.8947\n",
      "Epoch [721/4000], Loss: 1.1528\n",
      "Epoch [722/4000], Loss: 0.9382\n",
      "Epoch [723/4000], Loss: 0.9540\n",
      "Epoch [724/4000], Loss: 0.8723\n",
      "Epoch [725/4000], Loss: 0.7830\n",
      "Epoch [726/4000], Loss: 0.7783\n",
      "Epoch [727/4000], Loss: 0.7801\n",
      "Epoch [728/4000], Loss: 0.6566\n",
      "Epoch [729/4000], Loss: 0.9950\n",
      "Epoch [730/4000], Loss: 1.0011\n",
      "Epoch [731/4000], Loss: 0.6103\n",
      "Epoch [732/4000], Loss: 1.0350\n",
      "Epoch [733/4000], Loss: 0.9689\n",
      "Epoch [734/4000], Loss: 1.0222\n",
      "Epoch [735/4000], Loss: 0.8235\n",
      "Epoch [736/4000], Loss: 0.9569\n",
      "Epoch [737/4000], Loss: 0.7317\n",
      "Epoch [738/4000], Loss: 0.6866\n",
      "Epoch [739/4000], Loss: 0.9152\n",
      "Epoch [740/4000], Loss: 0.7761\n",
      "Epoch [741/4000], Loss: 0.5023\n",
      "Epoch [742/4000], Loss: 0.8813\n",
      "Epoch [743/4000], Loss: 0.7975\n",
      "Epoch [744/4000], Loss: 0.7514\n",
      "Epoch [745/4000], Loss: 0.8442\n",
      "Epoch [746/4000], Loss: 0.8454\n",
      "Epoch [747/4000], Loss: 0.7755\n",
      "Epoch [748/4000], Loss: 0.7694\n",
      "Epoch [749/4000], Loss: 0.8872\n",
      "Epoch [750/4000], Loss: 0.8113\n",
      "Epoch [751/4000], Loss: 0.7988\n",
      "Epoch [752/4000], Loss: 0.7666\n",
      "Epoch [753/4000], Loss: 1.0077\n",
      "Epoch [754/4000], Loss: 0.8953\n",
      "Epoch [755/4000], Loss: 0.9109\n",
      "Epoch [756/4000], Loss: 0.9806\n",
      "Epoch [757/4000], Loss: 0.8554\n",
      "Epoch [758/4000], Loss: 0.7070\n",
      "Epoch [759/4000], Loss: 0.7575\n",
      "Epoch [760/4000], Loss: 1.0193\n",
      "Epoch [761/4000], Loss: 1.0448\n",
      "Epoch [762/4000], Loss: 0.8557\n",
      "Epoch [763/4000], Loss: 0.8922\n",
      "Epoch [764/4000], Loss: 0.8420\n",
      "Epoch [765/4000], Loss: 0.8948\n",
      "Epoch [766/4000], Loss: 0.7092\n",
      "Epoch [767/4000], Loss: 0.7363\n",
      "Epoch [768/4000], Loss: 0.5648\n",
      "Epoch [769/4000], Loss: 0.6696\n",
      "Epoch [770/4000], Loss: 0.9883\n",
      "Epoch [771/4000], Loss: 0.7572\n",
      "Epoch [772/4000], Loss: 0.8708\n",
      "Epoch [773/4000], Loss: 0.7775\n",
      "Epoch [774/4000], Loss: 0.6961\n",
      "Epoch [775/4000], Loss: 0.7238\n",
      "Epoch [776/4000], Loss: 0.9621\n",
      "Epoch [777/4000], Loss: 0.7523\n",
      "Epoch [778/4000], Loss: 0.8107\n",
      "Epoch [779/4000], Loss: 0.9207\n",
      "Epoch [780/4000], Loss: 1.0937\n",
      "Epoch [781/4000], Loss: 0.7704\n",
      "Epoch [782/4000], Loss: 0.6040\n",
      "Epoch [783/4000], Loss: 0.9750\n",
      "Epoch [784/4000], Loss: 0.6530\n",
      "Epoch [785/4000], Loss: 0.6577\n",
      "Epoch [786/4000], Loss: 0.7190\n",
      "Epoch [787/4000], Loss: 0.9963\n",
      "Epoch [788/4000], Loss: 0.8323\n",
      "Epoch [789/4000], Loss: 0.7940\n",
      "Epoch [790/4000], Loss: 1.0720\n",
      "Epoch [791/4000], Loss: 0.6024\n",
      "Epoch [792/4000], Loss: 0.7196\n",
      "Epoch [793/4000], Loss: 0.7393\n",
      "Epoch [794/4000], Loss: 0.7606\n",
      "Epoch [795/4000], Loss: 0.7659\n",
      "Epoch [796/4000], Loss: 1.0257\n",
      "Epoch [797/4000], Loss: 0.8034\n",
      "Epoch [798/4000], Loss: 0.8394\n",
      "Epoch [799/4000], Loss: 0.7188\n",
      "Epoch [800/4000], Loss: 1.0115\n",
      "Epoch [801/4000], Loss: 0.9400\n",
      "Epoch [802/4000], Loss: 0.8002\n",
      "Epoch [803/4000], Loss: 0.8670\n",
      "Epoch [804/4000], Loss: 0.7363\n",
      "Epoch [805/4000], Loss: 0.7115\n",
      "Epoch [806/4000], Loss: 0.8039\n",
      "Epoch [807/4000], Loss: 0.6940\n",
      "Epoch [808/4000], Loss: 0.5483\n",
      "Epoch [809/4000], Loss: 1.1819\n",
      "Epoch [810/4000], Loss: 0.9390\n",
      "Epoch [811/4000], Loss: 0.7786\n",
      "Epoch [812/4000], Loss: 0.7659\n",
      "Epoch [813/4000], Loss: 0.7460\n",
      "Epoch [814/4000], Loss: 0.8064\n",
      "Epoch [815/4000], Loss: 0.8680\n",
      "Epoch [816/4000], Loss: 0.7305\n",
      "Epoch [817/4000], Loss: 0.9428\n",
      "Epoch [818/4000], Loss: 0.6768\n",
      "Epoch [819/4000], Loss: 0.8315\n",
      "Epoch [820/4000], Loss: 0.7150\n",
      "Epoch [821/4000], Loss: 0.5734\n",
      "Epoch [822/4000], Loss: 0.7304\n",
      "Epoch [823/4000], Loss: 0.8586\n",
      "Epoch [824/4000], Loss: 0.8211\n",
      "Epoch [825/4000], Loss: 0.7678\n",
      "Epoch [826/4000], Loss: 0.8313\n",
      "Epoch [827/4000], Loss: 0.8426\n",
      "Epoch [828/4000], Loss: 0.9730\n",
      "Epoch [829/4000], Loss: 0.7824\n",
      "Epoch [830/4000], Loss: 0.8375\n",
      "Epoch [831/4000], Loss: 0.7104\n",
      "Epoch [832/4000], Loss: 0.6319\n",
      "Epoch [833/4000], Loss: 0.7138\n",
      "Epoch [834/4000], Loss: 0.8549\n",
      "Epoch [835/4000], Loss: 0.7725\n",
      "Epoch [836/4000], Loss: 0.8710\n",
      "Epoch [837/4000], Loss: 0.7766\n",
      "Epoch [838/4000], Loss: 1.0363\n",
      "Epoch [839/4000], Loss: 0.6888\n",
      "Epoch [840/4000], Loss: 0.8235\n",
      "Epoch [841/4000], Loss: 0.6453\n",
      "Epoch [842/4000], Loss: 0.5993\n",
      "Epoch [843/4000], Loss: 0.8059\n",
      "Epoch [844/4000], Loss: 0.6931\n",
      "Epoch [845/4000], Loss: 0.5575\n",
      "Epoch [846/4000], Loss: 0.8074\n",
      "Epoch [847/4000], Loss: 0.8382\n",
      "Epoch [848/4000], Loss: 0.6247\n",
      "Epoch [849/4000], Loss: 0.7990\n",
      "Epoch [850/4000], Loss: 0.6571\n",
      "Epoch [851/4000], Loss: 0.6617\n",
      "Epoch [852/4000], Loss: 0.5398\n",
      "Epoch [853/4000], Loss: 0.7016\n",
      "Epoch [854/4000], Loss: 0.9180\n",
      "Epoch [855/4000], Loss: 0.7277\n",
      "Epoch [856/4000], Loss: 0.8321\n",
      "Epoch [857/4000], Loss: 0.5072\n",
      "Epoch [858/4000], Loss: 0.7660\n",
      "Epoch [859/4000], Loss: 0.8187\n",
      "Epoch [860/4000], Loss: 0.8414\n",
      "Epoch [861/4000], Loss: 0.6794\n",
      "Epoch [862/4000], Loss: 0.8005\n",
      "Epoch [863/4000], Loss: 0.6690\n",
      "Epoch [864/4000], Loss: 0.7221\n",
      "Epoch [865/4000], Loss: 0.7458\n",
      "Epoch [866/4000], Loss: 0.6897\n",
      "Epoch [867/4000], Loss: 0.7409\n",
      "Epoch [868/4000], Loss: 0.6188\n",
      "Epoch [869/4000], Loss: 0.7241\n",
      "Epoch [870/4000], Loss: 0.8291\n",
      "Epoch [871/4000], Loss: 0.6111\n",
      "Epoch [872/4000], Loss: 0.9501\n",
      "Epoch [873/4000], Loss: 0.7156\n",
      "Epoch [874/4000], Loss: 0.8697\n",
      "Epoch [875/4000], Loss: 0.7764\n",
      "Epoch [876/4000], Loss: 0.8216\n",
      "Epoch [877/4000], Loss: 0.7818\n",
      "Epoch [878/4000], Loss: 0.7843\n",
      "Epoch [879/4000], Loss: 0.7358\n",
      "Epoch [880/4000], Loss: 1.1364\n",
      "Epoch [881/4000], Loss: 0.6799\n",
      "Epoch [882/4000], Loss: 1.0132\n",
      "Epoch [883/4000], Loss: 0.7448\n",
      "Epoch [884/4000], Loss: 0.5648\n",
      "Epoch [885/4000], Loss: 0.9958\n",
      "Epoch [886/4000], Loss: 0.7600\n",
      "Epoch [887/4000], Loss: 0.6392\n",
      "Epoch [888/4000], Loss: 0.5808\n",
      "Epoch [889/4000], Loss: 0.5841\n",
      "Epoch [890/4000], Loss: 0.7243\n",
      "Epoch [891/4000], Loss: 0.8110\n",
      "Epoch [892/4000], Loss: 0.5253\n",
      "Epoch [893/4000], Loss: 0.5252\n",
      "Epoch [894/4000], Loss: 0.8084\n",
      "Epoch [895/4000], Loss: 1.0016\n",
      "Epoch [896/4000], Loss: 0.7540\n",
      "Epoch [897/4000], Loss: 0.8008\n",
      "Epoch [898/4000], Loss: 0.6557\n",
      "Epoch [899/4000], Loss: 0.6647\n",
      "Epoch [900/4000], Loss: 0.5613\n",
      "Epoch [901/4000], Loss: 0.5902\n",
      "Epoch [902/4000], Loss: 0.7138\n",
      "Epoch [903/4000], Loss: 1.0991\n",
      "Epoch [904/4000], Loss: 0.7288\n",
      "Epoch [905/4000], Loss: 0.5928\n",
      "Epoch [906/4000], Loss: 0.6404\n",
      "Epoch [907/4000], Loss: 0.8941\n",
      "Epoch [908/4000], Loss: 0.8718\n",
      "Epoch [909/4000], Loss: 0.7222\n",
      "Epoch [910/4000], Loss: 0.5586\n",
      "Epoch [911/4000], Loss: 0.7836\n",
      "Epoch [912/4000], Loss: 0.8642\n",
      "Epoch [913/4000], Loss: 0.6511\n",
      "Epoch [914/4000], Loss: 0.6738\n",
      "Epoch [915/4000], Loss: 0.6764\n",
      "Epoch [916/4000], Loss: 0.7880\n",
      "Epoch [917/4000], Loss: 0.9691\n",
      "Epoch [918/4000], Loss: 0.7552\n",
      "Epoch [919/4000], Loss: 0.8596\n",
      "Epoch [920/4000], Loss: 0.6293\n",
      "Epoch [921/4000], Loss: 0.7094\n",
      "Epoch [922/4000], Loss: 0.9297\n",
      "Epoch [923/4000], Loss: 0.6549\n",
      "Epoch [924/4000], Loss: 0.6834\n",
      "Epoch [925/4000], Loss: 0.6436\n",
      "Epoch [926/4000], Loss: 0.6578\n",
      "Epoch [927/4000], Loss: 1.0091\n",
      "Epoch [928/4000], Loss: 0.6430\n",
      "Epoch [929/4000], Loss: 0.7836\n",
      "Epoch [930/4000], Loss: 0.7257\n",
      "Epoch [931/4000], Loss: 0.7191\n",
      "Epoch [932/4000], Loss: 0.8971\n",
      "Epoch [933/4000], Loss: 0.7873\n",
      "Epoch [934/4000], Loss: 0.6020\n",
      "Epoch [935/4000], Loss: 0.6508\n",
      "Epoch [936/4000], Loss: 0.6201\n",
      "Epoch [937/4000], Loss: 0.5583\n",
      "Epoch [938/4000], Loss: 0.6381\n",
      "Epoch [939/4000], Loss: 0.7709\n",
      "Epoch [940/4000], Loss: 0.7502\n",
      "Epoch [941/4000], Loss: 0.6077\n",
      "Epoch [942/4000], Loss: 0.7233\n",
      "Epoch [943/4000], Loss: 0.6738\n",
      "Epoch [944/4000], Loss: 0.7605\n",
      "Epoch [945/4000], Loss: 0.5300\n",
      "Epoch [946/4000], Loss: 0.8417\n",
      "Epoch [947/4000], Loss: 0.7566\n",
      "Epoch [948/4000], Loss: 0.8246\n",
      "Epoch [949/4000], Loss: 0.7395\n",
      "Epoch [950/4000], Loss: 0.7043\n",
      "Epoch [951/4000], Loss: 0.6681\n",
      "Epoch [952/4000], Loss: 0.9308\n",
      "Epoch [953/4000], Loss: 0.9440\n",
      "Epoch [954/4000], Loss: 0.7579\n",
      "Epoch [955/4000], Loss: 0.7967\n",
      "Epoch [956/4000], Loss: 0.6478\n",
      "Epoch [957/4000], Loss: 0.5744\n",
      "Epoch [958/4000], Loss: 0.7237\n",
      "Epoch [959/4000], Loss: 0.8531\n",
      "Epoch [960/4000], Loss: 0.7577\n",
      "Epoch [961/4000], Loss: 0.7443\n",
      "Epoch [962/4000], Loss: 0.7650\n",
      "Epoch [963/4000], Loss: 0.6265\n",
      "Epoch [964/4000], Loss: 0.9045\n",
      "Epoch [965/4000], Loss: 0.7302\n",
      "Epoch [966/4000], Loss: 0.8382\n",
      "Epoch [967/4000], Loss: 0.7954\n",
      "Epoch [968/4000], Loss: 0.8957\n",
      "Epoch [969/4000], Loss: 0.8520\n",
      "Epoch [970/4000], Loss: 1.0924\n",
      "Epoch [971/4000], Loss: 0.8255\n",
      "Epoch [972/4000], Loss: 0.6970\n",
      "Epoch [973/4000], Loss: 0.6459\n",
      "Epoch [974/4000], Loss: 0.7943\n",
      "Epoch [975/4000], Loss: 0.6418\n",
      "Epoch [976/4000], Loss: 0.7106\n",
      "Epoch [977/4000], Loss: 0.7807\n",
      "Epoch [978/4000], Loss: 0.9817\n",
      "Epoch [979/4000], Loss: 0.6337\n",
      "Epoch [980/4000], Loss: 0.7056\n",
      "Epoch [981/4000], Loss: 0.8504\n",
      "Epoch [982/4000], Loss: 0.7275\n",
      "Epoch [983/4000], Loss: 0.9392\n",
      "Epoch [984/4000], Loss: 0.6233\n",
      "Epoch [985/4000], Loss: 0.8948\n",
      "Epoch [986/4000], Loss: 0.5115\n",
      "Epoch [987/4000], Loss: 1.0956\n",
      "Epoch [988/4000], Loss: 0.9338\n",
      "Epoch [989/4000], Loss: 0.8271\n",
      "Epoch [990/4000], Loss: 0.7910\n",
      "Epoch [991/4000], Loss: 0.7561\n",
      "Epoch [992/4000], Loss: 0.5612\n",
      "Epoch [993/4000], Loss: 0.7638\n",
      "Epoch [994/4000], Loss: 0.6931\n",
      "Epoch [995/4000], Loss: 0.6787\n",
      "Epoch [996/4000], Loss: 0.6813\n",
      "Epoch [997/4000], Loss: 0.7421\n",
      "Epoch [998/4000], Loss: 0.7589\n",
      "Epoch [999/4000], Loss: 0.7033\n",
      "Epoch [1000/4000], Loss: 0.5789\n",
      "Epoch [1001/4000], Loss: 0.6389\n",
      "Epoch [1002/4000], Loss: 0.6504\n",
      "Epoch [1003/4000], Loss: 0.6209\n",
      "Epoch [1004/4000], Loss: 0.5467\n",
      "Epoch [1005/4000], Loss: 0.6203\n",
      "Epoch [1006/4000], Loss: 0.4875\n",
      "Epoch [1007/4000], Loss: 0.5349\n",
      "Epoch [1008/4000], Loss: 0.6017\n",
      "Epoch [1009/4000], Loss: 1.0005\n",
      "Epoch [1010/4000], Loss: 0.5222\n",
      "Epoch [1011/4000], Loss: 0.5304\n",
      "Epoch [1012/4000], Loss: 0.8118\n",
      "Epoch [1013/4000], Loss: 0.6668\n",
      "Epoch [1014/4000], Loss: 0.5980\n",
      "Epoch [1015/4000], Loss: 0.7355\n",
      "Epoch [1016/4000], Loss: 0.6096\n",
      "Epoch [1017/4000], Loss: 0.7649\n",
      "Epoch [1018/4000], Loss: 0.5569\n",
      "Epoch [1019/4000], Loss: 0.6857\n",
      "Epoch [1020/4000], Loss: 0.5731\n",
      "Epoch [1021/4000], Loss: 0.4722\n",
      "Epoch [1022/4000], Loss: 1.1468\n",
      "Epoch [1023/4000], Loss: 0.7960\n",
      "Epoch [1024/4000], Loss: 0.5238\n",
      "Epoch [1025/4000], Loss: 0.6424\n",
      "Epoch [1026/4000], Loss: 0.6473\n",
      "Epoch [1027/4000], Loss: 0.8317\n",
      "Epoch [1028/4000], Loss: 0.6547\n",
      "Epoch [1029/4000], Loss: 0.6740\n",
      "Epoch [1030/4000], Loss: 0.5875\n",
      "Epoch [1031/4000], Loss: 0.6607\n",
      "Epoch [1032/4000], Loss: 0.6658\n",
      "Epoch [1033/4000], Loss: 0.6415\n",
      "Epoch [1034/4000], Loss: 0.5897\n",
      "Epoch [1035/4000], Loss: 0.5887\n",
      "Epoch [1036/4000], Loss: 0.7128\n",
      "Epoch [1037/4000], Loss: 0.8413\n",
      "Epoch [1038/4000], Loss: 0.7576\n",
      "Epoch [1039/4000], Loss: 0.5259\n",
      "Epoch [1040/4000], Loss: 0.6912\n",
      "Epoch [1041/4000], Loss: 0.7165\n",
      "Epoch [1042/4000], Loss: 0.6977\n",
      "Epoch [1043/4000], Loss: 0.4704\n",
      "Epoch [1044/4000], Loss: 0.7125\n",
      "Epoch [1045/4000], Loss: 1.0000\n",
      "Epoch [1046/4000], Loss: 0.7250\n",
      "Epoch [1047/4000], Loss: 0.8522\n",
      "Epoch [1048/4000], Loss: 0.6006\n",
      "Epoch [1049/4000], Loss: 0.6144\n",
      "Epoch [1050/4000], Loss: 0.6669\n",
      "Epoch [1051/4000], Loss: 0.6309\n",
      "Epoch [1052/4000], Loss: 0.6746\n",
      "Epoch [1053/4000], Loss: 0.6162\n",
      "Epoch [1054/4000], Loss: 0.6397\n",
      "Epoch [1055/4000], Loss: 0.4991\n",
      "Epoch [1056/4000], Loss: 0.7618\n",
      "Epoch [1057/4000], Loss: 0.9463\n",
      "Epoch [1058/4000], Loss: 0.8578\n",
      "Epoch [1059/4000], Loss: 0.5018\n",
      "Epoch [1060/4000], Loss: 0.7195\n",
      "Epoch [1061/4000], Loss: 0.8783\n",
      "Epoch [1062/4000], Loss: 0.5815\n",
      "Epoch [1063/4000], Loss: 0.5664\n",
      "Epoch [1064/4000], Loss: 0.6033\n",
      "Epoch [1065/4000], Loss: 0.9402\n",
      "Epoch [1066/4000], Loss: 0.6425\n",
      "Epoch [1067/4000], Loss: 0.6629\n",
      "Epoch [1068/4000], Loss: 0.9098\n",
      "Epoch [1069/4000], Loss: 0.5818\n",
      "Epoch [1070/4000], Loss: 0.5787\n",
      "Epoch [1071/4000], Loss: 0.6387\n",
      "Epoch [1072/4000], Loss: 0.5812\n",
      "Epoch [1073/4000], Loss: 0.6184\n",
      "Epoch [1074/4000], Loss: 0.6304\n",
      "Epoch [1075/4000], Loss: 0.3815\n",
      "Epoch [1076/4000], Loss: 0.4574\n",
      "Epoch [1077/4000], Loss: 0.7256\n",
      "Epoch [1078/4000], Loss: 0.6912\n",
      "Epoch [1079/4000], Loss: 0.8445\n",
      "Epoch [1080/4000], Loss: 0.7100\n",
      "Epoch [1081/4000], Loss: 0.7131\n",
      "Epoch [1082/4000], Loss: 0.5408\n",
      "Epoch [1083/4000], Loss: 0.6069\n",
      "Epoch [1084/4000], Loss: 0.5434\n",
      "Epoch [1085/4000], Loss: 0.5756\n",
      "Epoch [1086/4000], Loss: 0.8327\n",
      "Epoch [1087/4000], Loss: 0.5755\n",
      "Epoch [1088/4000], Loss: 0.7027\n",
      "Epoch [1089/4000], Loss: 0.5781\n",
      "Epoch [1090/4000], Loss: 0.6331\n",
      "Epoch [1091/4000], Loss: 0.5262\n",
      "Epoch [1092/4000], Loss: 0.6357\n",
      "Epoch [1093/4000], Loss: 0.5606\n",
      "Epoch [1094/4000], Loss: 0.6742\n",
      "Epoch [1095/4000], Loss: 0.8698\n",
      "Epoch [1096/4000], Loss: 0.7318\n",
      "Epoch [1097/4000], Loss: 0.6325\n",
      "Epoch [1098/4000], Loss: 0.5732\n",
      "Epoch [1099/4000], Loss: 0.8192\n",
      "Epoch [1100/4000], Loss: 0.7892\n",
      "Epoch [1101/4000], Loss: 0.5438\n",
      "Epoch [1102/4000], Loss: 0.6312\n",
      "Epoch [1103/4000], Loss: 0.6492\n",
      "Epoch [1104/4000], Loss: 0.6896\n",
      "Epoch [1105/4000], Loss: 0.7258\n",
      "Epoch [1106/4000], Loss: 0.7367\n",
      "Epoch [1107/4000], Loss: 0.5737\n",
      "Epoch [1108/4000], Loss: 0.8030\n",
      "Epoch [1109/4000], Loss: 0.9520\n",
      "Epoch [1110/4000], Loss: 0.6302\n",
      "Epoch [1111/4000], Loss: 0.6047\n",
      "Epoch [1112/4000], Loss: 0.6793\n",
      "Epoch [1113/4000], Loss: 0.5631\n",
      "Epoch [1114/4000], Loss: 0.5671\n",
      "Epoch [1115/4000], Loss: 0.5831\n",
      "Epoch [1116/4000], Loss: 0.6021\n",
      "Epoch [1117/4000], Loss: 0.5612\n",
      "Epoch [1118/4000], Loss: 0.8038\n",
      "Epoch [1119/4000], Loss: 0.7049\n",
      "Epoch [1120/4000], Loss: 0.7991\n",
      "Epoch [1121/4000], Loss: 0.7438\n",
      "Epoch [1122/4000], Loss: 0.6565\n",
      "Epoch [1123/4000], Loss: 0.6101\n",
      "Epoch [1124/4000], Loss: 0.6019\n",
      "Epoch [1125/4000], Loss: 0.8763\n",
      "Epoch [1126/4000], Loss: 0.5335\n",
      "Epoch [1127/4000], Loss: 0.8455\n",
      "Epoch [1128/4000], Loss: 0.6584\n",
      "Epoch [1129/4000], Loss: 0.5797\n",
      "Epoch [1130/4000], Loss: 0.6151\n",
      "Epoch [1131/4000], Loss: 0.6893\n",
      "Epoch [1132/4000], Loss: 0.6121\n",
      "Epoch [1133/4000], Loss: 0.6623\n",
      "Epoch [1134/4000], Loss: 0.4628\n",
      "Epoch [1135/4000], Loss: 0.7312\n",
      "Epoch [1136/4000], Loss: 0.6152\n",
      "Epoch [1137/4000], Loss: 0.8195\n",
      "Epoch [1138/4000], Loss: 0.6426\n",
      "Epoch [1139/4000], Loss: 0.6383\n",
      "Epoch [1140/4000], Loss: 0.6555\n",
      "Epoch [1141/4000], Loss: 0.6338\n",
      "Epoch [1142/4000], Loss: 0.5799\n",
      "Epoch [1143/4000], Loss: 0.7843\n",
      "Epoch [1144/4000], Loss: 0.8888\n",
      "Epoch [1145/4000], Loss: 0.6460\n",
      "Epoch [1146/4000], Loss: 0.6104\n",
      "Epoch [1147/4000], Loss: 0.5459\n",
      "Epoch [1148/4000], Loss: 0.7265\n",
      "Epoch [1149/4000], Loss: 0.8103\n",
      "Epoch [1150/4000], Loss: 0.5368\n",
      "Epoch [1151/4000], Loss: 0.6509\n",
      "Epoch [1152/4000], Loss: 0.5992\n",
      "Epoch [1153/4000], Loss: 0.7124\n",
      "Epoch [1154/4000], Loss: 0.4825\n",
      "Epoch [1155/4000], Loss: 0.4921\n",
      "Epoch [1156/4000], Loss: 0.8283\n",
      "Epoch [1157/4000], Loss: 0.6863\n",
      "Epoch [1158/4000], Loss: 0.5591\n",
      "Epoch [1159/4000], Loss: 0.4063\n",
      "Epoch [1160/4000], Loss: 0.7999\n",
      "Epoch [1161/4000], Loss: 1.0746\n",
      "Epoch [1162/4000], Loss: 0.4841\n",
      "Epoch [1163/4000], Loss: 0.4901\n",
      "Epoch [1164/4000], Loss: 0.6353\n",
      "Epoch [1165/4000], Loss: 0.5195\n",
      "Epoch [1166/4000], Loss: 0.5607\n",
      "Epoch [1167/4000], Loss: 0.8549\n",
      "Epoch [1168/4000], Loss: 0.8816\n",
      "Epoch [1169/4000], Loss: 0.5864\n",
      "Epoch [1170/4000], Loss: 0.5364\n",
      "Epoch [1171/4000], Loss: 0.9652\n",
      "Epoch [1172/4000], Loss: 0.8543\n",
      "Epoch [1173/4000], Loss: 0.6636\n",
      "Epoch [1174/4000], Loss: 0.7574\n",
      "Epoch [1175/4000], Loss: 0.8391\n",
      "Epoch [1176/4000], Loss: 0.5572\n",
      "Epoch [1177/4000], Loss: 0.6171\n",
      "Epoch [1178/4000], Loss: 0.5151\n",
      "Epoch [1179/4000], Loss: 0.5704\n",
      "Epoch [1180/4000], Loss: 0.4331\n",
      "Epoch [1181/4000], Loss: 0.4844\n",
      "Epoch [1182/4000], Loss: 0.4140\n",
      "Epoch [1183/4000], Loss: 0.4383\n",
      "Epoch [1184/4000], Loss: 0.5278\n",
      "Epoch [1185/4000], Loss: 0.6070\n",
      "Epoch [1186/4000], Loss: 0.5363\n",
      "Epoch [1187/4000], Loss: 0.7018\n",
      "Epoch [1188/4000], Loss: 0.5219\n",
      "Epoch [1189/4000], Loss: 0.5610\n",
      "Epoch [1190/4000], Loss: 0.5194\n",
      "Epoch [1191/4000], Loss: 0.6327\n",
      "Epoch [1192/4000], Loss: 0.5416\n",
      "Epoch [1193/4000], Loss: 0.7164\n",
      "Epoch [1194/4000], Loss: 0.6644\n",
      "Epoch [1195/4000], Loss: 0.6709\n",
      "Epoch [1196/4000], Loss: 0.8102\n",
      "Epoch [1197/4000], Loss: 0.5700\n",
      "Epoch [1198/4000], Loss: 0.5956\n",
      "Epoch [1199/4000], Loss: 0.7008\n",
      "Epoch [1200/4000], Loss: 0.6492\n",
      "Epoch [1201/4000], Loss: 0.6687\n",
      "Epoch [1202/4000], Loss: 0.6729\n",
      "Epoch [1203/4000], Loss: 0.7035\n",
      "Epoch [1204/4000], Loss: 0.5504\n",
      "Epoch [1205/4000], Loss: 0.5941\n",
      "Epoch [1206/4000], Loss: 0.6305\n",
      "Epoch [1207/4000], Loss: 0.4807\n",
      "Epoch [1208/4000], Loss: 0.7599\n",
      "Epoch [1209/4000], Loss: 0.6020\n",
      "Epoch [1210/4000], Loss: 0.5857\n",
      "Epoch [1211/4000], Loss: 0.5614\n",
      "Epoch [1212/4000], Loss: 0.7882\n",
      "Epoch [1213/4000], Loss: 0.7454\n",
      "Epoch [1214/4000], Loss: 0.4991\n",
      "Epoch [1215/4000], Loss: 0.5054\n",
      "Epoch [1216/4000], Loss: 0.6374\n",
      "Epoch [1217/4000], Loss: 0.5581\n",
      "Epoch [1218/4000], Loss: 0.6286\n",
      "Epoch [1219/4000], Loss: 0.6424\n",
      "Epoch [1220/4000], Loss: 0.6765\n",
      "Epoch [1221/4000], Loss: 0.7767\n",
      "Epoch [1222/4000], Loss: 0.5521\n",
      "Epoch [1223/4000], Loss: 0.6399\n",
      "Epoch [1224/4000], Loss: 0.5868\n",
      "Epoch [1225/4000], Loss: 0.5590\n",
      "Epoch [1226/4000], Loss: 0.8332\n",
      "Epoch [1227/4000], Loss: 0.3854\n",
      "Epoch [1228/4000], Loss: 0.7939\n",
      "Epoch [1229/4000], Loss: 0.8057\n",
      "Epoch [1230/4000], Loss: 0.5032\n",
      "Epoch [1231/4000], Loss: 0.4155\n",
      "Epoch [1232/4000], Loss: 0.4417\n",
      "Epoch [1233/4000], Loss: 0.7396\n",
      "Epoch [1234/4000], Loss: 0.6515\n",
      "Epoch [1235/4000], Loss: 0.4863\n",
      "Epoch [1236/4000], Loss: 0.6615\n",
      "Epoch [1237/4000], Loss: 0.6154\n",
      "Epoch [1238/4000], Loss: 0.5339\n",
      "Epoch [1239/4000], Loss: 0.6185\n",
      "Epoch [1240/4000], Loss: 0.5366\n",
      "Epoch [1241/4000], Loss: 0.5990\n",
      "Epoch [1242/4000], Loss: 0.5352\n",
      "Epoch [1243/4000], Loss: 0.6296\n",
      "Epoch [1244/4000], Loss: 0.6320\n",
      "Epoch [1245/4000], Loss: 1.0156\n",
      "Epoch [1246/4000], Loss: 0.6538\n",
      "Epoch [1247/4000], Loss: 0.6059\n",
      "Epoch [1248/4000], Loss: 0.8049\n",
      "Epoch [1249/4000], Loss: 0.4266\n",
      "Epoch [1250/4000], Loss: 0.6156\n",
      "Epoch [1251/4000], Loss: 0.6742\n",
      "Epoch [1252/4000], Loss: 0.3942\n",
      "Epoch [1253/4000], Loss: 0.4821\n",
      "Epoch [1254/4000], Loss: 0.8602\n",
      "Epoch [1255/4000], Loss: 0.4558\n",
      "Epoch [1256/4000], Loss: 0.7224\n",
      "Epoch [1257/4000], Loss: 0.7345\n",
      "Epoch [1258/4000], Loss: 0.4971\n",
      "Epoch [1259/4000], Loss: 0.5497\n",
      "Epoch [1260/4000], Loss: 0.7378\n",
      "Epoch [1261/4000], Loss: 0.7023\n",
      "Epoch [1262/4000], Loss: 0.4132\n",
      "Epoch [1263/4000], Loss: 0.7638\n",
      "Epoch [1264/4000], Loss: 0.6909\n",
      "Epoch [1265/4000], Loss: 0.5406\n",
      "Epoch [1266/4000], Loss: 0.8794\n",
      "Epoch [1267/4000], Loss: 0.5005\n",
      "Epoch [1268/4000], Loss: 0.6617\n",
      "Epoch [1269/4000], Loss: 0.5551\n",
      "Epoch [1270/4000], Loss: 0.7670\n",
      "Epoch [1271/4000], Loss: 0.6125\n",
      "Epoch [1272/4000], Loss: 0.4924\n",
      "Epoch [1273/4000], Loss: 0.4179\n",
      "Epoch [1274/4000], Loss: 0.5633\n",
      "Epoch [1275/4000], Loss: 0.5139\n",
      "Epoch [1276/4000], Loss: 0.4540\n",
      "Epoch [1277/4000], Loss: 0.7637\n",
      "Epoch [1278/4000], Loss: 0.5428\n",
      "Epoch [1279/4000], Loss: 0.6786\n",
      "Epoch [1280/4000], Loss: 0.6003\n",
      "Epoch [1281/4000], Loss: 0.7416\n",
      "Epoch [1282/4000], Loss: 0.5259\n",
      "Epoch [1283/4000], Loss: 0.6637\n",
      "Epoch [1284/4000], Loss: 0.7496\n",
      "Epoch [1285/4000], Loss: 0.6146\n",
      "Epoch [1286/4000], Loss: 0.4616\n",
      "Epoch [1287/4000], Loss: 0.5886\n",
      "Epoch [1288/4000], Loss: 0.5735\n",
      "Epoch [1289/4000], Loss: 0.5550\n",
      "Epoch [1290/4000], Loss: 0.5533\n",
      "Epoch [1291/4000], Loss: 0.6249\n",
      "Epoch [1292/4000], Loss: 0.4879\n",
      "Epoch [1293/4000], Loss: 0.7162\n",
      "Epoch [1294/4000], Loss: 0.6583\n",
      "Epoch [1295/4000], Loss: 0.4052\n",
      "Epoch [1296/4000], Loss: 0.6413\n",
      "Epoch [1297/4000], Loss: 0.5810\n",
      "Epoch [1298/4000], Loss: 0.5296\n",
      "Epoch [1299/4000], Loss: 0.7239\n",
      "Epoch [1300/4000], Loss: 0.6869\n",
      "Epoch [1301/4000], Loss: 0.5845\n",
      "Epoch [1302/4000], Loss: 0.7199\n",
      "Epoch [1303/4000], Loss: 0.5102\n",
      "Epoch [1304/4000], Loss: 0.4943\n",
      "Epoch [1305/4000], Loss: 0.4926\n",
      "Epoch [1306/4000], Loss: 0.5166\n",
      "Epoch [1307/4000], Loss: 0.7968\n",
      "Epoch [1308/4000], Loss: 0.4775\n",
      "Epoch [1309/4000], Loss: 0.6102\n",
      "Epoch [1310/4000], Loss: 0.5804\n",
      "Epoch [1311/4000], Loss: 0.5035\n",
      "Epoch [1312/4000], Loss: 0.4245\n",
      "Epoch [1313/4000], Loss: 0.6172\n",
      "Epoch [1314/4000], Loss: 0.5716\n",
      "Epoch [1315/4000], Loss: 0.5456\n",
      "Epoch [1316/4000], Loss: 0.6544\n",
      "Epoch [1317/4000], Loss: 0.8143\n",
      "Epoch [1318/4000], Loss: 0.7472\n",
      "Epoch [1319/4000], Loss: 0.3690\n",
      "Epoch [1320/4000], Loss: 0.4278\n",
      "Epoch [1321/4000], Loss: 0.5580\n",
      "Epoch [1322/4000], Loss: 0.6544\n",
      "Epoch [1323/4000], Loss: 0.6381\n",
      "Epoch [1324/4000], Loss: 0.5236\n",
      "Epoch [1325/4000], Loss: 0.3978\n",
      "Epoch [1326/4000], Loss: 0.4741\n",
      "Epoch [1327/4000], Loss: 0.5161\n",
      "Epoch [1328/4000], Loss: 0.5887\n",
      "Epoch [1329/4000], Loss: 0.4979\n",
      "Epoch [1330/4000], Loss: 0.6994\n",
      "Epoch [1331/4000], Loss: 0.4161\n",
      "Epoch [1332/4000], Loss: 0.6225\n",
      "Epoch [1333/4000], Loss: 0.3925\n",
      "Epoch [1334/4000], Loss: 0.4536\n",
      "Epoch [1335/4000], Loss: 0.6424\n",
      "Epoch [1336/4000], Loss: 0.6789\n",
      "Epoch [1337/4000], Loss: 0.5907\n",
      "Epoch [1338/4000], Loss: 0.4376\n",
      "Epoch [1339/4000], Loss: 0.7373\n",
      "Epoch [1340/4000], Loss: 0.5103\n",
      "Epoch [1341/4000], Loss: 0.4530\n",
      "Epoch [1342/4000], Loss: 0.6496\n",
      "Epoch [1343/4000], Loss: 0.6118\n",
      "Epoch [1344/4000], Loss: 0.6596\n",
      "Epoch [1345/4000], Loss: 0.4067\n",
      "Epoch [1346/4000], Loss: 0.4673\n",
      "Epoch [1347/4000], Loss: 0.5662\n",
      "Epoch [1348/4000], Loss: 0.4464\n",
      "Epoch [1349/4000], Loss: 0.6065\n",
      "Epoch [1350/4000], Loss: 0.4916\n",
      "Epoch [1351/4000], Loss: 0.5572\n",
      "Epoch [1352/4000], Loss: 0.5415\n",
      "Epoch [1353/4000], Loss: 0.7303\n",
      "Epoch [1354/4000], Loss: 0.6146\n",
      "Epoch [1355/4000], Loss: 0.5121\n",
      "Epoch [1356/4000], Loss: 0.5767\n",
      "Epoch [1357/4000], Loss: 0.4082\n",
      "Epoch [1358/4000], Loss: 0.7075\n",
      "Epoch [1359/4000], Loss: 0.7550\n",
      "Epoch [1360/4000], Loss: 0.4526\n",
      "Epoch [1361/4000], Loss: 0.8834\n",
      "Epoch [1362/4000], Loss: 0.6364\n",
      "Epoch [1363/4000], Loss: 0.8206\n",
      "Epoch [1364/4000], Loss: 0.4411\n",
      "Epoch [1365/4000], Loss: 0.7703\n",
      "Epoch [1366/4000], Loss: 0.5682\n",
      "Epoch [1367/4000], Loss: 0.7624\n",
      "Epoch [1368/4000], Loss: 0.6369\n",
      "Epoch [1369/4000], Loss: 0.6591\n",
      "Epoch [1370/4000], Loss: 0.3840\n",
      "Epoch [1371/4000], Loss: 0.4193\n",
      "Epoch [1372/4000], Loss: 0.7181\n",
      "Epoch [1373/4000], Loss: 0.4013\n",
      "Epoch [1374/4000], Loss: 0.6080\n",
      "Epoch [1375/4000], Loss: 0.5397\n",
      "Epoch [1376/4000], Loss: 0.5583\n",
      "Epoch [1377/4000], Loss: 0.6559\n",
      "Epoch [1378/4000], Loss: 0.5738\n",
      "Epoch [1379/4000], Loss: 0.3727\n",
      "Epoch [1380/4000], Loss: 0.4278\n",
      "Epoch [1381/4000], Loss: 0.4975\n",
      "Epoch [1382/4000], Loss: 0.4334\n",
      "Epoch [1383/4000], Loss: 0.5256\n",
      "Epoch [1384/4000], Loss: 0.3857\n",
      "Epoch [1385/4000], Loss: 0.8762\n",
      "Epoch [1386/4000], Loss: 0.7250\n",
      "Epoch [1387/4000], Loss: 0.6159\n",
      "Epoch [1388/4000], Loss: 0.5142\n",
      "Epoch [1389/4000], Loss: 0.5043\n",
      "Epoch [1390/4000], Loss: 0.5908\n",
      "Epoch [1391/4000], Loss: 0.5790\n",
      "Epoch [1392/4000], Loss: 0.5246\n",
      "Epoch [1393/4000], Loss: 0.6188\n",
      "Epoch [1394/4000], Loss: 0.5188\n",
      "Epoch [1395/4000], Loss: 0.5879\n",
      "Epoch [1396/4000], Loss: 0.4162\n",
      "Epoch [1397/4000], Loss: 0.5236\n",
      "Epoch [1398/4000], Loss: 0.6797\n",
      "Epoch [1399/4000], Loss: 0.5885\n",
      "Epoch [1400/4000], Loss: 0.4679\n",
      "Epoch [1401/4000], Loss: 0.5946\n",
      "Epoch [1402/4000], Loss: 0.4292\n",
      "Epoch [1403/4000], Loss: 0.4750\n",
      "Epoch [1404/4000], Loss: 0.8002\n",
      "Epoch [1405/4000], Loss: 0.4655\n",
      "Epoch [1406/4000], Loss: 0.7969\n",
      "Epoch [1407/4000], Loss: 0.7477\n",
      "Epoch [1408/4000], Loss: 0.5124\n",
      "Epoch [1409/4000], Loss: 0.7237\n",
      "Epoch [1410/4000], Loss: 0.5186\n",
      "Epoch [1411/4000], Loss: 0.6078\n",
      "Epoch [1412/4000], Loss: 0.3965\n",
      "Epoch [1413/4000], Loss: 0.5232\n",
      "Epoch [1414/4000], Loss: 0.4341\n",
      "Epoch [1415/4000], Loss: 0.4900\n",
      "Epoch [1416/4000], Loss: 0.5188\n",
      "Epoch [1417/4000], Loss: 0.7345\n",
      "Epoch [1418/4000], Loss: 0.7658\n",
      "Epoch [1419/4000], Loss: 0.3690\n",
      "Epoch [1420/4000], Loss: 0.3678\n",
      "Epoch [1421/4000], Loss: 0.5381\n",
      "Epoch [1422/4000], Loss: 0.6179\n",
      "Epoch [1423/4000], Loss: 0.5090\n",
      "Epoch [1424/4000], Loss: 0.4429\n",
      "Epoch [1425/4000], Loss: 0.4388\n",
      "Epoch [1426/4000], Loss: 0.4419\n",
      "Epoch [1427/4000], Loss: 0.5653\n",
      "Epoch [1428/4000], Loss: 0.4581\n",
      "Epoch [1429/4000], Loss: 0.5403\n",
      "Epoch [1430/4000], Loss: 0.5468\n",
      "Epoch [1431/4000], Loss: 0.4166\n",
      "Epoch [1432/4000], Loss: 0.4338\n",
      "Epoch [1433/4000], Loss: 0.6694\n",
      "Epoch [1434/4000], Loss: 0.3563\n",
      "Epoch [1435/4000], Loss: 0.4648\n",
      "Epoch [1436/4000], Loss: 0.5754\n",
      "Epoch [1437/4000], Loss: 0.5440\n",
      "Epoch [1438/4000], Loss: 0.5368\n",
      "Epoch [1439/4000], Loss: 0.6194\n",
      "Epoch [1440/4000], Loss: 0.6752\n",
      "Epoch [1441/4000], Loss: 0.4592\n",
      "Epoch [1442/4000], Loss: 0.5343\n",
      "Epoch [1443/4000], Loss: 0.5474\n",
      "Epoch [1444/4000], Loss: 0.5587\n",
      "Epoch [1445/4000], Loss: 0.5551\n",
      "Epoch [1446/4000], Loss: 0.5550\n",
      "Epoch [1447/4000], Loss: 0.5041\n",
      "Epoch [1448/4000], Loss: 0.4252\n",
      "Epoch [1449/4000], Loss: 0.5084\n",
      "Epoch [1450/4000], Loss: 0.6060\n",
      "Epoch [1451/4000], Loss: 0.6619\n",
      "Epoch [1452/4000], Loss: 0.3967\n",
      "Epoch [1453/4000], Loss: 0.5309\n",
      "Epoch [1454/4000], Loss: 0.8246\n",
      "Epoch [1455/4000], Loss: 0.5142\n",
      "Epoch [1456/4000], Loss: 0.7364\n",
      "Epoch [1457/4000], Loss: 0.4231\n",
      "Epoch [1458/4000], Loss: 0.5630\n",
      "Epoch [1459/4000], Loss: 0.4461\n",
      "Epoch [1460/4000], Loss: 0.5268\n",
      "Epoch [1461/4000], Loss: 0.4852\n",
      "Epoch [1462/4000], Loss: 0.6891\n",
      "Epoch [1463/4000], Loss: 0.6083\n",
      "Epoch [1464/4000], Loss: 0.6910\n",
      "Epoch [1465/4000], Loss: 0.5781\n",
      "Epoch [1466/4000], Loss: 0.6008\n",
      "Epoch [1467/4000], Loss: 0.4804\n",
      "Epoch [1468/4000], Loss: 0.5504\n",
      "Epoch [1469/4000], Loss: 0.7067\n",
      "Epoch [1470/4000], Loss: 0.5144\n",
      "Epoch [1471/4000], Loss: 0.5703\n",
      "Epoch [1472/4000], Loss: 0.5038\n",
      "Epoch [1473/4000], Loss: 0.3361\n",
      "Epoch [1474/4000], Loss: 0.5459\n",
      "Epoch [1475/4000], Loss: 0.4739\n",
      "Epoch [1476/4000], Loss: 0.4323\n",
      "Epoch [1477/4000], Loss: 0.5207\n",
      "Epoch [1478/4000], Loss: 0.4037\n",
      "Epoch [1479/4000], Loss: 0.8387\n",
      "Epoch [1480/4000], Loss: 0.7852\n",
      "Epoch [1481/4000], Loss: 0.5158\n",
      "Epoch [1482/4000], Loss: 0.4991\n",
      "Epoch [1483/4000], Loss: 0.4647\n",
      "Epoch [1484/4000], Loss: 0.4508\n",
      "Epoch [1485/4000], Loss: 0.5039\n",
      "Epoch [1486/4000], Loss: 0.5265\n",
      "Epoch [1487/4000], Loss: 0.4981\n",
      "Epoch [1488/4000], Loss: 0.6086\n",
      "Epoch [1489/4000], Loss: 0.5018\n",
      "Epoch [1490/4000], Loss: 0.5183\n",
      "Epoch [1491/4000], Loss: 0.6023\n",
      "Epoch [1492/4000], Loss: 0.6165\n",
      "Epoch [1493/4000], Loss: 0.4626\n",
      "Epoch [1494/4000], Loss: 0.3972\n",
      "Epoch [1495/4000], Loss: 0.4149\n",
      "Epoch [1496/4000], Loss: 0.6102\n",
      "Epoch [1497/4000], Loss: 0.4165\n",
      "Epoch [1498/4000], Loss: 0.5363\n",
      "Epoch [1499/4000], Loss: 0.4586\n",
      "Epoch [1500/4000], Loss: 0.7003\n",
      "Epoch [1501/4000], Loss: 0.6918\n",
      "Epoch [1502/4000], Loss: 0.6471\n",
      "Epoch [1503/4000], Loss: 0.5518\n",
      "Epoch [1504/4000], Loss: 0.4577\n",
      "Epoch [1505/4000], Loss: 0.6114\n",
      "Epoch [1506/4000], Loss: 0.4714\n",
      "Epoch [1507/4000], Loss: 0.5578\n",
      "Epoch [1508/4000], Loss: 0.6783\n",
      "Epoch [1509/4000], Loss: 0.5253\n",
      "Epoch [1510/4000], Loss: 0.4671\n",
      "Epoch [1511/4000], Loss: 0.4446\n",
      "Epoch [1512/4000], Loss: 0.6068\n",
      "Epoch [1513/4000], Loss: 0.6909\n",
      "Epoch [1514/4000], Loss: 0.5685\n",
      "Epoch [1515/4000], Loss: 0.5925\n",
      "Epoch [1516/4000], Loss: 0.3301\n",
      "Epoch [1517/4000], Loss: 0.7340\n",
      "Epoch [1518/4000], Loss: 0.4052\n",
      "Epoch [1519/4000], Loss: 0.6518\n",
      "Epoch [1520/4000], Loss: 0.3731\n",
      "Epoch [1521/4000], Loss: 0.6984\n",
      "Epoch [1522/4000], Loss: 0.5882\n",
      "Epoch [1523/4000], Loss: 0.3908\n",
      "Epoch [1524/4000], Loss: 0.4692\n",
      "Epoch [1525/4000], Loss: 0.5191\n",
      "Epoch [1526/4000], Loss: 0.5263\n",
      "Epoch [1527/4000], Loss: 0.4349\n",
      "Epoch [1528/4000], Loss: 0.5332\n",
      "Epoch [1529/4000], Loss: 0.5755\n",
      "Epoch [1530/4000], Loss: 0.6164\n",
      "Epoch [1531/4000], Loss: 0.4391\n",
      "Epoch [1532/4000], Loss: 0.5700\n",
      "Epoch [1533/4000], Loss: 0.4522\n",
      "Epoch [1534/4000], Loss: 0.4833\n",
      "Epoch [1535/4000], Loss: 0.5923\n",
      "Epoch [1536/4000], Loss: 0.3742\n",
      "Epoch [1537/4000], Loss: 0.5079\n",
      "Epoch [1538/4000], Loss: 0.7769\n",
      "Epoch [1539/4000], Loss: 0.6792\n",
      "Epoch [1540/4000], Loss: 0.6111\n",
      "Epoch [1541/4000], Loss: 0.4321\n",
      "Epoch [1542/4000], Loss: 0.3989\n",
      "Epoch [1543/4000], Loss: 0.4894\n",
      "Epoch [1544/4000], Loss: 0.4820\n",
      "Epoch [1545/4000], Loss: 0.4303\n",
      "Epoch [1546/4000], Loss: 0.6034\n",
      "Epoch [1547/4000], Loss: 0.7706\n",
      "Epoch [1548/4000], Loss: 0.4332\n",
      "Epoch [1549/4000], Loss: 0.3273\n",
      "Epoch [1550/4000], Loss: 0.4976\n",
      "Epoch [1551/4000], Loss: 0.4297\n",
      "Epoch [1552/4000], Loss: 0.3417\n",
      "Epoch [1553/4000], Loss: 0.5293\n",
      "Epoch [1554/4000], Loss: 0.7209\n",
      "Epoch [1555/4000], Loss: 0.6674\n",
      "Epoch [1556/4000], Loss: 0.4091\n",
      "Epoch [1557/4000], Loss: 0.3816\n",
      "Epoch [1558/4000], Loss: 0.5332\n",
      "Epoch [1559/4000], Loss: 0.3960\n",
      "Epoch [1560/4000], Loss: 0.6671\n",
      "Epoch [1561/4000], Loss: 0.5434\n",
      "Epoch [1562/4000], Loss: 0.4050\n",
      "Epoch [1563/4000], Loss: 0.6709\n",
      "Epoch [1564/4000], Loss: 0.4986\n",
      "Epoch [1565/4000], Loss: 0.6688\n",
      "Epoch [1566/4000], Loss: 0.5026\n",
      "Epoch [1567/4000], Loss: 0.5585\n",
      "Epoch [1568/4000], Loss: 0.5284\n",
      "Epoch [1569/4000], Loss: 0.7456\n",
      "Epoch [1570/4000], Loss: 0.6022\n",
      "Epoch [1571/4000], Loss: 0.4636\n",
      "Epoch [1572/4000], Loss: 0.7601\n",
      "Epoch [1573/4000], Loss: 0.4724\n",
      "Epoch [1574/4000], Loss: 0.2957\n",
      "Epoch [1575/4000], Loss: 0.5993\n",
      "Epoch [1576/4000], Loss: 0.4303\n",
      "Epoch [1577/4000], Loss: 0.3389\n",
      "Epoch [1578/4000], Loss: 0.4367\n",
      "Epoch [1579/4000], Loss: 0.6097\n",
      "Epoch [1580/4000], Loss: 0.4497\n",
      "Epoch [1581/4000], Loss: 0.6015\n",
      "Epoch [1582/4000], Loss: 0.4294\n",
      "Epoch [1583/4000], Loss: 0.5654\n",
      "Epoch [1584/4000], Loss: 0.6095\n",
      "Epoch [1585/4000], Loss: 0.4534\n",
      "Epoch [1586/4000], Loss: 0.6793\n",
      "Epoch [1587/4000], Loss: 0.3262\n",
      "Epoch [1588/4000], Loss: 0.5705\n",
      "Epoch [1589/4000], Loss: 0.4687\n",
      "Epoch [1590/4000], Loss: 0.7231\n",
      "Epoch [1591/4000], Loss: 0.4599\n",
      "Epoch [1592/4000], Loss: 0.7128\n",
      "Epoch [1593/4000], Loss: 0.7382\n",
      "Epoch [1594/4000], Loss: 0.6323\n",
      "Epoch [1595/4000], Loss: 0.4098\n",
      "Epoch [1596/4000], Loss: 0.5197\n",
      "Epoch [1597/4000], Loss: 0.5080\n",
      "Epoch [1598/4000], Loss: 0.3798\n",
      "Epoch [1599/4000], Loss: 0.3312\n",
      "Epoch [1600/4000], Loss: 0.6180\n",
      "Epoch [1601/4000], Loss: 0.5834\n",
      "Epoch [1602/4000], Loss: 0.4060\n",
      "Epoch [1603/4000], Loss: 0.4465\n",
      "Epoch [1604/4000], Loss: 0.5485\n",
      "Epoch [1605/4000], Loss: 0.5198\n",
      "Epoch [1606/4000], Loss: 0.4637\n",
      "Epoch [1607/4000], Loss: 0.4851\n",
      "Epoch [1608/4000], Loss: 0.5051\n",
      "Epoch [1609/4000], Loss: 0.4585\n",
      "Epoch [1610/4000], Loss: 0.4372\n",
      "Epoch [1611/4000], Loss: 0.5336\n",
      "Epoch [1612/4000], Loss: 0.7686\n",
      "Epoch [1613/4000], Loss: 0.4640\n",
      "Epoch [1614/4000], Loss: 0.6292\n",
      "Epoch [1615/4000], Loss: 0.5415\n",
      "Epoch [1616/4000], Loss: 0.6005\n",
      "Epoch [1617/4000], Loss: 0.3450\n",
      "Epoch [1618/4000], Loss: 0.4247\n",
      "Epoch [1619/4000], Loss: 0.4565\n",
      "Epoch [1620/4000], Loss: 0.5863\n",
      "Epoch [1621/4000], Loss: 0.3834\n",
      "Epoch [1622/4000], Loss: 0.4364\n",
      "Epoch [1623/4000], Loss: 0.5494\n",
      "Epoch [1624/4000], Loss: 0.3095\n",
      "Epoch [1625/4000], Loss: 0.6476\n",
      "Epoch [1626/4000], Loss: 0.4468\n",
      "Epoch [1627/4000], Loss: 0.4475\n",
      "Epoch [1628/4000], Loss: 0.5904\n",
      "Epoch [1629/4000], Loss: 0.4294\n",
      "Epoch [1630/4000], Loss: 0.4468\n",
      "Epoch [1631/4000], Loss: 0.5376\n",
      "Epoch [1632/4000], Loss: 0.4695\n",
      "Epoch [1633/4000], Loss: 0.4007\n",
      "Epoch [1634/4000], Loss: 0.3799\n",
      "Epoch [1635/4000], Loss: 0.4830\n",
      "Epoch [1636/4000], Loss: 0.5020\n",
      "Epoch [1637/4000], Loss: 0.3833\n",
      "Epoch [1638/4000], Loss: 0.4810\n",
      "Epoch [1639/4000], Loss: 0.5890\n",
      "Epoch [1640/4000], Loss: 0.4468\n",
      "Epoch [1641/4000], Loss: 0.5744\n",
      "Epoch [1642/4000], Loss: 0.4354\n",
      "Epoch [1643/4000], Loss: 0.4754\n",
      "Epoch [1644/4000], Loss: 0.4918\n",
      "Epoch [1645/4000], Loss: 0.4476\n",
      "Epoch [1646/4000], Loss: 0.4257\n",
      "Epoch [1647/4000], Loss: 0.3297\n",
      "Epoch [1648/4000], Loss: 0.5665\n",
      "Epoch [1649/4000], Loss: 0.4196\n",
      "Epoch [1650/4000], Loss: 0.3742\n",
      "Epoch [1651/4000], Loss: 0.4015\n",
      "Epoch [1652/4000], Loss: 0.3947\n",
      "Epoch [1653/4000], Loss: 0.6164\n",
      "Epoch [1654/4000], Loss: 0.7139\n",
      "Epoch [1655/4000], Loss: 0.5569\n",
      "Epoch [1656/4000], Loss: 0.4319\n",
      "Epoch [1657/4000], Loss: 0.5426\n",
      "Epoch [1658/4000], Loss: 0.4150\n",
      "Epoch [1659/4000], Loss: 0.7253\n",
      "Epoch [1660/4000], Loss: 0.4856\n",
      "Epoch [1661/4000], Loss: 0.6091\n",
      "Epoch [1662/4000], Loss: 0.4089\n",
      "Epoch [1663/4000], Loss: 0.4223\n",
      "Epoch [1664/4000], Loss: 0.3842\n",
      "Epoch [1665/4000], Loss: 0.5122\n",
      "Epoch [1666/4000], Loss: 0.4763\n",
      "Epoch [1667/4000], Loss: 0.6284\n",
      "Epoch [1668/4000], Loss: 0.4838\n",
      "Epoch [1669/4000], Loss: 0.5082\n",
      "Epoch [1670/4000], Loss: 0.4197\n",
      "Epoch [1671/4000], Loss: 0.4415\n",
      "Epoch [1672/4000], Loss: 0.6729\n",
      "Epoch [1673/4000], Loss: 0.5315\n",
      "Epoch [1674/4000], Loss: 0.6039\n",
      "Epoch [1675/4000], Loss: 0.5564\n",
      "Epoch [1676/4000], Loss: 0.7372\n",
      "Epoch [1677/4000], Loss: 0.1897\n",
      "Epoch [1678/4000], Loss: 0.7138\n",
      "Epoch [1679/4000], Loss: 0.4458\n",
      "Epoch [1680/4000], Loss: 0.5716\n",
      "Epoch [1681/4000], Loss: 0.4873\n",
      "Epoch [1682/4000], Loss: 0.5383\n",
      "Epoch [1683/4000], Loss: 0.5411\n",
      "Epoch [1684/4000], Loss: 0.3014\n",
      "Epoch [1685/4000], Loss: 0.3148\n",
      "Epoch [1686/4000], Loss: 0.3949\n",
      "Epoch [1687/4000], Loss: 0.4375\n",
      "Epoch [1688/4000], Loss: 0.3836\n",
      "Epoch [1689/4000], Loss: 0.4854\n",
      "Epoch [1690/4000], Loss: 0.4155\n",
      "Epoch [1691/4000], Loss: 0.4195\n",
      "Epoch [1692/4000], Loss: 0.5263\n",
      "Epoch [1693/4000], Loss: 0.6222\n",
      "Epoch [1694/4000], Loss: 0.5554\n",
      "Epoch [1695/4000], Loss: 0.6642\n",
      "Epoch [1696/4000], Loss: 0.4602\n",
      "Epoch [1697/4000], Loss: 0.4420\n",
      "Epoch [1698/4000], Loss: 0.4842\n",
      "Epoch [1699/4000], Loss: 0.5122\n",
      "Epoch [1700/4000], Loss: 0.5662\n",
      "Epoch [1701/4000], Loss: 0.4866\n",
      "Epoch [1702/4000], Loss: 0.4195\n",
      "Epoch [1703/4000], Loss: 0.4034\n",
      "Epoch [1704/4000], Loss: 0.3959\n",
      "Epoch [1705/4000], Loss: 0.4033\n",
      "Epoch [1706/4000], Loss: 0.5173\n",
      "Epoch [1707/4000], Loss: 0.4802\n",
      "Epoch [1708/4000], Loss: 0.7540\n",
      "Epoch [1709/4000], Loss: 0.5290\n",
      "Epoch [1710/4000], Loss: 0.3198\n",
      "Epoch [1711/4000], Loss: 0.3877\n",
      "Epoch [1712/4000], Loss: 0.5802\n",
      "Epoch [1713/4000], Loss: 0.4036\n",
      "Epoch [1714/4000], Loss: 0.5447\n",
      "Epoch [1715/4000], Loss: 0.3183\n",
      "Epoch [1716/4000], Loss: 0.4097\n",
      "Epoch [1717/4000], Loss: 0.7099\n",
      "Epoch [1718/4000], Loss: 0.5735\n",
      "Epoch [1719/4000], Loss: 0.4982\n",
      "Epoch [1720/4000], Loss: 0.2925\n",
      "Epoch [1721/4000], Loss: 0.3590\n",
      "Epoch [1722/4000], Loss: 0.3075\n",
      "Epoch [1723/4000], Loss: 0.3441\n",
      "Epoch [1724/4000], Loss: 0.4479\n",
      "Epoch [1725/4000], Loss: 0.4447\n",
      "Epoch [1726/4000], Loss: 0.4312\n",
      "Epoch [1727/4000], Loss: 0.5771\n",
      "Epoch [1728/4000], Loss: 0.2877\n",
      "Epoch [1729/4000], Loss: 0.5007\n",
      "Epoch [1730/4000], Loss: 0.5233\n",
      "Epoch [1731/4000], Loss: 0.2651\n",
      "Epoch [1732/4000], Loss: 0.5962\n",
      "Epoch [1733/4000], Loss: 0.4947\n",
      "Epoch [1734/4000], Loss: 0.4899\n",
      "Epoch [1735/4000], Loss: 0.4130\n",
      "Epoch [1736/4000], Loss: 0.6782\n",
      "Epoch [1737/4000], Loss: 0.4620\n",
      "Epoch [1738/4000], Loss: 0.4701\n",
      "Epoch [1739/4000], Loss: 0.3029\n",
      "Epoch [1740/4000], Loss: 0.4121\n",
      "Epoch [1741/4000], Loss: 0.6348\n",
      "Epoch [1742/4000], Loss: 0.4184\n",
      "Epoch [1743/4000], Loss: 0.3576\n",
      "Epoch [1744/4000], Loss: 0.6229\n",
      "Epoch [1745/4000], Loss: 0.5730\n",
      "Epoch [1746/4000], Loss: 0.3629\n",
      "Epoch [1747/4000], Loss: 0.5542\n",
      "Epoch [1748/4000], Loss: 0.4914\n",
      "Epoch [1749/4000], Loss: 0.4072\n",
      "Epoch [1750/4000], Loss: 0.4162\n",
      "Epoch [1751/4000], Loss: 0.3499\n",
      "Epoch [1752/4000], Loss: 0.4100\n",
      "Epoch [1753/4000], Loss: 0.5908\n",
      "Epoch [1754/4000], Loss: 0.2663\n",
      "Epoch [1755/4000], Loss: 0.3237\n",
      "Epoch [1756/4000], Loss: 0.4042\n",
      "Epoch [1757/4000], Loss: 0.4785\n",
      "Epoch [1758/4000], Loss: 0.5326\n",
      "Epoch [1759/4000], Loss: 0.5767\n",
      "Epoch [1760/4000], Loss: 0.4405\n",
      "Epoch [1761/4000], Loss: 0.4421\n",
      "Epoch [1762/4000], Loss: 0.2840\n",
      "Epoch [1763/4000], Loss: 0.6024\n",
      "Epoch [1764/4000], Loss: 0.7084\n",
      "Epoch [1765/4000], Loss: 0.4736\n",
      "Epoch [1766/4000], Loss: 0.6658\n",
      "Epoch [1767/4000], Loss: 0.4774\n",
      "Epoch [1768/4000], Loss: 0.4059\n",
      "Epoch [1769/4000], Loss: 0.4015\n",
      "Epoch [1770/4000], Loss: 0.4011\n",
      "Epoch [1771/4000], Loss: 0.3517\n",
      "Epoch [1772/4000], Loss: 0.6329\n",
      "Epoch [1773/4000], Loss: 0.4975\n",
      "Epoch [1774/4000], Loss: 0.4385\n",
      "Epoch [1775/4000], Loss: 0.7847\n",
      "Epoch [1776/4000], Loss: 0.5857\n",
      "Epoch [1777/4000], Loss: 0.4073\n",
      "Epoch [1778/4000], Loss: 0.3501\n",
      "Epoch [1779/4000], Loss: 0.6436\n",
      "Epoch [1780/4000], Loss: 0.5927\n",
      "Epoch [1781/4000], Loss: 0.4418\n",
      "Epoch [1782/4000], Loss: 0.4728\n",
      "Epoch [1783/4000], Loss: 0.5281\n",
      "Epoch [1784/4000], Loss: 0.4670\n",
      "Epoch [1785/4000], Loss: 0.5325\n",
      "Epoch [1786/4000], Loss: 0.5783\n",
      "Epoch [1787/4000], Loss: 0.3178\n",
      "Epoch [1788/4000], Loss: 0.4218\n",
      "Epoch [1789/4000], Loss: 0.4137\n",
      "Epoch [1790/4000], Loss: 0.5112\n",
      "Epoch [1791/4000], Loss: 0.3932\n",
      "Epoch [1792/4000], Loss: 0.4500\n",
      "Epoch [1793/4000], Loss: 0.4841\n",
      "Epoch [1794/4000], Loss: 0.6183\n",
      "Epoch [1795/4000], Loss: 0.3370\n",
      "Epoch [1796/4000], Loss: 0.4009\n",
      "Epoch [1797/4000], Loss: 0.4048\n",
      "Epoch [1798/4000], Loss: 0.7508\n",
      "Epoch [1799/4000], Loss: 0.5912\n",
      "Epoch [1800/4000], Loss: 0.4095\n",
      "Epoch [1801/4000], Loss: 0.5197\n",
      "Epoch [1802/4000], Loss: 0.5191\n",
      "Epoch [1803/4000], Loss: 0.6641\n",
      "Epoch [1804/4000], Loss: 0.7839\n",
      "Epoch [1805/4000], Loss: 0.4794\n",
      "Epoch [1806/4000], Loss: 0.4025\n",
      "Epoch [1807/4000], Loss: 0.4242\n",
      "Epoch [1808/4000], Loss: 0.3930\n",
      "Epoch [1809/4000], Loss: 0.5993\n",
      "Epoch [1810/4000], Loss: 0.5159\n",
      "Epoch [1811/4000], Loss: 0.5977\n",
      "Epoch [1812/4000], Loss: 0.4490\n",
      "Epoch [1813/4000], Loss: 0.4070\n",
      "Epoch [1814/4000], Loss: 0.4022\n",
      "Epoch [1815/4000], Loss: 0.5308\n",
      "Epoch [1816/4000], Loss: 0.4634\n",
      "Epoch [1817/4000], Loss: 0.3967\n",
      "Epoch [1818/4000], Loss: 0.5850\n",
      "Epoch [1819/4000], Loss: 0.6225\n",
      "Epoch [1820/4000], Loss: 0.6810\n",
      "Epoch [1821/4000], Loss: 0.6582\n",
      "Epoch [1822/4000], Loss: 0.6241\n",
      "Epoch [1823/4000], Loss: 0.3806\n",
      "Epoch [1824/4000], Loss: 0.2644\n",
      "Epoch [1825/4000], Loss: 0.3190\n",
      "Epoch [1826/4000], Loss: 0.4054\n",
      "Epoch [1827/4000], Loss: 0.3256\n",
      "Epoch [1828/4000], Loss: 0.5586\n",
      "Epoch [1829/4000], Loss: 0.3847\n",
      "Epoch [1830/4000], Loss: 0.4665\n",
      "Epoch [1831/4000], Loss: 0.3873\n",
      "Epoch [1832/4000], Loss: 0.5074\n",
      "Epoch [1833/4000], Loss: 0.5157\n",
      "Epoch [1834/4000], Loss: 0.4840\n",
      "Epoch [1835/4000], Loss: 0.4542\n",
      "Epoch [1836/4000], Loss: 0.2630\n",
      "Epoch [1837/4000], Loss: 0.4780\n",
      "Epoch [1838/4000], Loss: 0.8750\n",
      "Epoch [1839/4000], Loss: 0.5112\n",
      "Epoch [1840/4000], Loss: 0.5535\n",
      "Epoch [1841/4000], Loss: 0.6667\n",
      "Epoch [1842/4000], Loss: 0.5215\n",
      "Epoch [1843/4000], Loss: 0.2630\n",
      "Epoch [1844/4000], Loss: 0.4390\n",
      "Epoch [1845/4000], Loss: 0.4265\n",
      "Epoch [1846/4000], Loss: 0.4191\n",
      "Epoch [1847/4000], Loss: 0.4085\n",
      "Epoch [1848/4000], Loss: 0.4298\n",
      "Epoch [1849/4000], Loss: 0.5431\n",
      "Epoch [1850/4000], Loss: 0.7100\n",
      "Epoch [1851/4000], Loss: 0.4982\n",
      "Epoch [1852/4000], Loss: 0.5370\n",
      "Epoch [1853/4000], Loss: 0.6376\n",
      "Epoch [1854/4000], Loss: 0.3242\n",
      "Epoch [1855/4000], Loss: 0.3243\n",
      "Epoch [1856/4000], Loss: 0.6129\n",
      "Epoch [1857/4000], Loss: 0.2840\n",
      "Epoch [1858/4000], Loss: 0.3873\n",
      "Epoch [1859/4000], Loss: 0.4570\n",
      "Epoch [1860/4000], Loss: 0.6550\n",
      "Epoch [1861/4000], Loss: 0.6546\n",
      "Epoch [1862/4000], Loss: 0.5136\n",
      "Epoch [1863/4000], Loss: 0.3124\n",
      "Epoch [1864/4000], Loss: 0.3609\n",
      "Epoch [1865/4000], Loss: 0.5468\n",
      "Epoch [1866/4000], Loss: 0.3202\n",
      "Epoch [1867/4000], Loss: 0.5097\n",
      "Epoch [1868/4000], Loss: 0.2947\n",
      "Epoch [1869/4000], Loss: 0.3262\n",
      "Epoch [1870/4000], Loss: 0.4792\n",
      "Epoch [1871/4000], Loss: 0.5026\n",
      "Epoch [1872/4000], Loss: 0.4544\n",
      "Epoch [1873/4000], Loss: 0.4666\n",
      "Epoch [1874/4000], Loss: 0.4146\n",
      "Epoch [1875/4000], Loss: 0.4056\n",
      "Epoch [1876/4000], Loss: 0.6246\n",
      "Epoch [1877/4000], Loss: 0.5574\n",
      "Epoch [1878/4000], Loss: 0.3340\n",
      "Epoch [1879/4000], Loss: 0.5033\n",
      "Epoch [1880/4000], Loss: 0.6650\n",
      "Epoch [1881/4000], Loss: 0.4962\n",
      "Epoch [1882/4000], Loss: 0.3170\n",
      "Epoch [1883/4000], Loss: 0.4725\n",
      "Epoch [1884/4000], Loss: 0.3231\n",
      "Epoch [1885/4000], Loss: 0.5645\n",
      "Epoch [1886/4000], Loss: 0.4059\n",
      "Epoch [1887/4000], Loss: 0.3186\n",
      "Epoch [1888/4000], Loss: 0.4701\n",
      "Epoch [1889/4000], Loss: 0.6214\n",
      "Epoch [1890/4000], Loss: 0.8425\n",
      "Epoch [1891/4000], Loss: 0.4196\n",
      "Epoch [1892/4000], Loss: 0.7560\n",
      "Epoch [1893/4000], Loss: 0.7533\n",
      "Epoch [1894/4000], Loss: 0.3617\n",
      "Epoch [1895/4000], Loss: 0.5143\n",
      "Epoch [1896/4000], Loss: 0.3309\n",
      "Epoch [1897/4000], Loss: 0.4209\n",
      "Epoch [1898/4000], Loss: 0.3531\n",
      "Epoch [1899/4000], Loss: 0.4358\n",
      "Epoch [1900/4000], Loss: 0.4361\n",
      "Epoch [1901/4000], Loss: 0.3355\n",
      "Epoch [1902/4000], Loss: 0.3750\n",
      "Epoch [1903/4000], Loss: 0.5359\n",
      "Epoch [1904/4000], Loss: 0.4137\n",
      "Epoch [1905/4000], Loss: 0.4560\n",
      "Epoch [1906/4000], Loss: 0.4715\n",
      "Epoch [1907/4000], Loss: 0.3809\n",
      "Epoch [1908/4000], Loss: 0.4360\n",
      "Epoch [1909/4000], Loss: 0.4866\n",
      "Epoch [1910/4000], Loss: 0.4458\n",
      "Epoch [1911/4000], Loss: 0.4071\n",
      "Epoch [1912/4000], Loss: 0.4459\n",
      "Epoch [1913/4000], Loss: 0.3954\n",
      "Epoch [1914/4000], Loss: 0.6161\n",
      "Epoch [1915/4000], Loss: 0.3272\n",
      "Epoch [1916/4000], Loss: 0.4598\n",
      "Epoch [1917/4000], Loss: 0.3664\n",
      "Epoch [1918/4000], Loss: 0.6201\n",
      "Epoch [1919/4000], Loss: 0.3476\n",
      "Epoch [1920/4000], Loss: 0.4636\n",
      "Epoch [1921/4000], Loss: 0.3865\n",
      "Epoch [1922/4000], Loss: 0.2845\n",
      "Epoch [1923/4000], Loss: 0.5321\n",
      "Epoch [1924/4000], Loss: 0.3995\n",
      "Epoch [1925/4000], Loss: 0.4406\n",
      "Epoch [1926/4000], Loss: 0.4257\n",
      "Epoch [1927/4000], Loss: 0.4743\n",
      "Epoch [1928/4000], Loss: 0.3727\n",
      "Epoch [1929/4000], Loss: 0.6080\n",
      "Epoch [1930/4000], Loss: 0.4984\n",
      "Epoch [1931/4000], Loss: 0.6380\n",
      "Epoch [1932/4000], Loss: 0.4347\n",
      "Epoch [1933/4000], Loss: 0.4096\n",
      "Epoch [1934/4000], Loss: 0.4100\n",
      "Epoch [1935/4000], Loss: 0.6970\n",
      "Epoch [1936/4000], Loss: 0.5869\n",
      "Epoch [1937/4000], Loss: 0.5277\n",
      "Epoch [1938/4000], Loss: 0.5574\n",
      "Epoch [1939/4000], Loss: 0.5552\n",
      "Epoch [1940/4000], Loss: 0.4183\n",
      "Epoch [1941/4000], Loss: 0.3326\n",
      "Epoch [1942/4000], Loss: 0.5211\n",
      "Epoch [1943/4000], Loss: 0.5533\n",
      "Epoch [1944/4000], Loss: 0.3571\n",
      "Epoch [1945/4000], Loss: 0.4880\n",
      "Epoch [1946/4000], Loss: 0.5661\n",
      "Epoch [1947/4000], Loss: 0.4275\n",
      "Epoch [1948/4000], Loss: 0.3142\n",
      "Epoch [1949/4000], Loss: 0.4052\n",
      "Epoch [1950/4000], Loss: 0.8653\n",
      "Epoch [1951/4000], Loss: 0.5565\n",
      "Epoch [1952/4000], Loss: 0.4882\n",
      "Epoch [1953/4000], Loss: 0.2833\n",
      "Epoch [1954/4000], Loss: 0.3839\n",
      "Epoch [1955/4000], Loss: 0.4740\n",
      "Epoch [1956/4000], Loss: 0.3762\n",
      "Epoch [1957/4000], Loss: 0.3851\n",
      "Epoch [1958/4000], Loss: 0.5417\n",
      "Epoch [1959/4000], Loss: 0.3221\n",
      "Epoch [1960/4000], Loss: 0.3941\n",
      "Epoch [1961/4000], Loss: 0.4716\n",
      "Epoch [1962/4000], Loss: 0.5055\n",
      "Epoch [1963/4000], Loss: 0.4628\n",
      "Epoch [1964/4000], Loss: 0.2424\n",
      "Epoch [1965/4000], Loss: 0.5141\n",
      "Epoch [1966/4000], Loss: 0.4022\n",
      "Epoch [1967/4000], Loss: 0.6748\n",
      "Epoch [1968/4000], Loss: 0.4405\n",
      "Epoch [1969/4000], Loss: 0.3004\n",
      "Epoch [1970/4000], Loss: 0.4484\n",
      "Epoch [1971/4000], Loss: 0.4384\n",
      "Epoch [1972/4000], Loss: 0.5143\n",
      "Epoch [1973/4000], Loss: 0.4401\n",
      "Epoch [1974/4000], Loss: 0.4016\n",
      "Epoch [1975/4000], Loss: 0.5371\n",
      "Epoch [1976/4000], Loss: 0.5457\n",
      "Epoch [1977/4000], Loss: 0.3224\n",
      "Epoch [1978/4000], Loss: 0.3756\n",
      "Epoch [1979/4000], Loss: 0.2984\n",
      "Epoch [1980/4000], Loss: 0.4977\n",
      "Epoch [1981/4000], Loss: 0.2058\n",
      "Epoch [1982/4000], Loss: 0.4478\n",
      "Epoch [1983/4000], Loss: 0.4423\n",
      "Epoch [1984/4000], Loss: 0.4476\n",
      "Epoch [1985/4000], Loss: 0.4012\n",
      "Epoch [1986/4000], Loss: 0.5696\n",
      "Epoch [1987/4000], Loss: 0.4914\n",
      "Epoch [1988/4000], Loss: 0.3928\n",
      "Epoch [1989/4000], Loss: 0.3222\n",
      "Epoch [1990/4000], Loss: 0.2759\n",
      "Epoch [1991/4000], Loss: 0.4920\n",
      "Epoch [1992/4000], Loss: 0.3607\n",
      "Epoch [1993/4000], Loss: 0.4287\n",
      "Epoch [1994/4000], Loss: 0.5969\n",
      "Epoch [1995/4000], Loss: 0.5074\n",
      "Epoch [1996/4000], Loss: 0.3311\n",
      "Epoch [1997/4000], Loss: 0.4932\n",
      "Epoch [1998/4000], Loss: 0.4799\n",
      "Epoch [1999/4000], Loss: 0.5411\n",
      "Epoch [2000/4000], Loss: 0.3340\n",
      "Epoch [2001/4000], Loss: 0.4625\n",
      "Epoch [2002/4000], Loss: 0.2470\n",
      "Epoch [2003/4000], Loss: 0.5252\n",
      "Epoch [2004/4000], Loss: 0.5035\n",
      "Epoch [2005/4000], Loss: 0.4185\n",
      "Epoch [2006/4000], Loss: 0.5191\n",
      "Epoch [2007/4000], Loss: 0.3391\n",
      "Epoch [2008/4000], Loss: 0.3066\n",
      "Epoch [2009/4000], Loss: 0.3478\n",
      "Epoch [2010/4000], Loss: 0.3306\n",
      "Epoch [2011/4000], Loss: 0.5294\n",
      "Epoch [2012/4000], Loss: 0.3933\n",
      "Epoch [2013/4000], Loss: 0.3947\n",
      "Epoch [2014/4000], Loss: 0.2818\n",
      "Epoch [2015/4000], Loss: 0.3231\n",
      "Epoch [2016/4000], Loss: 0.3401\n",
      "Epoch [2017/4000], Loss: 0.5151\n",
      "Epoch [2018/4000], Loss: 0.2787\n",
      "Epoch [2019/4000], Loss: 0.4237\n",
      "Epoch [2020/4000], Loss: 0.3177\n",
      "Epoch [2021/4000], Loss: 0.2383\n",
      "Epoch [2022/4000], Loss: 0.4239\n",
      "Epoch [2023/4000], Loss: 0.3871\n",
      "Epoch [2024/4000], Loss: 0.4711\n",
      "Epoch [2025/4000], Loss: 0.4312\n",
      "Epoch [2026/4000], Loss: 0.3848\n",
      "Epoch [2027/4000], Loss: 0.4487\n",
      "Epoch [2028/4000], Loss: 0.3891\n",
      "Epoch [2029/4000], Loss: 0.3524\n",
      "Epoch [2030/4000], Loss: 0.4940\n",
      "Epoch [2031/4000], Loss: 0.3903\n",
      "Epoch [2032/4000], Loss: 0.3045\n",
      "Epoch [2033/4000], Loss: 0.3103\n",
      "Epoch [2034/4000], Loss: 0.2284\n",
      "Epoch [2035/4000], Loss: 0.3302\n",
      "Epoch [2036/4000], Loss: 0.3534\n",
      "Epoch [2037/4000], Loss: 0.2565\n",
      "Epoch [2038/4000], Loss: 0.3668\n",
      "Epoch [2039/4000], Loss: 0.4027\n",
      "Epoch [2040/4000], Loss: 0.7729\n",
      "Epoch [2041/4000], Loss: 0.4114\n",
      "Epoch [2042/4000], Loss: 0.3166\n",
      "Epoch [2043/4000], Loss: 0.5514\n",
      "Epoch [2044/4000], Loss: 0.3989\n",
      "Epoch [2045/4000], Loss: 0.5624\n",
      "Epoch [2046/4000], Loss: 0.3520\n",
      "Epoch [2047/4000], Loss: 0.2371\n",
      "Epoch [2048/4000], Loss: 0.2822\n",
      "Epoch [2049/4000], Loss: 0.3583\n",
      "Epoch [2050/4000], Loss: 0.2461\n",
      "Epoch [2051/4000], Loss: 0.3293\n",
      "Epoch [2052/4000], Loss: 0.4806\n",
      "Epoch [2053/4000], Loss: 0.3280\n",
      "Epoch [2054/4000], Loss: 0.3401\n",
      "Epoch [2055/4000], Loss: 0.2968\n",
      "Epoch [2056/4000], Loss: 0.3902\n",
      "Epoch [2057/4000], Loss: 0.6095\n",
      "Epoch [2058/4000], Loss: 0.5098\n",
      "Epoch [2059/4000], Loss: 0.4787\n",
      "Epoch [2060/4000], Loss: 0.3380\n",
      "Epoch [2061/4000], Loss: 0.3237\n",
      "Epoch [2062/4000], Loss: 0.2318\n",
      "Epoch [2063/4000], Loss: 0.4717\n",
      "Epoch [2064/4000], Loss: 0.3842\n",
      "Epoch [2065/4000], Loss: 0.6310\n",
      "Epoch [2066/4000], Loss: 0.2625\n",
      "Epoch [2067/4000], Loss: 0.4469\n",
      "Epoch [2068/4000], Loss: 0.2700\n",
      "Epoch [2069/4000], Loss: 0.4927\n",
      "Epoch [2070/4000], Loss: 0.5085\n",
      "Epoch [2071/4000], Loss: 0.4065\n",
      "Epoch [2072/4000], Loss: 0.4816\n",
      "Epoch [2073/4000], Loss: 0.4756\n",
      "Epoch [2074/4000], Loss: 0.2108\n",
      "Epoch [2075/4000], Loss: 0.4553\n",
      "Epoch [2076/4000], Loss: 0.8241\n",
      "Epoch [2077/4000], Loss: 0.3704\n",
      "Epoch [2078/4000], Loss: 0.6958\n",
      "Epoch [2079/4000], Loss: 0.3116\n",
      "Epoch [2080/4000], Loss: 0.3901\n",
      "Epoch [2081/4000], Loss: 0.3233\n",
      "Epoch [2082/4000], Loss: 0.3404\n",
      "Epoch [2083/4000], Loss: 0.4732\n",
      "Epoch [2084/4000], Loss: 0.3815\n",
      "Epoch [2085/4000], Loss: 0.6907\n",
      "Epoch [2086/4000], Loss: 0.3812\n",
      "Epoch [2087/4000], Loss: 0.2968\n",
      "Epoch [2088/4000], Loss: 0.5109\n",
      "Epoch [2089/4000], Loss: 0.5285\n",
      "Epoch [2090/4000], Loss: 0.5029\n",
      "Epoch [2091/4000], Loss: 0.4644\n",
      "Epoch [2092/4000], Loss: 0.3386\n",
      "Epoch [2093/4000], Loss: 0.4256\n",
      "Epoch [2094/4000], Loss: 0.3190\n",
      "Epoch [2095/4000], Loss: 0.5908\n",
      "Epoch [2096/4000], Loss: 0.3435\n",
      "Epoch [2097/4000], Loss: 0.3653\n",
      "Epoch [2098/4000], Loss: 0.2968\n",
      "Epoch [2099/4000], Loss: 0.3066\n",
      "Epoch [2100/4000], Loss: 0.6614\n",
      "Epoch [2101/4000], Loss: 0.4489\n",
      "Epoch [2102/4000], Loss: 0.4827\n",
      "Epoch [2103/4000], Loss: 0.3377\n",
      "Epoch [2104/4000], Loss: 0.3491\n",
      "Epoch [2105/4000], Loss: 0.4611\n",
      "Epoch [2106/4000], Loss: 0.3466\n",
      "Epoch [2107/4000], Loss: 0.5370\n",
      "Epoch [2108/4000], Loss: 0.3066\n",
      "Epoch [2109/4000], Loss: 0.5896\n",
      "Epoch [2110/4000], Loss: 0.4132\n",
      "Epoch [2111/4000], Loss: 0.4240\n",
      "Epoch [2112/4000], Loss: 0.2596\n",
      "Epoch [2113/4000], Loss: 0.2948\n",
      "Epoch [2114/4000], Loss: 0.4225\n",
      "Epoch [2115/4000], Loss: 0.3187\n",
      "Epoch [2116/4000], Loss: 0.3224\n",
      "Epoch [2117/4000], Loss: 0.5537\n",
      "Epoch [2118/4000], Loss: 0.2719\n",
      "Epoch [2119/4000], Loss: 0.2894\n",
      "Epoch [2120/4000], Loss: 0.2810\n",
      "Epoch [2121/4000], Loss: 0.5338\n",
      "Epoch [2122/4000], Loss: 0.4105\n",
      "Epoch [2123/4000], Loss: 0.3224\n",
      "Epoch [2124/4000], Loss: 0.4307\n",
      "Epoch [2125/4000], Loss: 0.3122\n",
      "Epoch [2126/4000], Loss: 0.4277\n",
      "Epoch [2127/4000], Loss: 0.5038\n",
      "Epoch [2128/4000], Loss: 0.3952\n",
      "Epoch [2129/4000], Loss: 0.2643\n",
      "Epoch [2130/4000], Loss: 0.3051\n",
      "Epoch [2131/4000], Loss: 0.5029\n",
      "Epoch [2132/4000], Loss: 0.4232\n",
      "Epoch [2133/4000], Loss: 0.3368\n",
      "Epoch [2134/4000], Loss: 0.3694\n",
      "Epoch [2135/4000], Loss: 0.1901\n",
      "Epoch [2136/4000], Loss: 0.5035\n",
      "Epoch [2137/4000], Loss: 0.3360\n",
      "Epoch [2138/4000], Loss: 0.2929\n",
      "Epoch [2139/4000], Loss: 0.2400\n",
      "Epoch [2140/4000], Loss: 0.3327\n",
      "Epoch [2141/4000], Loss: 0.3680\n",
      "Epoch [2142/4000], Loss: 0.3879\n",
      "Epoch [2143/4000], Loss: 0.5053\n",
      "Epoch [2144/4000], Loss: 0.5135\n",
      "Epoch [2145/4000], Loss: 0.2767\n",
      "Epoch [2146/4000], Loss: 0.3665\n",
      "Epoch [2147/4000], Loss: 0.4491\n",
      "Epoch [2148/4000], Loss: 0.3366\n",
      "Epoch [2149/4000], Loss: 0.3472\n",
      "Epoch [2150/4000], Loss: 0.4202\n",
      "Epoch [2151/4000], Loss: 0.4831\n",
      "Epoch [2152/4000], Loss: 0.5328\n",
      "Epoch [2153/4000], Loss: 0.3824\n",
      "Epoch [2154/4000], Loss: 0.3339\n",
      "Epoch [2155/4000], Loss: 0.3762\n",
      "Epoch [2156/4000], Loss: 0.5168\n",
      "Epoch [2157/4000], Loss: 0.3828\n",
      "Epoch [2158/4000], Loss: 0.4740\n",
      "Epoch [2159/4000], Loss: 0.3351\n",
      "Epoch [2160/4000], Loss: 0.5551\n",
      "Epoch [2161/4000], Loss: 0.4667\n",
      "Epoch [2162/4000], Loss: 0.3993\n",
      "Epoch [2163/4000], Loss: 0.2602\n",
      "Epoch [2164/4000], Loss: 0.5644\n",
      "Epoch [2165/4000], Loss: 0.3475\n",
      "Epoch [2166/4000], Loss: 0.3584\n",
      "Epoch [2167/4000], Loss: 0.3216\n",
      "Epoch [2168/4000], Loss: 0.6411\n",
      "Epoch [2169/4000], Loss: 0.5545\n",
      "Epoch [2170/4000], Loss: 0.3941\n",
      "Epoch [2171/4000], Loss: 0.4496\n",
      "Epoch [2172/4000], Loss: 0.3660\n",
      "Epoch [2173/4000], Loss: 0.4295\n",
      "Epoch [2174/4000], Loss: 0.4246\n",
      "Epoch [2175/4000], Loss: 0.5123\n",
      "Epoch [2176/4000], Loss: 0.3220\n",
      "Epoch [2177/4000], Loss: 0.3773\n",
      "Epoch [2178/4000], Loss: 0.4316\n",
      "Epoch [2179/4000], Loss: 0.2245\n",
      "Epoch [2180/4000], Loss: 0.4364\n",
      "Epoch [2181/4000], Loss: 0.2612\n",
      "Epoch [2182/4000], Loss: 0.2837\n",
      "Epoch [2183/4000], Loss: 0.5994\n",
      "Epoch [2184/4000], Loss: 0.3794\n",
      "Epoch [2185/4000], Loss: 0.4252\n",
      "Epoch [2186/4000], Loss: 0.4126\n",
      "Epoch [2187/4000], Loss: 0.5308\n",
      "Epoch [2188/4000], Loss: 0.5537\n",
      "Epoch [2189/4000], Loss: 0.5041\n",
      "Epoch [2190/4000], Loss: 0.6308\n",
      "Epoch [2191/4000], Loss: 0.3609\n",
      "Epoch [2192/4000], Loss: 0.5033\n",
      "Epoch [2193/4000], Loss: 0.3185\n",
      "Epoch [2194/4000], Loss: 0.3859\n",
      "Epoch [2195/4000], Loss: 0.3854\n",
      "Epoch [2196/4000], Loss: 0.3424\n",
      "Epoch [2197/4000], Loss: 0.4790\n",
      "Epoch [2198/4000], Loss: 0.5133\n",
      "Epoch [2199/4000], Loss: 0.2786\n",
      "Epoch [2200/4000], Loss: 0.5992\n",
      "Epoch [2201/4000], Loss: 0.3352\n",
      "Epoch [2202/4000], Loss: 0.3266\n",
      "Epoch [2203/4000], Loss: 0.6347\n",
      "Epoch [2204/4000], Loss: 0.3796\n",
      "Epoch [2205/4000], Loss: 0.2653\n",
      "Epoch [2206/4000], Loss: 0.3293\n",
      "Epoch [2207/4000], Loss: 0.4188\n",
      "Epoch [2208/4000], Loss: 0.3180\n",
      "Epoch [2209/4000], Loss: 0.2222\n",
      "Epoch [2210/4000], Loss: 0.2478\n",
      "Epoch [2211/4000], Loss: 0.4656\n",
      "Epoch [2212/4000], Loss: 0.2670\n",
      "Epoch [2213/4000], Loss: 0.3176\n",
      "Epoch [2214/4000], Loss: 0.5398\n",
      "Epoch [2215/4000], Loss: 0.3173\n",
      "Epoch [2216/4000], Loss: 0.3932\n",
      "Epoch [2217/4000], Loss: 0.3709\n",
      "Epoch [2218/4000], Loss: 0.4324\n",
      "Epoch [2219/4000], Loss: 0.4466\n",
      "Epoch [2220/4000], Loss: 0.6232\n",
      "Epoch [2221/4000], Loss: 0.4645\n",
      "Epoch [2222/4000], Loss: 0.4036\n",
      "Epoch [2223/4000], Loss: 0.2660\n",
      "Epoch [2224/4000], Loss: 0.2743\n",
      "Epoch [2225/4000], Loss: 0.3967\n",
      "Epoch [2226/4000], Loss: 0.3228\n",
      "Epoch [2227/4000], Loss: 0.4594\n",
      "Epoch [2228/4000], Loss: 0.2608\n",
      "Epoch [2229/4000], Loss: 0.2721\n",
      "Epoch [2230/4000], Loss: 0.3525\n",
      "Epoch [2231/4000], Loss: 0.3392\n",
      "Epoch [2232/4000], Loss: 0.3605\n",
      "Epoch [2233/4000], Loss: 0.2905\n",
      "Epoch [2234/4000], Loss: 0.3464\n",
      "Epoch [2235/4000], Loss: 0.2782\n",
      "Epoch [2236/4000], Loss: 0.3334\n",
      "Epoch [2237/4000], Loss: 0.4742\n",
      "Epoch [2238/4000], Loss: 0.2708\n",
      "Epoch [2239/4000], Loss: 0.3888\n",
      "Epoch [2240/4000], Loss: 0.3748\n",
      "Epoch [2241/4000], Loss: 0.3372\n",
      "Epoch [2242/4000], Loss: 0.3802\n",
      "Epoch [2243/4000], Loss: 0.2664\n",
      "Epoch [2244/4000], Loss: 0.4289\n",
      "Epoch [2245/4000], Loss: 0.3240\n",
      "Epoch [2246/4000], Loss: 0.3165\n",
      "Epoch [2247/4000], Loss: 0.6205\n",
      "Epoch [2248/4000], Loss: 0.4330\n",
      "Epoch [2249/4000], Loss: 0.3286\n",
      "Epoch [2250/4000], Loss: 0.3047\n",
      "Epoch [2251/4000], Loss: 0.4183\n",
      "Epoch [2252/4000], Loss: 0.3237\n",
      "Epoch [2253/4000], Loss: 0.5649\n",
      "Epoch [2254/4000], Loss: 0.2899\n",
      "Epoch [2255/4000], Loss: 0.3843\n",
      "Epoch [2256/4000], Loss: 0.3742\n",
      "Epoch [2257/4000], Loss: 0.3516\n",
      "Epoch [2258/4000], Loss: 0.3106\n",
      "Epoch [2259/4000], Loss: 0.4579\n",
      "Epoch [2260/4000], Loss: 0.3369\n",
      "Epoch [2261/4000], Loss: 0.5168\n",
      "Epoch [2262/4000], Loss: 0.3629\n",
      "Epoch [2263/4000], Loss: 0.3871\n",
      "Epoch [2264/4000], Loss: 0.2322\n",
      "Epoch [2265/4000], Loss: 0.2586\n",
      "Epoch [2266/4000], Loss: 0.3096\n",
      "Epoch [2267/4000], Loss: 0.2141\n",
      "Epoch [2268/4000], Loss: 0.4342\n",
      "Epoch [2269/4000], Loss: 0.4894\n",
      "Epoch [2270/4000], Loss: 0.3586\n",
      "Epoch [2271/4000], Loss: 0.5302\n",
      "Epoch [2272/4000], Loss: 0.3923\n",
      "Epoch [2273/4000], Loss: 0.3600\n",
      "Epoch [2274/4000], Loss: 0.6038\n",
      "Epoch [2275/4000], Loss: 0.5133\n",
      "Epoch [2276/4000], Loss: 0.2435\n",
      "Epoch [2277/4000], Loss: 0.3286\n",
      "Epoch [2278/4000], Loss: 0.4684\n",
      "Epoch [2279/4000], Loss: 0.3527\n",
      "Epoch [2280/4000], Loss: 0.4148\n",
      "Epoch [2281/4000], Loss: 0.3263\n",
      "Epoch [2282/4000], Loss: 0.5149\n",
      "Epoch [2283/4000], Loss: 0.3819\n",
      "Epoch [2284/4000], Loss: 0.4768\n",
      "Epoch [2285/4000], Loss: 0.2697\n",
      "Epoch [2286/4000], Loss: 0.2866\n",
      "Epoch [2287/4000], Loss: 0.5868\n",
      "Epoch [2288/4000], Loss: 0.3769\n",
      "Epoch [2289/4000], Loss: 0.4117\n",
      "Epoch [2290/4000], Loss: 0.4909\n",
      "Epoch [2291/4000], Loss: 0.3235\n",
      "Epoch [2292/4000], Loss: 0.3172\n",
      "Epoch [2293/4000], Loss: 0.3511\n",
      "Epoch [2294/4000], Loss: 0.5369\n",
      "Epoch [2295/4000], Loss: 0.4610\n",
      "Epoch [2296/4000], Loss: 0.2318\n",
      "Epoch [2297/4000], Loss: 0.4228\n",
      "Epoch [2298/4000], Loss: 0.2943\n",
      "Epoch [2299/4000], Loss: 0.2993\n",
      "Epoch [2300/4000], Loss: 0.3597\n",
      "Epoch [2301/4000], Loss: 0.4477\n",
      "Epoch [2302/4000], Loss: 0.3300\n",
      "Epoch [2303/4000], Loss: 0.3850\n",
      "Epoch [2304/4000], Loss: 0.3279\n",
      "Epoch [2305/4000], Loss: 0.3063\n",
      "Epoch [2306/4000], Loss: 0.3029\n",
      "Epoch [2307/4000], Loss: 0.4508\n",
      "Epoch [2308/4000], Loss: 0.3122\n",
      "Epoch [2309/4000], Loss: 0.4384\n",
      "Epoch [2310/4000], Loss: 0.2802\n",
      "Epoch [2311/4000], Loss: 0.3919\n",
      "Epoch [2312/4000], Loss: 0.2327\n",
      "Epoch [2313/4000], Loss: 0.4884\n",
      "Epoch [2314/4000], Loss: 0.4995\n",
      "Epoch [2315/4000], Loss: 0.3864\n",
      "Epoch [2316/4000], Loss: 0.2871\n",
      "Epoch [2317/4000], Loss: 0.3026\n",
      "Epoch [2318/4000], Loss: 0.4574\n",
      "Epoch [2319/4000], Loss: 0.3143\n",
      "Epoch [2320/4000], Loss: 0.4173\n",
      "Epoch [2321/4000], Loss: 0.3412\n",
      "Epoch [2322/4000], Loss: 0.3878\n",
      "Epoch [2323/4000], Loss: 0.3420\n",
      "Epoch [2324/4000], Loss: 0.6006\n",
      "Epoch [2325/4000], Loss: 0.3179\n",
      "Epoch [2326/4000], Loss: 0.3819\n",
      "Epoch [2327/4000], Loss: 0.3918\n",
      "Epoch [2328/4000], Loss: 0.2786\n",
      "Epoch [2329/4000], Loss: 0.3505\n",
      "Epoch [2330/4000], Loss: 0.3835\n",
      "Epoch [2331/4000], Loss: 0.3762\n",
      "Epoch [2332/4000], Loss: 0.2718\n",
      "Epoch [2333/4000], Loss: 0.4924\n",
      "Epoch [2334/4000], Loss: 0.3865\n",
      "Epoch [2335/4000], Loss: 0.3365\n",
      "Epoch [2336/4000], Loss: 0.3062\n",
      "Epoch [2337/4000], Loss: 0.2687\n",
      "Epoch [2338/4000], Loss: 0.3941\n",
      "Epoch [2339/4000], Loss: 0.3318\n",
      "Epoch [2340/4000], Loss: 0.2994\n",
      "Epoch [2341/4000], Loss: 0.4522\n",
      "Epoch [2342/4000], Loss: 0.2427\n",
      "Epoch [2343/4000], Loss: 0.2973\n",
      "Epoch [2344/4000], Loss: 0.3282\n",
      "Epoch [2345/4000], Loss: 0.3911\n",
      "Epoch [2346/4000], Loss: 0.3296\n",
      "Epoch [2347/4000], Loss: 0.3191\n",
      "Epoch [2348/4000], Loss: 0.2796\n",
      "Epoch [2349/4000], Loss: 0.3774\n",
      "Epoch [2350/4000], Loss: 0.3350\n",
      "Epoch [2351/4000], Loss: 0.2296\n",
      "Epoch [2352/4000], Loss: 0.4712\n",
      "Epoch [2353/4000], Loss: 0.5859\n",
      "Epoch [2354/4000], Loss: 0.3433\n",
      "Epoch [2355/4000], Loss: 0.4943\n",
      "Epoch [2356/4000], Loss: 0.5151\n",
      "Epoch [2357/4000], Loss: 0.4134\n",
      "Epoch [2358/4000], Loss: 0.3139\n",
      "Epoch [2359/4000], Loss: 0.3046\n",
      "Epoch [2360/4000], Loss: 0.2503\n",
      "Epoch [2361/4000], Loss: 0.3026\n",
      "Epoch [2362/4000], Loss: 0.3126\n",
      "Epoch [2363/4000], Loss: 0.5819\n",
      "Epoch [2364/4000], Loss: 0.2708\n",
      "Epoch [2365/4000], Loss: 0.2657\n",
      "Epoch [2366/4000], Loss: 0.2161\n",
      "Epoch [2367/4000], Loss: 0.2800\n",
      "Epoch [2368/4000], Loss: 0.2061\n",
      "Epoch [2369/4000], Loss: 0.5206\n",
      "Epoch [2370/4000], Loss: 0.2520\n",
      "Epoch [2371/4000], Loss: 0.2382\n",
      "Epoch [2372/4000], Loss: 0.4060\n",
      "Epoch [2373/4000], Loss: 0.4071\n",
      "Epoch [2374/4000], Loss: 0.3405\n",
      "Epoch [2375/4000], Loss: 0.4715\n",
      "Epoch [2376/4000], Loss: 0.1643\n",
      "Epoch [2377/4000], Loss: 0.3632\n",
      "Epoch [2378/4000], Loss: 0.3058\n",
      "Epoch [2379/4000], Loss: 0.2362\n",
      "Epoch [2380/4000], Loss: 0.2982\n",
      "Epoch [2381/4000], Loss: 0.3694\n",
      "Epoch [2382/4000], Loss: 0.3749\n",
      "Epoch [2383/4000], Loss: 0.2705\n",
      "Epoch [2384/4000], Loss: 0.5281\n",
      "Epoch [2385/4000], Loss: 0.2582\n",
      "Epoch [2386/4000], Loss: 0.2933\n",
      "Epoch [2387/4000], Loss: 0.3835\n",
      "Epoch [2388/4000], Loss: 0.5351\n",
      "Epoch [2389/4000], Loss: 0.3792\n",
      "Epoch [2390/4000], Loss: 0.4501\n",
      "Epoch [2391/4000], Loss: 0.2698\n",
      "Epoch [2392/4000], Loss: 0.1878\n",
      "Epoch [2393/4000], Loss: 0.3977\n",
      "Epoch [2394/4000], Loss: 0.4095\n",
      "Epoch [2395/4000], Loss: 0.5651\n",
      "Epoch [2396/4000], Loss: 0.4227\n",
      "Epoch [2397/4000], Loss: 0.2919\n",
      "Epoch [2398/4000], Loss: 0.5996\n",
      "Epoch [2399/4000], Loss: 0.4026\n",
      "Epoch [2400/4000], Loss: 0.7017\n",
      "Epoch [2401/4000], Loss: 0.2781\n",
      "Epoch [2402/4000], Loss: 0.2495\n",
      "Epoch [2403/4000], Loss: 0.2546\n",
      "Epoch [2404/4000], Loss: 0.3106\n",
      "Epoch [2405/4000], Loss: 0.3063\n",
      "Epoch [2406/4000], Loss: 0.4546\n",
      "Epoch [2407/4000], Loss: 0.4556\n",
      "Epoch [2408/4000], Loss: 0.4269\n",
      "Epoch [2409/4000], Loss: 0.3247\n",
      "Epoch [2410/4000], Loss: 0.5158\n",
      "Epoch [2411/4000], Loss: 0.5651\n",
      "Epoch [2412/4000], Loss: 0.3384\n",
      "Epoch [2413/4000], Loss: 0.3542\n",
      "Epoch [2414/4000], Loss: 0.3690\n",
      "Epoch [2415/4000], Loss: 0.3493\n",
      "Epoch [2416/4000], Loss: 0.4877\n",
      "Epoch [2417/4000], Loss: 0.4276\n",
      "Epoch [2418/4000], Loss: 0.2604\n",
      "Epoch [2419/4000], Loss: 0.4489\n",
      "Epoch [2420/4000], Loss: 0.3749\n",
      "Epoch [2421/4000], Loss: 0.3997\n",
      "Epoch [2422/4000], Loss: 0.2801\n",
      "Epoch [2423/4000], Loss: 0.5154\n",
      "Epoch [2424/4000], Loss: 0.2498\n",
      "Epoch [2425/4000], Loss: 0.3192\n",
      "Epoch [2426/4000], Loss: 0.3150\n",
      "Epoch [2427/4000], Loss: 0.2217\n",
      "Epoch [2428/4000], Loss: 0.2298\n",
      "Epoch [2429/4000], Loss: 0.4599\n",
      "Epoch [2430/4000], Loss: 0.2815\n",
      "Epoch [2431/4000], Loss: 0.2880\n",
      "Epoch [2432/4000], Loss: 0.3444\n",
      "Epoch [2433/4000], Loss: 0.2872\n",
      "Epoch [2434/4000], Loss: 0.4042\n",
      "Epoch [2435/4000], Loss: 0.2411\n",
      "Epoch [2436/4000], Loss: 0.2957\n",
      "Epoch [2437/4000], Loss: 0.3620\n",
      "Epoch [2438/4000], Loss: 0.3268\n",
      "Epoch [2439/4000], Loss: 0.2281\n",
      "Epoch [2440/4000], Loss: 0.1905\n",
      "Epoch [2441/4000], Loss: 0.5678\n",
      "Epoch [2442/4000], Loss: 0.2912\n",
      "Epoch [2443/4000], Loss: 0.5023\n",
      "Epoch [2444/4000], Loss: 0.2986\n",
      "Epoch [2445/4000], Loss: 0.3897\n",
      "Epoch [2446/4000], Loss: 0.5412\n",
      "Epoch [2447/4000], Loss: 0.4522\n",
      "Epoch [2448/4000], Loss: 0.4023\n",
      "Epoch [2449/4000], Loss: 0.5482\n",
      "Epoch [2450/4000], Loss: 0.4148\n",
      "Epoch [2451/4000], Loss: 0.4192\n",
      "Epoch [2452/4000], Loss: 0.5655\n",
      "Epoch [2453/4000], Loss: 0.3134\n",
      "Epoch [2454/4000], Loss: 0.2182\n",
      "Epoch [2455/4000], Loss: 0.5670\n",
      "Epoch [2456/4000], Loss: 0.2414\n",
      "Epoch [2457/4000], Loss: 0.2809\n",
      "Epoch [2458/4000], Loss: 0.5491\n",
      "Epoch [2459/4000], Loss: 0.2745\n",
      "Epoch [2460/4000], Loss: 0.1707\n",
      "Epoch [2461/4000], Loss: 0.3532\n",
      "Epoch [2462/4000], Loss: 0.3445\n",
      "Epoch [2463/4000], Loss: 0.4319\n",
      "Epoch [2464/4000], Loss: 0.4926\n",
      "Epoch [2465/4000], Loss: 0.2862\n",
      "Epoch [2466/4000], Loss: 0.3214\n",
      "Epoch [2467/4000], Loss: 0.4975\n",
      "Epoch [2468/4000], Loss: 0.4010\n",
      "Epoch [2469/4000], Loss: 0.3550\n",
      "Epoch [2470/4000], Loss: 0.4917\n",
      "Epoch [2471/4000], Loss: 0.2980\n",
      "Epoch [2472/4000], Loss: 0.2742\n",
      "Epoch [2473/4000], Loss: 0.3084\n",
      "Epoch [2474/4000], Loss: 0.4085\n",
      "Epoch [2475/4000], Loss: 0.2651\n",
      "Epoch [2476/4000], Loss: 0.4051\n",
      "Epoch [2477/4000], Loss: 0.2233\n",
      "Epoch [2478/4000], Loss: 0.3317\n",
      "Epoch [2479/4000], Loss: 0.4559\n",
      "Epoch [2480/4000], Loss: 0.2496\n",
      "Epoch [2481/4000], Loss: 0.3353\n",
      "Epoch [2482/4000], Loss: 0.4628\n",
      "Epoch [2483/4000], Loss: 0.2501\n",
      "Epoch [2484/4000], Loss: 0.3293\n",
      "Epoch [2485/4000], Loss: 0.3834\n",
      "Epoch [2486/4000], Loss: 0.4403\n",
      "Epoch [2487/4000], Loss: 0.4310\n",
      "Epoch [2488/4000], Loss: 0.3711\n",
      "Epoch [2489/4000], Loss: 0.3195\n",
      "Epoch [2490/4000], Loss: 0.4532\n",
      "Epoch [2491/4000], Loss: 0.2323\n",
      "Epoch [2492/4000], Loss: 0.2364\n",
      "Epoch [2493/4000], Loss: 0.3494\n",
      "Epoch [2494/4000], Loss: 0.2600\n",
      "Epoch [2495/4000], Loss: 0.3541\n",
      "Epoch [2496/4000], Loss: 0.2886\n",
      "Epoch [2497/4000], Loss: 0.2551\n",
      "Epoch [2498/4000], Loss: 0.4953\n",
      "Epoch [2499/4000], Loss: 0.3223\n",
      "Epoch [2500/4000], Loss: 0.3050\n",
      "Epoch [2501/4000], Loss: 0.3542\n",
      "Epoch [2502/4000], Loss: 0.3184\n",
      "Epoch [2503/4000], Loss: 0.1515\n",
      "Epoch [2504/4000], Loss: 0.2600\n",
      "Epoch [2505/4000], Loss: 0.3868\n",
      "Epoch [2506/4000], Loss: 0.2930\n",
      "Epoch [2507/4000], Loss: 0.4744\n",
      "Epoch [2508/4000], Loss: 0.6258\n",
      "Epoch [2509/4000], Loss: 0.3618\n",
      "Epoch [2510/4000], Loss: 0.4227\n",
      "Epoch [2511/4000], Loss: 0.3945\n",
      "Epoch [2512/4000], Loss: 0.4958\n",
      "Epoch [2513/4000], Loss: 0.2931\n",
      "Epoch [2514/4000], Loss: 0.4754\n",
      "Epoch [2515/4000], Loss: 0.2998\n",
      "Epoch [2516/4000], Loss: 0.3089\n",
      "Epoch [2517/4000], Loss: 0.5396\n",
      "Epoch [2518/4000], Loss: 0.3926\n",
      "Epoch [2519/4000], Loss: 0.2977\n",
      "Epoch [2520/4000], Loss: 0.2757\n",
      "Epoch [2521/4000], Loss: 0.3310\n",
      "Epoch [2522/4000], Loss: 0.3142\n",
      "Epoch [2523/4000], Loss: 0.3769\n",
      "Epoch [2524/4000], Loss: 0.2315\n",
      "Epoch [2525/4000], Loss: 0.2786\n",
      "Epoch [2526/4000], Loss: 0.2919\n",
      "Epoch [2527/4000], Loss: 0.3633\n",
      "Epoch [2528/4000], Loss: 0.4612\n",
      "Epoch [2529/4000], Loss: 0.2768\n",
      "Epoch [2530/4000], Loss: 0.3821\n",
      "Epoch [2531/4000], Loss: 0.3349\n",
      "Epoch [2532/4000], Loss: 0.4599\n",
      "Epoch [2533/4000], Loss: 0.3448\n",
      "Epoch [2534/4000], Loss: 0.3886\n",
      "Epoch [2535/4000], Loss: 0.2581\n",
      "Epoch [2536/4000], Loss: 0.2703\n",
      "Epoch [2537/4000], Loss: 0.2897\n",
      "Epoch [2538/4000], Loss: 0.4793\n",
      "Epoch [2539/4000], Loss: 0.2343\n",
      "Epoch [2540/4000], Loss: 0.2429\n",
      "Epoch [2541/4000], Loss: 0.3225\n",
      "Epoch [2542/4000], Loss: 0.3323\n",
      "Epoch [2543/4000], Loss: 0.4351\n",
      "Epoch [2544/4000], Loss: 0.4488\n",
      "Epoch [2545/4000], Loss: 0.3617\n",
      "Epoch [2546/4000], Loss: 0.5251\n",
      "Epoch [2547/4000], Loss: 0.7064\n",
      "Epoch [2548/4000], Loss: 0.3575\n",
      "Epoch [2549/4000], Loss: 0.5644\n",
      "Epoch [2550/4000], Loss: 0.2477\n",
      "Epoch [2551/4000], Loss: 0.2657\n",
      "Epoch [2552/4000], Loss: 0.6822\n",
      "Epoch [2553/4000], Loss: 0.3426\n",
      "Epoch [2554/4000], Loss: 0.2528\n",
      "Epoch [2555/4000], Loss: 0.2824\n",
      "Epoch [2556/4000], Loss: 0.2960\n",
      "Epoch [2557/4000], Loss: 0.4148\n",
      "Epoch [2558/4000], Loss: 0.2784\n",
      "Epoch [2559/4000], Loss: 0.3153\n",
      "Epoch [2560/4000], Loss: 0.2823\n",
      "Epoch [2561/4000], Loss: 0.2252\n",
      "Epoch [2562/4000], Loss: 0.2267\n",
      "Epoch [2563/4000], Loss: 0.2564\n",
      "Epoch [2564/4000], Loss: 0.3780\n",
      "Epoch [2565/4000], Loss: 0.3911\n",
      "Epoch [2566/4000], Loss: 0.2503\n",
      "Epoch [2567/4000], Loss: 0.3041\n",
      "Epoch [2568/4000], Loss: 0.2458\n",
      "Epoch [2569/4000], Loss: 0.3039\n",
      "Epoch [2570/4000], Loss: 0.3876\n",
      "Epoch [2571/4000], Loss: 0.2381\n",
      "Epoch [2572/4000], Loss: 0.6109\n",
      "Epoch [2573/4000], Loss: 0.2253\n",
      "Epoch [2574/4000], Loss: 0.3396\n",
      "Epoch [2575/4000], Loss: 0.3585\n",
      "Epoch [2576/4000], Loss: 0.7336\n",
      "Epoch [2577/4000], Loss: 0.4060\n",
      "Epoch [2578/4000], Loss: 0.3425\n",
      "Epoch [2579/4000], Loss: 0.2536\n",
      "Epoch [2580/4000], Loss: 0.2380\n",
      "Epoch [2581/4000], Loss: 0.3368\n",
      "Epoch [2582/4000], Loss: 0.3709\n",
      "Epoch [2583/4000], Loss: 0.3164\n",
      "Epoch [2584/4000], Loss: 0.3422\n",
      "Epoch [2585/4000], Loss: 0.4077\n",
      "Epoch [2586/4000], Loss: 0.3630\n",
      "Epoch [2587/4000], Loss: 0.4073\n",
      "Epoch [2588/4000], Loss: 0.3577\n",
      "Epoch [2589/4000], Loss: 0.3354\n",
      "Epoch [2590/4000], Loss: 0.2748\n",
      "Epoch [2591/4000], Loss: 0.4254\n",
      "Epoch [2592/4000], Loss: 0.2669\n",
      "Epoch [2593/4000], Loss: 0.3886\n",
      "Epoch [2594/4000], Loss: 0.3047\n",
      "Epoch [2595/4000], Loss: 0.4442\n",
      "Epoch [2596/4000], Loss: 0.1713\n",
      "Epoch [2597/4000], Loss: 0.3491\n",
      "Epoch [2598/4000], Loss: 0.3574\n",
      "Epoch [2599/4000], Loss: 0.2424\n",
      "Epoch [2600/4000], Loss: 0.2767\n",
      "Epoch [2601/4000], Loss: 0.2309\n",
      "Epoch [2602/4000], Loss: 0.3806\n",
      "Epoch [2603/4000], Loss: 0.2766\n",
      "Epoch [2604/4000], Loss: 0.4627\n",
      "Epoch [2605/4000], Loss: 0.3558\n",
      "Epoch [2606/4000], Loss: 0.2567\n",
      "Epoch [2607/4000], Loss: 0.4201\n",
      "Epoch [2608/4000], Loss: 0.2644\n",
      "Epoch [2609/4000], Loss: 0.4938\n",
      "Epoch [2610/4000], Loss: 0.2341\n",
      "Epoch [2611/4000], Loss: 0.3567\n",
      "Epoch [2612/4000], Loss: 0.3591\n",
      "Epoch [2613/4000], Loss: 0.2687\n",
      "Epoch [2614/4000], Loss: 0.4362\n",
      "Epoch [2615/4000], Loss: 0.3880\n",
      "Epoch [2616/4000], Loss: 0.3127\n",
      "Epoch [2617/4000], Loss: 0.2422\n",
      "Epoch [2618/4000], Loss: 0.4552\n",
      "Epoch [2619/4000], Loss: 0.3328\n",
      "Epoch [2620/4000], Loss: 0.4037\n",
      "Epoch [2621/4000], Loss: 0.3315\n",
      "Epoch [2622/4000], Loss: 0.2172\n",
      "Epoch [2623/4000], Loss: 0.5764\n",
      "Epoch [2624/4000], Loss: 0.2804\n",
      "Epoch [2625/4000], Loss: 0.6216\n",
      "Epoch [2626/4000], Loss: 0.2420\n",
      "Epoch [2627/4000], Loss: 0.4963\n",
      "Epoch [2628/4000], Loss: 0.3005\n",
      "Epoch [2629/4000], Loss: 0.2979\n",
      "Epoch [2630/4000], Loss: 0.2398\n",
      "Epoch [2631/4000], Loss: 0.3675\n",
      "Epoch [2632/4000], Loss: 0.2406\n",
      "Epoch [2633/4000], Loss: 0.3697\n",
      "Epoch [2634/4000], Loss: 0.2903\n",
      "Epoch [2635/4000], Loss: 0.4077\n",
      "Epoch [2636/4000], Loss: 0.3986\n",
      "Epoch [2637/4000], Loss: 0.2962\n",
      "Epoch [2638/4000], Loss: 0.3155\n",
      "Epoch [2639/4000], Loss: 0.3446\n",
      "Epoch [2640/4000], Loss: 0.6264\n",
      "Epoch [2641/4000], Loss: 0.2702\n",
      "Epoch [2642/4000], Loss: 0.2775\n",
      "Epoch [2643/4000], Loss: 0.4610\n",
      "Epoch [2644/4000], Loss: 0.2819\n",
      "Epoch [2645/4000], Loss: 0.2584\n",
      "Epoch [2646/4000], Loss: 0.3992\n",
      "Epoch [2647/4000], Loss: 0.3664\n",
      "Epoch [2648/4000], Loss: 0.5146\n",
      "Epoch [2649/4000], Loss: 0.3131\n",
      "Epoch [2650/4000], Loss: 0.2540\n",
      "Epoch [2651/4000], Loss: 0.4666\n",
      "Epoch [2652/4000], Loss: 0.4872\n",
      "Epoch [2653/4000], Loss: 0.3614\n",
      "Epoch [2654/4000], Loss: 0.1951\n",
      "Epoch [2655/4000], Loss: 0.2857\n",
      "Epoch [2656/4000], Loss: 0.2343\n",
      "Epoch [2657/4000], Loss: 0.3374\n",
      "Epoch [2658/4000], Loss: 0.3855\n",
      "Epoch [2659/4000], Loss: 0.4576\n",
      "Epoch [2660/4000], Loss: 0.4162\n",
      "Epoch [2661/4000], Loss: 0.3128\n",
      "Epoch [2662/4000], Loss: 0.1879\n",
      "Epoch [2663/4000], Loss: 0.5635\n",
      "Epoch [2664/4000], Loss: 0.2874\n",
      "Epoch [2665/4000], Loss: 0.2915\n",
      "Epoch [2666/4000], Loss: 0.4333\n",
      "Epoch [2667/4000], Loss: 0.5234\n",
      "Epoch [2668/4000], Loss: 0.3922\n",
      "Epoch [2669/4000], Loss: 0.2910\n",
      "Epoch [2670/4000], Loss: 0.3777\n",
      "Epoch [2671/4000], Loss: 0.2513\n",
      "Epoch [2672/4000], Loss: 0.2505\n",
      "Epoch [2673/4000], Loss: 0.3431\n",
      "Epoch [2674/4000], Loss: 0.4659\n",
      "Epoch [2675/4000], Loss: 0.3394\n",
      "Epoch [2676/4000], Loss: 0.2936\n",
      "Epoch [2677/4000], Loss: 0.2783\n",
      "Epoch [2678/4000], Loss: 0.2550\n",
      "Epoch [2679/4000], Loss: 0.3760\n",
      "Epoch [2680/4000], Loss: 0.3307\n",
      "Epoch [2681/4000], Loss: 0.2207\n",
      "Epoch [2682/4000], Loss: 0.2829\n",
      "Epoch [2683/4000], Loss: 0.3458\n",
      "Epoch [2684/4000], Loss: 0.2831\n",
      "Epoch [2685/4000], Loss: 0.2911\n",
      "Epoch [2686/4000], Loss: 0.3598\n",
      "Epoch [2687/4000], Loss: 0.3088\n",
      "Epoch [2688/4000], Loss: 0.2765\n",
      "Epoch [2689/4000], Loss: 0.2044\n",
      "Epoch [2690/4000], Loss: 0.3135\n",
      "Epoch [2691/4000], Loss: 0.1809\n",
      "Epoch [2692/4000], Loss: 0.3547\n",
      "Epoch [2693/4000], Loss: 0.2936\n",
      "Epoch [2694/4000], Loss: 0.3237\n",
      "Epoch [2695/4000], Loss: 0.2309\n",
      "Epoch [2696/4000], Loss: 0.3419\n",
      "Epoch [2697/4000], Loss: 0.2070\n",
      "Epoch [2698/4000], Loss: 0.2978\n",
      "Epoch [2699/4000], Loss: 0.2354\n",
      "Epoch [2700/4000], Loss: 0.3671\n",
      "Epoch [2701/4000], Loss: 0.5218\n",
      "Epoch [2702/4000], Loss: 0.3195\n",
      "Epoch [2703/4000], Loss: 0.3758\n",
      "Epoch [2704/4000], Loss: 0.2456\n",
      "Epoch [2705/4000], Loss: 0.6297\n",
      "Epoch [2706/4000], Loss: 0.2463\n",
      "Epoch [2707/4000], Loss: 0.2609\n",
      "Epoch [2708/4000], Loss: 0.3000\n",
      "Epoch [2709/4000], Loss: 0.2566\n",
      "Epoch [2710/4000], Loss: 0.2870\n",
      "Epoch [2711/4000], Loss: 0.2618\n",
      "Epoch [2712/4000], Loss: 0.2153\n",
      "Epoch [2713/4000], Loss: 0.2238\n",
      "Epoch [2714/4000], Loss: 0.3119\n",
      "Epoch [2715/4000], Loss: 0.4305\n",
      "Epoch [2716/4000], Loss: 0.4198\n",
      "Epoch [2717/4000], Loss: 0.2435\n",
      "Epoch [2718/4000], Loss: 0.4565\n",
      "Epoch [2719/4000], Loss: 0.3707\n",
      "Epoch [2720/4000], Loss: 0.6028\n",
      "Epoch [2721/4000], Loss: 0.2884\n",
      "Epoch [2722/4000], Loss: 0.3489\n",
      "Epoch [2723/4000], Loss: 0.2824\n",
      "Epoch [2724/4000], Loss: 0.3374\n",
      "Epoch [2725/4000], Loss: 0.3074\n",
      "Epoch [2726/4000], Loss: 0.4219\n",
      "Epoch [2727/4000], Loss: 0.2719\n",
      "Epoch [2728/4000], Loss: 0.3145\n",
      "Epoch [2729/4000], Loss: 0.2527\n",
      "Epoch [2730/4000], Loss: 0.1962\n",
      "Epoch [2731/4000], Loss: 0.2302\n",
      "Epoch [2732/4000], Loss: 0.5164\n",
      "Epoch [2733/4000], Loss: 0.2576\n",
      "Epoch [2734/4000], Loss: 0.4097\n",
      "Epoch [2735/4000], Loss: 0.2781\n",
      "Epoch [2736/4000], Loss: 0.2826\n",
      "Epoch [2737/4000], Loss: 0.2828\n",
      "Epoch [2738/4000], Loss: 0.3500\n",
      "Epoch [2739/4000], Loss: 0.3527\n",
      "Epoch [2740/4000], Loss: 0.3097\n",
      "Epoch [2741/4000], Loss: 0.1986\n",
      "Epoch [2742/4000], Loss: 0.4925\n",
      "Epoch [2743/4000], Loss: 0.4375\n",
      "Epoch [2744/4000], Loss: 0.3238\n",
      "Epoch [2745/4000], Loss: 0.2109\n",
      "Epoch [2746/4000], Loss: 0.1913\n",
      "Epoch [2747/4000], Loss: 0.4058\n",
      "Epoch [2748/4000], Loss: 0.6195\n",
      "Epoch [2749/4000], Loss: 0.3254\n",
      "Epoch [2750/4000], Loss: 0.3401\n",
      "Epoch [2751/4000], Loss: 0.2095\n",
      "Epoch [2752/4000], Loss: 0.1923\n",
      "Epoch [2753/4000], Loss: 0.2418\n",
      "Epoch [2754/4000], Loss: 0.3104\n",
      "Epoch [2755/4000], Loss: 0.5642\n",
      "Epoch [2756/4000], Loss: 0.2300\n",
      "Epoch [2757/4000], Loss: 0.2234\n",
      "Epoch [2758/4000], Loss: 0.2488\n",
      "Epoch [2759/4000], Loss: 0.1710\n",
      "Epoch [2760/4000], Loss: 0.1791\n",
      "Epoch [2761/4000], Loss: 0.3096\n",
      "Epoch [2762/4000], Loss: 0.2704\n",
      "Epoch [2763/4000], Loss: 0.1437\n",
      "Epoch [2764/4000], Loss: 0.3492\n",
      "Epoch [2765/4000], Loss: 0.2587\n",
      "Epoch [2766/4000], Loss: 0.2582\n",
      "Epoch [2767/4000], Loss: 0.3193\n",
      "Epoch [2768/4000], Loss: 0.1999\n",
      "Epoch [2769/4000], Loss: 0.1451\n",
      "Epoch [2770/4000], Loss: 0.3298\n",
      "Epoch [2771/4000], Loss: 0.3174\n",
      "Epoch [2772/4000], Loss: 0.3165\n",
      "Epoch [2773/4000], Loss: 0.1786\n",
      "Epoch [2774/4000], Loss: 0.4143\n",
      "Epoch [2775/4000], Loss: 0.3893\n",
      "Epoch [2776/4000], Loss: 0.2511\n",
      "Epoch [2777/4000], Loss: 0.2766\n",
      "Epoch [2778/4000], Loss: 0.2921\n",
      "Epoch [2779/4000], Loss: 0.2159\n",
      "Epoch [2780/4000], Loss: 0.3133\n",
      "Epoch [2781/4000], Loss: 0.6280\n",
      "Epoch [2782/4000], Loss: 0.3024\n",
      "Epoch [2783/4000], Loss: 0.2200\n",
      "Epoch [2784/4000], Loss: 0.3210\n",
      "Epoch [2785/4000], Loss: 0.3432\n",
      "Epoch [2786/4000], Loss: 0.3848\n",
      "Epoch [2787/4000], Loss: 0.2468\n",
      "Epoch [2788/4000], Loss: 0.2734\n",
      "Epoch [2789/4000], Loss: 0.4157\n",
      "Epoch [2790/4000], Loss: 0.2627\n",
      "Epoch [2791/4000], Loss: 0.2128\n",
      "Epoch [2792/4000], Loss: 0.2904\n",
      "Epoch [2793/4000], Loss: 0.2913\n",
      "Epoch [2794/4000], Loss: 0.4113\n",
      "Epoch [2795/4000], Loss: 0.3079\n",
      "Epoch [2796/4000], Loss: 0.4741\n",
      "Epoch [2797/4000], Loss: 0.2163\n",
      "Epoch [2798/4000], Loss: 0.2414\n",
      "Epoch [2799/4000], Loss: 0.4303\n",
      "Epoch [2800/4000], Loss: 0.4208\n",
      "Epoch [2801/4000], Loss: 0.1874\n",
      "Epoch [2802/4000], Loss: 0.3728\n",
      "Epoch [2803/4000], Loss: 0.3770\n",
      "Epoch [2804/4000], Loss: 0.2824\n",
      "Epoch [2805/4000], Loss: 0.2204\n",
      "Epoch [2806/4000], Loss: 0.4860\n",
      "Epoch [2807/4000], Loss: 0.3107\n",
      "Epoch [2808/4000], Loss: 0.2964\n",
      "Epoch [2809/4000], Loss: 0.3001\n",
      "Epoch [2810/4000], Loss: 0.2574\n",
      "Epoch [2811/4000], Loss: 0.3916\n",
      "Epoch [2812/4000], Loss: 0.2749\n",
      "Epoch [2813/4000], Loss: 0.2798\n",
      "Epoch [2814/4000], Loss: 0.2306\n",
      "Epoch [2815/4000], Loss: 0.3240\n",
      "Epoch [2816/4000], Loss: 0.3446\n",
      "Epoch [2817/4000], Loss: 0.3033\n",
      "Epoch [2818/4000], Loss: 0.2970\n",
      "Epoch [2819/4000], Loss: 0.2644\n",
      "Epoch [2820/4000], Loss: 0.2435\n",
      "Epoch [2821/4000], Loss: 0.4787\n",
      "Epoch [2822/4000], Loss: 0.5199\n",
      "Epoch [2823/4000], Loss: 0.2322\n",
      "Epoch [2824/4000], Loss: 0.2426\n",
      "Epoch [2825/4000], Loss: 0.3203\n",
      "Epoch [2826/4000], Loss: 0.1777\n",
      "Epoch [2827/4000], Loss: 0.1730\n",
      "Epoch [2828/4000], Loss: 0.3032\n",
      "Epoch [2829/4000], Loss: 0.2868\n",
      "Epoch [2830/4000], Loss: 0.3300\n",
      "Epoch [2831/4000], Loss: 0.4633\n",
      "Epoch [2832/4000], Loss: 0.3268\n",
      "Epoch [2833/4000], Loss: 0.4221\n",
      "Epoch [2834/4000], Loss: 0.3961\n",
      "Epoch [2835/4000], Loss: 0.3213\n",
      "Epoch [2836/4000], Loss: 0.3481\n",
      "Epoch [2837/4000], Loss: 0.1999\n",
      "Epoch [2838/4000], Loss: 0.3511\n",
      "Epoch [2839/4000], Loss: 0.4343\n",
      "Epoch [2840/4000], Loss: 0.3439\n",
      "Epoch [2841/4000], Loss: 0.1774\n",
      "Epoch [2842/4000], Loss: 0.5201\n",
      "Epoch [2843/4000], Loss: 0.4123\n",
      "Epoch [2844/4000], Loss: 0.2540\n",
      "Epoch [2845/4000], Loss: 0.2833\n",
      "Epoch [2846/4000], Loss: 0.2263\n",
      "Epoch [2847/4000], Loss: 0.2783\n",
      "Epoch [2848/4000], Loss: 0.4881\n",
      "Epoch [2849/4000], Loss: 0.3137\n",
      "Epoch [2850/4000], Loss: 0.4904\n",
      "Epoch [2851/4000], Loss: 0.2230\n",
      "Epoch [2852/4000], Loss: 0.3259\n",
      "Epoch [2853/4000], Loss: 0.4078\n",
      "Epoch [2854/4000], Loss: 0.2119\n",
      "Epoch [2855/4000], Loss: 0.2325\n",
      "Epoch [2856/4000], Loss: 0.4904\n",
      "Epoch [2857/4000], Loss: 0.1660\n",
      "Epoch [2858/4000], Loss: 0.1414\n",
      "Epoch [2859/4000], Loss: 0.1536\n",
      "Epoch [2860/4000], Loss: 0.4311\n",
      "Epoch [2861/4000], Loss: 0.2419\n",
      "Epoch [2862/4000], Loss: 0.2931\n",
      "Epoch [2863/4000], Loss: 0.2740\n",
      "Epoch [2864/4000], Loss: 0.4244\n",
      "Epoch [2865/4000], Loss: 0.2994\n",
      "Epoch [2866/4000], Loss: 0.2615\n",
      "Epoch [2867/4000], Loss: 0.3778\n",
      "Epoch [2868/4000], Loss: 0.3058\n",
      "Epoch [2869/4000], Loss: 0.2498\n",
      "Epoch [2870/4000], Loss: 0.4088\n",
      "Epoch [2871/4000], Loss: 0.2762\n",
      "Epoch [2872/4000], Loss: 0.4253\n",
      "Epoch [2873/4000], Loss: 0.2564\n",
      "Epoch [2874/4000], Loss: 0.2326\n",
      "Epoch [2875/4000], Loss: 0.1992\n",
      "Epoch [2876/4000], Loss: 0.3337\n",
      "Epoch [2877/4000], Loss: 0.2205\n",
      "Epoch [2878/4000], Loss: 0.2871\n",
      "Epoch [2879/4000], Loss: 0.2270\n",
      "Epoch [2880/4000], Loss: 0.2685\n",
      "Epoch [2881/4000], Loss: 0.2672\n",
      "Epoch [2882/4000], Loss: 0.2497\n",
      "Epoch [2883/4000], Loss: 0.3196\n",
      "Epoch [2884/4000], Loss: 0.1952\n",
      "Epoch [2885/4000], Loss: 0.3427\n",
      "Epoch [2886/4000], Loss: 0.2552\n",
      "Epoch [2887/4000], Loss: 0.3130\n",
      "Epoch [2888/4000], Loss: 0.2497\n",
      "Epoch [2889/4000], Loss: 0.2357\n",
      "Epoch [2890/4000], Loss: 0.2597\n",
      "Epoch [2891/4000], Loss: 0.3431\n",
      "Epoch [2892/4000], Loss: 0.3371\n",
      "Epoch [2893/4000], Loss: 0.4011\n",
      "Epoch [2894/4000], Loss: 0.3378\n",
      "Epoch [2895/4000], Loss: 0.1626\n",
      "Epoch [2896/4000], Loss: 0.4714\n",
      "Epoch [2897/4000], Loss: 0.2239\n",
      "Epoch [2898/4000], Loss: 0.2399\n",
      "Epoch [2899/4000], Loss: 0.1514\n",
      "Epoch [2900/4000], Loss: 0.2490\n",
      "Epoch [2901/4000], Loss: 0.2581\n",
      "Epoch [2902/4000], Loss: 0.2925\n",
      "Epoch [2903/4000], Loss: 0.3245\n",
      "Epoch [2904/4000], Loss: 0.2481\n",
      "Epoch [2905/4000], Loss: 0.1498\n",
      "Epoch [2906/4000], Loss: 0.3525\n",
      "Epoch [2907/4000], Loss: 0.2192\n",
      "Epoch [2908/4000], Loss: 0.1943\n",
      "Epoch [2909/4000], Loss: 0.3049\n",
      "Epoch [2910/4000], Loss: 0.2965\n",
      "Epoch [2911/4000], Loss: 0.3290\n",
      "Epoch [2912/4000], Loss: 0.2472\n",
      "Epoch [2913/4000], Loss: 0.3955\n",
      "Epoch [2914/4000], Loss: 0.5401\n",
      "Epoch [2915/4000], Loss: 0.2647\n",
      "Epoch [2916/4000], Loss: 0.3617\n",
      "Epoch [2917/4000], Loss: 0.2408\n",
      "Epoch [2918/4000], Loss: 0.2122\n",
      "Epoch [2919/4000], Loss: 0.4124\n",
      "Epoch [2920/4000], Loss: 0.1693\n",
      "Epoch [2921/4000], Loss: 0.4457\n",
      "Epoch [2922/4000], Loss: 0.1689\n",
      "Epoch [2923/4000], Loss: 0.3220\n",
      "Epoch [2924/4000], Loss: 0.2624\n",
      "Epoch [2925/4000], Loss: 0.3396\n",
      "Epoch [2926/4000], Loss: 0.3075\n",
      "Epoch [2927/4000], Loss: 0.3141\n",
      "Epoch [2928/4000], Loss: 0.2762\n",
      "Epoch [2929/4000], Loss: 0.1980\n",
      "Epoch [2930/4000], Loss: 0.3875\n",
      "Epoch [2931/4000], Loss: 0.1974\n",
      "Epoch [2932/4000], Loss: 0.2076\n",
      "Epoch [2933/4000], Loss: 0.3544\n",
      "Epoch [2934/4000], Loss: 0.2521\n",
      "Epoch [2935/4000], Loss: 0.1879\n",
      "Epoch [2936/4000], Loss: 0.3354\n",
      "Epoch [2937/4000], Loss: 0.2454\n",
      "Epoch [2938/4000], Loss: 0.3495\n",
      "Epoch [2939/4000], Loss: 0.3314\n",
      "Epoch [2940/4000], Loss: 0.6168\n",
      "Epoch [2941/4000], Loss: 0.1691\n",
      "Epoch [2942/4000], Loss: 0.4207\n",
      "Epoch [2943/4000], Loss: 0.3654\n",
      "Epoch [2944/4000], Loss: 0.2405\n",
      "Epoch [2945/4000], Loss: 0.3457\n",
      "Epoch [2946/4000], Loss: 0.2850\n",
      "Epoch [2947/4000], Loss: 0.2876\n",
      "Epoch [2948/4000], Loss: 0.3527\n",
      "Epoch [2949/4000], Loss: 0.5409\n",
      "Epoch [2950/4000], Loss: 0.3644\n",
      "Epoch [2951/4000], Loss: 0.2360\n",
      "Epoch [2952/4000], Loss: 0.2682\n",
      "Epoch [2953/4000], Loss: 0.2905\n",
      "Epoch [2954/4000], Loss: 0.3370\n",
      "Epoch [2955/4000], Loss: 0.2673\n",
      "Epoch [2956/4000], Loss: 0.6006\n",
      "Epoch [2957/4000], Loss: 0.1512\n",
      "Epoch [2958/4000], Loss: 0.2424\n",
      "Epoch [2959/4000], Loss: 0.2628\n",
      "Epoch [2960/4000], Loss: 0.4243\n",
      "Epoch [2961/4000], Loss: 0.2261\n",
      "Epoch [2962/4000], Loss: 0.2808\n",
      "Epoch [2963/4000], Loss: 0.1674\n",
      "Epoch [2964/4000], Loss: 0.2989\n",
      "Epoch [2965/4000], Loss: 0.3164\n",
      "Epoch [2966/4000], Loss: 0.3134\n",
      "Epoch [2967/4000], Loss: 0.1895\n",
      "Epoch [2968/4000], Loss: 0.2230\n",
      "Epoch [2969/4000], Loss: 0.4915\n",
      "Epoch [2970/4000], Loss: 0.4779\n",
      "Epoch [2971/4000], Loss: 0.1931\n",
      "Epoch [2972/4000], Loss: 0.3530\n",
      "Epoch [2973/4000], Loss: 0.2395\n",
      "Epoch [2974/4000], Loss: 0.2248\n",
      "Epoch [2975/4000], Loss: 0.2937\n",
      "Epoch [2976/4000], Loss: 0.3136\n",
      "Epoch [2977/4000], Loss: 0.3119\n",
      "Epoch [2978/4000], Loss: 0.3377\n",
      "Epoch [2979/4000], Loss: 0.1978\n",
      "Epoch [2980/4000], Loss: 0.3082\n",
      "Epoch [2981/4000], Loss: 0.2349\n",
      "Epoch [2982/4000], Loss: 0.1870\n",
      "Epoch [2983/4000], Loss: 0.2878\n",
      "Epoch [2984/4000], Loss: 0.1215\n",
      "Epoch [2985/4000], Loss: 0.2326\n",
      "Epoch [2986/4000], Loss: 0.1635\n",
      "Epoch [2987/4000], Loss: 0.1737\n",
      "Epoch [2988/4000], Loss: 0.3066\n",
      "Epoch [2989/4000], Loss: 0.1584\n",
      "Epoch [2990/4000], Loss: 0.2180\n",
      "Epoch [2991/4000], Loss: 0.2317\n",
      "Epoch [2992/4000], Loss: 0.1825\n",
      "Epoch [2993/4000], Loss: 0.2776\n",
      "Epoch [2994/4000], Loss: 0.1450\n",
      "Epoch [2995/4000], Loss: 0.2706\n",
      "Epoch [2996/4000], Loss: 0.3276\n",
      "Epoch [2997/4000], Loss: 0.2117\n",
      "Epoch [2998/4000], Loss: 0.2336\n",
      "Epoch [2999/4000], Loss: 0.2037\n",
      "Epoch [3000/4000], Loss: 0.1833\n",
      "Epoch [3001/4000], Loss: 0.2161\n",
      "Epoch [3002/4000], Loss: 0.3414\n",
      "Epoch [3003/4000], Loss: 0.4351\n",
      "Epoch [3004/4000], Loss: 0.4711\n",
      "Epoch [3005/4000], Loss: 0.2131\n",
      "Epoch [3006/4000], Loss: 0.2052\n",
      "Epoch [3007/4000], Loss: 0.3214\n",
      "Epoch [3008/4000], Loss: 0.2772\n",
      "Epoch [3009/4000], Loss: 0.2805\n",
      "Epoch [3010/4000], Loss: 0.3723\n",
      "Epoch [3011/4000], Loss: 0.2487\n",
      "Epoch [3012/4000], Loss: 0.1246\n",
      "Epoch [3013/4000], Loss: 0.3218\n",
      "Epoch [3014/4000], Loss: 0.2212\n",
      "Epoch [3015/4000], Loss: 0.3953\n",
      "Epoch [3016/4000], Loss: 0.2059\n",
      "Epoch [3017/4000], Loss: 0.2845\n",
      "Epoch [3018/4000], Loss: 0.2142\n",
      "Epoch [3019/4000], Loss: 0.2894\n",
      "Epoch [3020/4000], Loss: 0.3337\n",
      "Epoch [3021/4000], Loss: 0.2287\n",
      "Epoch [3022/4000], Loss: 0.4216\n",
      "Epoch [3023/4000], Loss: 0.2815\n",
      "Epoch [3024/4000], Loss: 0.4013\n",
      "Epoch [3025/4000], Loss: 0.5201\n",
      "Epoch [3026/4000], Loss: 0.3238\n",
      "Epoch [3027/4000], Loss: 0.2174\n",
      "Epoch [3028/4000], Loss: 0.3648\n",
      "Epoch [3029/4000], Loss: 0.2970\n",
      "Epoch [3030/4000], Loss: 0.2599\n",
      "Epoch [3031/4000], Loss: 0.1384\n",
      "Epoch [3032/4000], Loss: 0.2651\n",
      "Epoch [3033/4000], Loss: 0.2321\n",
      "Epoch [3034/4000], Loss: 0.3873\n",
      "Epoch [3035/4000], Loss: 0.2699\n",
      "Epoch [3036/4000], Loss: 0.2243\n",
      "Epoch [3037/4000], Loss: 0.3409\n",
      "Epoch [3038/4000], Loss: 0.4393\n",
      "Epoch [3039/4000], Loss: 0.3226\n",
      "Epoch [3040/4000], Loss: 0.2389\n",
      "Epoch [3041/4000], Loss: 0.1946\n",
      "Epoch [3042/4000], Loss: 0.4654\n",
      "Epoch [3043/4000], Loss: 0.2906\n",
      "Epoch [3044/4000], Loss: 0.2483\n",
      "Epoch [3045/4000], Loss: 0.2181\n",
      "Epoch [3046/4000], Loss: 0.2575\n",
      "Epoch [3047/4000], Loss: 0.2324\n",
      "Epoch [3048/4000], Loss: 0.1520\n",
      "Epoch [3049/4000], Loss: 0.2699\n",
      "Epoch [3050/4000], Loss: 0.1683\n",
      "Epoch [3051/4000], Loss: 0.2272\n",
      "Epoch [3052/4000], Loss: 0.1605\n",
      "Epoch [3053/4000], Loss: 0.3215\n",
      "Epoch [3054/4000], Loss: 0.1958\n",
      "Epoch [3055/4000], Loss: 0.2935\n",
      "Epoch [3056/4000], Loss: 0.1394\n",
      "Epoch [3057/4000], Loss: 0.2052\n",
      "Epoch [3058/4000], Loss: 0.1848\n",
      "Epoch [3059/4000], Loss: 0.2837\n",
      "Epoch [3060/4000], Loss: 0.3383\n",
      "Epoch [3061/4000], Loss: 0.2564\n",
      "Epoch [3062/4000], Loss: 0.3540\n",
      "Epoch [3063/4000], Loss: 0.3195\n",
      "Epoch [3064/4000], Loss: 0.4728\n",
      "Epoch [3065/4000], Loss: 0.3766\n",
      "Epoch [3066/4000], Loss: 0.4303\n",
      "Epoch [3067/4000], Loss: 0.2609\n",
      "Epoch [3068/4000], Loss: 0.2435\n",
      "Epoch [3069/4000], Loss: 0.3300\n",
      "Epoch [3070/4000], Loss: 0.2475\n",
      "Epoch [3071/4000], Loss: 0.3584\n",
      "Epoch [3072/4000], Loss: 0.1851\n",
      "Epoch [3073/4000], Loss: 0.2446\n",
      "Epoch [3074/4000], Loss: 0.2146\n",
      "Epoch [3075/4000], Loss: 0.1962\n",
      "Epoch [3076/4000], Loss: 0.1940\n",
      "Epoch [3077/4000], Loss: 0.4238\n",
      "Epoch [3078/4000], Loss: 0.1786\n",
      "Epoch [3079/4000], Loss: 0.2122\n",
      "Epoch [3080/4000], Loss: 0.4714\n",
      "Epoch [3081/4000], Loss: 0.2122\n",
      "Epoch [3082/4000], Loss: 0.2956\n",
      "Epoch [3083/4000], Loss: 0.2177\n",
      "Epoch [3084/4000], Loss: 0.1692\n",
      "Epoch [3085/4000], Loss: 0.2184\n",
      "Epoch [3086/4000], Loss: 0.2801\n",
      "Epoch [3087/4000], Loss: 0.2211\n",
      "Epoch [3088/4000], Loss: 0.2322\n",
      "Epoch [3089/4000], Loss: 0.3321\n",
      "Epoch [3090/4000], Loss: 0.3312\n",
      "Epoch [3091/4000], Loss: 0.2608\n",
      "Epoch [3092/4000], Loss: 0.2885\n",
      "Epoch [3093/4000], Loss: 0.2422\n",
      "Epoch [3094/4000], Loss: 0.2959\n",
      "Epoch [3095/4000], Loss: 0.3907\n",
      "Epoch [3096/4000], Loss: 0.2997\n",
      "Epoch [3097/4000], Loss: 0.2774\n",
      "Epoch [3098/4000], Loss: 0.2365\n",
      "Epoch [3099/4000], Loss: 0.2939\n",
      "Epoch [3100/4000], Loss: 0.1427\n",
      "Epoch [3101/4000], Loss: 0.2555\n",
      "Epoch [3102/4000], Loss: 0.2502\n",
      "Epoch [3103/4000], Loss: 0.1726\n",
      "Epoch [3104/4000], Loss: 0.2228\n",
      "Epoch [3105/4000], Loss: 0.3109\n",
      "Epoch [3106/4000], Loss: 0.3211\n",
      "Epoch [3107/4000], Loss: 0.2664\n",
      "Epoch [3108/4000], Loss: 0.1769\n",
      "Epoch [3109/4000], Loss: 0.2330\n",
      "Epoch [3110/4000], Loss: 0.2410\n",
      "Epoch [3111/4000], Loss: 0.2375\n",
      "Epoch [3112/4000], Loss: 0.2882\n",
      "Epoch [3113/4000], Loss: 0.1957\n",
      "Epoch [3114/4000], Loss: 0.2287\n",
      "Epoch [3115/4000], Loss: 0.1690\n",
      "Epoch [3116/4000], Loss: 0.2212\n",
      "Epoch [3117/4000], Loss: 0.2484\n",
      "Epoch [3118/4000], Loss: 0.2017\n",
      "Epoch [3119/4000], Loss: 0.4063\n",
      "Epoch [3120/4000], Loss: 0.3394\n",
      "Epoch [3121/4000], Loss: 0.1397\n",
      "Epoch [3122/4000], Loss: 0.2654\n",
      "Epoch [3123/4000], Loss: 0.1938\n",
      "Epoch [3124/4000], Loss: 0.2592\n",
      "Epoch [3125/4000], Loss: 0.3554\n",
      "Epoch [3126/4000], Loss: 0.1640\n",
      "Epoch [3127/4000], Loss: 0.2959\n",
      "Epoch [3128/4000], Loss: 0.2077\n",
      "Epoch [3129/4000], Loss: 0.2521\n",
      "Epoch [3130/4000], Loss: 0.1989\n",
      "Epoch [3131/4000], Loss: 0.1764\n",
      "Epoch [3132/4000], Loss: 0.2110\n",
      "Epoch [3133/4000], Loss: 0.2023\n",
      "Epoch [3134/4000], Loss: 0.1727\n",
      "Epoch [3135/4000], Loss: 0.2578\n",
      "Epoch [3136/4000], Loss: 0.3458\n",
      "Epoch [3137/4000], Loss: 0.2460\n",
      "Epoch [3138/4000], Loss: 0.3316\n",
      "Epoch [3139/4000], Loss: 0.2313\n",
      "Epoch [3140/4000], Loss: 0.1920\n",
      "Epoch [3141/4000], Loss: 0.2991\n",
      "Epoch [3142/4000], Loss: 0.2144\n",
      "Epoch [3143/4000], Loss: 0.1664\n",
      "Epoch [3144/4000], Loss: 0.2338\n",
      "Epoch [3145/4000], Loss: 0.2418\n",
      "Epoch [3146/4000], Loss: 0.2506\n",
      "Epoch [3147/4000], Loss: 0.2915\n",
      "Epoch [3148/4000], Loss: 0.3801\n",
      "Epoch [3149/4000], Loss: 0.3048\n",
      "Epoch [3150/4000], Loss: 0.2719\n",
      "Epoch [3151/4000], Loss: 0.2102\n",
      "Epoch [3152/4000], Loss: 0.2005\n",
      "Epoch [3153/4000], Loss: 0.1196\n",
      "Epoch [3154/4000], Loss: 0.2379\n",
      "Epoch [3155/4000], Loss: 0.3763\n",
      "Epoch [3156/4000], Loss: 0.1638\n",
      "Epoch [3157/4000], Loss: 0.1994\n",
      "Epoch [3158/4000], Loss: 0.2420\n",
      "Epoch [3159/4000], Loss: 0.2200\n",
      "Epoch [3160/4000], Loss: 0.2707\n",
      "Epoch [3161/4000], Loss: 0.3336\n",
      "Epoch [3162/4000], Loss: 0.3749\n",
      "Epoch [3163/4000], Loss: 0.2991\n",
      "Epoch [3164/4000], Loss: 0.2923\n",
      "Epoch [3165/4000], Loss: 0.2479\n",
      "Epoch [3166/4000], Loss: 0.1702\n",
      "Epoch [3167/4000], Loss: 0.2493\n",
      "Epoch [3168/4000], Loss: 0.1932\n",
      "Epoch [3169/4000], Loss: 0.3175\n",
      "Epoch [3170/4000], Loss: 0.2152\n",
      "Epoch [3171/4000], Loss: 0.1304\n",
      "Epoch [3172/4000], Loss: 0.2931\n",
      "Epoch [3173/4000], Loss: 0.1997\n",
      "Epoch [3174/4000], Loss: 0.2080\n",
      "Epoch [3175/4000], Loss: 0.1923\n",
      "Epoch [3176/4000], Loss: 0.2684\n",
      "Epoch [3177/4000], Loss: 0.2670\n",
      "Epoch [3178/4000], Loss: 0.4307\n",
      "Epoch [3179/4000], Loss: 0.2053\n",
      "Epoch [3180/4000], Loss: 0.1628\n",
      "Epoch [3181/4000], Loss: 0.1958\n",
      "Epoch [3182/4000], Loss: 0.2012\n",
      "Epoch [3183/4000], Loss: 0.2356\n",
      "Epoch [3184/4000], Loss: 0.3378\n",
      "Epoch [3185/4000], Loss: 0.2918\n",
      "Epoch [3186/4000], Loss: 0.2791\n",
      "Epoch [3187/4000], Loss: 0.1673\n",
      "Epoch [3188/4000], Loss: 0.1314\n",
      "Epoch [3189/4000], Loss: 0.3200\n",
      "Epoch [3190/4000], Loss: 0.2041\n",
      "Epoch [3191/4000], Loss: 0.4056\n",
      "Epoch [3192/4000], Loss: 0.1656\n",
      "Epoch [3193/4000], Loss: 0.3271\n",
      "Epoch [3194/4000], Loss: 0.2473\n",
      "Epoch [3195/4000], Loss: 0.2227\n",
      "Epoch [3196/4000], Loss: 0.2508\n",
      "Epoch [3197/4000], Loss: 0.2321\n",
      "Epoch [3198/4000], Loss: 0.2594\n",
      "Epoch [3199/4000], Loss: 0.2741\n",
      "Epoch [3200/4000], Loss: 0.2026\n",
      "Epoch [3201/4000], Loss: 0.2902\n",
      "Epoch [3202/4000], Loss: 0.4486\n",
      "Epoch [3203/4000], Loss: 0.2768\n",
      "Epoch [3204/4000], Loss: 0.5371\n",
      "Epoch [3205/4000], Loss: 0.2029\n",
      "Epoch [3206/4000], Loss: 0.1318\n",
      "Epoch [3207/4000], Loss: 0.3512\n",
      "Epoch [3208/4000], Loss: 0.2243\n",
      "Epoch [3209/4000], Loss: 0.3147\n",
      "Epoch [3210/4000], Loss: 0.5643\n",
      "Epoch [3211/4000], Loss: 0.2643\n",
      "Epoch [3212/4000], Loss: 0.2762\n",
      "Epoch [3213/4000], Loss: 0.2875\n",
      "Epoch [3214/4000], Loss: 0.2943\n",
      "Epoch [3215/4000], Loss: 0.3133\n",
      "Epoch [3216/4000], Loss: 0.2663\n",
      "Epoch [3217/4000], Loss: 0.2799\n",
      "Epoch [3218/4000], Loss: 0.2386\n",
      "Epoch [3219/4000], Loss: 0.2091\n",
      "Epoch [3220/4000], Loss: 0.2017\n",
      "Epoch [3221/4000], Loss: 0.3440\n",
      "Epoch [3222/4000], Loss: 0.2944\n",
      "Epoch [3223/4000], Loss: 0.2575\n",
      "Epoch [3224/4000], Loss: 0.2670\n",
      "Epoch [3225/4000], Loss: 0.1577\n",
      "Epoch [3226/4000], Loss: 0.4663\n",
      "Epoch [3227/4000], Loss: 0.2321\n",
      "Epoch [3228/4000], Loss: 0.2548\n",
      "Epoch [3229/4000], Loss: 0.2158\n",
      "Epoch [3230/4000], Loss: 0.3922\n",
      "Epoch [3231/4000], Loss: 0.2698\n",
      "Epoch [3232/4000], Loss: 0.1976\n",
      "Epoch [3233/4000], Loss: 0.2924\n",
      "Epoch [3234/4000], Loss: 0.4220\n",
      "Epoch [3235/4000], Loss: 0.2669\n",
      "Epoch [3236/4000], Loss: 0.2150\n",
      "Epoch [3237/4000], Loss: 0.2962\n",
      "Epoch [3238/4000], Loss: 0.1928\n",
      "Epoch [3239/4000], Loss: 0.2550\n",
      "Epoch [3240/4000], Loss: 0.2632\n",
      "Epoch [3241/4000], Loss: 0.3231\n",
      "Epoch [3242/4000], Loss: 0.3705\n",
      "Epoch [3243/4000], Loss: 0.2559\n",
      "Epoch [3244/4000], Loss: 0.2710\n",
      "Epoch [3245/4000], Loss: 0.2637\n",
      "Epoch [3246/4000], Loss: 0.2792\n",
      "Epoch [3247/4000], Loss: 0.1645\n",
      "Epoch [3248/4000], Loss: 0.2757\n",
      "Epoch [3249/4000], Loss: 0.3445\n",
      "Epoch [3250/4000], Loss: 0.2325\n",
      "Epoch [3251/4000], Loss: 0.2321\n",
      "Epoch [3252/4000], Loss: 0.2960\n",
      "Epoch [3253/4000], Loss: 0.3783\n",
      "Epoch [3254/4000], Loss: 0.3324\n",
      "Epoch [3255/4000], Loss: 0.2007\n",
      "Epoch [3256/4000], Loss: 0.2041\n",
      "Epoch [3257/4000], Loss: 0.1844\n",
      "Epoch [3258/4000], Loss: 0.2914\n",
      "Epoch [3259/4000], Loss: 0.3215\n",
      "Epoch [3260/4000], Loss: 0.3356\n",
      "Epoch [3261/4000], Loss: 0.6294\n",
      "Epoch [3262/4000], Loss: 0.2080\n",
      "Epoch [3263/4000], Loss: 0.2476\n",
      "Epoch [3264/4000], Loss: 0.3201\n",
      "Epoch [3265/4000], Loss: 0.3307\n",
      "Epoch [3266/4000], Loss: 0.3239\n",
      "Epoch [3267/4000], Loss: 0.1711\n",
      "Epoch [3268/4000], Loss: 0.0958\n",
      "Epoch [3269/4000], Loss: 0.2087\n",
      "Epoch [3270/4000], Loss: 0.1212\n",
      "Epoch [3271/4000], Loss: 0.3483\n",
      "Epoch [3272/4000], Loss: 0.2400\n",
      "Epoch [3273/4000], Loss: 0.2039\n",
      "Epoch [3274/4000], Loss: 0.1611\n",
      "Epoch [3275/4000], Loss: 0.1622\n",
      "Epoch [3276/4000], Loss: 0.1749\n",
      "Epoch [3277/4000], Loss: 0.2827\n",
      "Epoch [3278/4000], Loss: 0.3332\n",
      "Epoch [3279/4000], Loss: 0.2242\n",
      "Epoch [3280/4000], Loss: 0.1871\n",
      "Epoch [3281/4000], Loss: 0.2940\n",
      "Epoch [3282/4000], Loss: 0.1584\n",
      "Epoch [3283/4000], Loss: 0.2122\n",
      "Epoch [3284/4000], Loss: 0.2328\n",
      "Epoch [3285/4000], Loss: 0.3602\n",
      "Epoch [3286/4000], Loss: 0.2051\n",
      "Epoch [3287/4000], Loss: 0.3399\n",
      "Epoch [3288/4000], Loss: 0.3704\n",
      "Epoch [3289/4000], Loss: 0.2272\n",
      "Epoch [3290/4000], Loss: 0.3332\n",
      "Epoch [3291/4000], Loss: 0.3705\n",
      "Epoch [3292/4000], Loss: 0.2222\n",
      "Epoch [3293/4000], Loss: 0.2001\n",
      "Epoch [3294/4000], Loss: 0.1670\n",
      "Epoch [3295/4000], Loss: 0.3878\n",
      "Epoch [3296/4000], Loss: 0.3130\n",
      "Epoch [3297/4000], Loss: 0.2661\n",
      "Epoch [3298/4000], Loss: 0.2081\n",
      "Epoch [3299/4000], Loss: 0.2087\n",
      "Epoch [3300/4000], Loss: 0.3928\n",
      "Epoch [3301/4000], Loss: 0.3708\n",
      "Epoch [3302/4000], Loss: 0.3967\n",
      "Epoch [3303/4000], Loss: 0.1791\n",
      "Epoch [3304/4000], Loss: 0.3308\n",
      "Epoch [3305/4000], Loss: 0.1283\n",
      "Epoch [3306/4000], Loss: 0.1727\n",
      "Epoch [3307/4000], Loss: 0.3326\n",
      "Epoch [3308/4000], Loss: 0.2558\n",
      "Epoch [3309/4000], Loss: 0.1925\n",
      "Epoch [3310/4000], Loss: 0.3730\n",
      "Epoch [3311/4000], Loss: 0.3310\n",
      "Epoch [3312/4000], Loss: 0.2614\n",
      "Epoch [3313/4000], Loss: 0.3217\n",
      "Epoch [3314/4000], Loss: 0.2553\n",
      "Epoch [3315/4000], Loss: 0.2343\n",
      "Epoch [3316/4000], Loss: 0.3617\n",
      "Epoch [3317/4000], Loss: 0.2534\n",
      "Epoch [3318/4000], Loss: 0.1993\n",
      "Epoch [3319/4000], Loss: 0.2521\n",
      "Epoch [3320/4000], Loss: 0.3237\n",
      "Epoch [3321/4000], Loss: 0.2318\n",
      "Epoch [3322/4000], Loss: 0.2655\n",
      "Epoch [3323/4000], Loss: 0.3364\n",
      "Epoch [3324/4000], Loss: 0.2741\n",
      "Epoch [3325/4000], Loss: 0.1849\n",
      "Epoch [3326/4000], Loss: 0.2454\n",
      "Epoch [3327/4000], Loss: 0.1923\n",
      "Epoch [3328/4000], Loss: 0.4600\n",
      "Epoch [3329/4000], Loss: 0.1651\n",
      "Epoch [3330/4000], Loss: 0.3497\n",
      "Epoch [3331/4000], Loss: 0.3054\n",
      "Epoch [3332/4000], Loss: 0.3059\n",
      "Epoch [3333/4000], Loss: 0.2823\n",
      "Epoch [3334/4000], Loss: 0.1763\n",
      "Epoch [3335/4000], Loss: 0.2372\n",
      "Epoch [3336/4000], Loss: 0.2267\n",
      "Epoch [3337/4000], Loss: 0.1682\n",
      "Epoch [3338/4000], Loss: 0.1179\n",
      "Epoch [3339/4000], Loss: 0.3004\n",
      "Epoch [3340/4000], Loss: 0.1738\n",
      "Epoch [3341/4000], Loss: 0.2729\n",
      "Epoch [3342/4000], Loss: 0.2804\n",
      "Epoch [3343/4000], Loss: 0.3010\n",
      "Epoch [3344/4000], Loss: 0.1243\n",
      "Epoch [3345/4000], Loss: 0.2793\n",
      "Epoch [3346/4000], Loss: 0.2931\n",
      "Epoch [3347/4000], Loss: 0.2306\n",
      "Epoch [3348/4000], Loss: 0.2488\n",
      "Epoch [3349/4000], Loss: 0.2424\n",
      "Epoch [3350/4000], Loss: 0.4438\n",
      "Epoch [3351/4000], Loss: 0.1846\n",
      "Epoch [3352/4000], Loss: 0.2185\n",
      "Epoch [3353/4000], Loss: 0.2711\n",
      "Epoch [3354/4000], Loss: 0.3674\n",
      "Epoch [3355/4000], Loss: 0.1577\n",
      "Epoch [3356/4000], Loss: 0.2065\n",
      "Epoch [3357/4000], Loss: 0.3423\n",
      "Epoch [3358/4000], Loss: 0.1692\n",
      "Epoch [3359/4000], Loss: 0.2814\n",
      "Epoch [3360/4000], Loss: 0.1503\n",
      "Epoch [3361/4000], Loss: 0.4020\n",
      "Epoch [3362/4000], Loss: 0.1959\n",
      "Epoch [3363/4000], Loss: 0.1419\n",
      "Epoch [3364/4000], Loss: 0.4500\n",
      "Epoch [3365/4000], Loss: 0.3489\n",
      "Epoch [3366/4000], Loss: 0.2679\n",
      "Epoch [3367/4000], Loss: 0.2941\n",
      "Epoch [3368/4000], Loss: 0.2400\n",
      "Epoch [3369/4000], Loss: 0.2800\n",
      "Epoch [3370/4000], Loss: 0.3954\n",
      "Epoch [3371/4000], Loss: 0.2451\n",
      "Epoch [3372/4000], Loss: 0.3118\n",
      "Epoch [3373/4000], Loss: 0.1351\n",
      "Epoch [3374/4000], Loss: 0.1106\n",
      "Epoch [3375/4000], Loss: 0.1349\n",
      "Epoch [3376/4000], Loss: 0.2003\n",
      "Epoch [3377/4000], Loss: 0.2075\n",
      "Epoch [3378/4000], Loss: 0.3052\n",
      "Epoch [3379/4000], Loss: 0.2411\n",
      "Epoch [3380/4000], Loss: 0.3957\n",
      "Epoch [3381/4000], Loss: 0.4303\n",
      "Epoch [3382/4000], Loss: 0.4212\n",
      "Epoch [3383/4000], Loss: 0.2385\n",
      "Epoch [3384/4000], Loss: 0.2441\n",
      "Epoch [3385/4000], Loss: 0.1827\n",
      "Epoch [3386/4000], Loss: 0.2356\n",
      "Epoch [3387/4000], Loss: 0.1858\n",
      "Epoch [3388/4000], Loss: 0.1137\n",
      "Epoch [3389/4000], Loss: 0.2271\n",
      "Epoch [3390/4000], Loss: 0.5183\n",
      "Epoch [3391/4000], Loss: 0.2305\n",
      "Epoch [3392/4000], Loss: 0.1971\n",
      "Epoch [3393/4000], Loss: 0.1409\n",
      "Epoch [3394/4000], Loss: 0.2308\n",
      "Epoch [3395/4000], Loss: 0.1624\n",
      "Epoch [3396/4000], Loss: 0.1654\n",
      "Epoch [3397/4000], Loss: 0.6440\n",
      "Epoch [3398/4000], Loss: 0.1845\n",
      "Epoch [3399/4000], Loss: 0.2798\n",
      "Epoch [3400/4000], Loss: 0.3487\n",
      "Epoch [3401/4000], Loss: 0.2228\n",
      "Epoch [3402/4000], Loss: 0.3247\n",
      "Epoch [3403/4000], Loss: 0.2668\n",
      "Epoch [3404/4000], Loss: 0.3315\n",
      "Epoch [3405/4000], Loss: 0.2319\n",
      "Epoch [3406/4000], Loss: 0.1668\n",
      "Epoch [3407/4000], Loss: 0.2363\n",
      "Epoch [3408/4000], Loss: 0.2124\n",
      "Epoch [3409/4000], Loss: 0.1126\n",
      "Epoch [3410/4000], Loss: 0.3077\n",
      "Epoch [3411/4000], Loss: 0.2315\n",
      "Epoch [3412/4000], Loss: 0.2583\n",
      "Epoch [3413/4000], Loss: 0.2946\n",
      "Epoch [3414/4000], Loss: 0.2168\n",
      "Epoch [3415/4000], Loss: 0.3449\n",
      "Epoch [3416/4000], Loss: 0.2857\n",
      "Epoch [3417/4000], Loss: 0.1538\n",
      "Epoch [3418/4000], Loss: 0.1236\n",
      "Epoch [3419/4000], Loss: 0.2949\n",
      "Epoch [3420/4000], Loss: 0.2691\n",
      "Epoch [3421/4000], Loss: 0.1940\n",
      "Epoch [3422/4000], Loss: 0.3724\n",
      "Epoch [3423/4000], Loss: 0.3397\n",
      "Epoch [3424/4000], Loss: 0.2351\n",
      "Epoch [3425/4000], Loss: 0.2779\n",
      "Epoch [3426/4000], Loss: 0.1827\n",
      "Epoch [3427/4000], Loss: 0.2426\n",
      "Epoch [3428/4000], Loss: 0.2262\n",
      "Epoch [3429/4000], Loss: 0.1443\n",
      "Epoch [3430/4000], Loss: 0.3747\n",
      "Epoch [3431/4000], Loss: 0.3146\n",
      "Epoch [3432/4000], Loss: 0.1357\n",
      "Epoch [3433/4000], Loss: 0.2372\n",
      "Epoch [3434/4000], Loss: 0.3731\n",
      "Epoch [3435/4000], Loss: 0.3110\n",
      "Epoch [3436/4000], Loss: 0.2809\n",
      "Epoch [3437/4000], Loss: 0.2923\n",
      "Epoch [3438/4000], Loss: 0.3275\n",
      "Epoch [3439/4000], Loss: 0.1673\n",
      "Epoch [3440/4000], Loss: 0.1099\n",
      "Epoch [3441/4000], Loss: 0.2517\n",
      "Epoch [3442/4000], Loss: 0.2859\n",
      "Epoch [3443/4000], Loss: 0.2902\n",
      "Epoch [3444/4000], Loss: 0.3130\n",
      "Epoch [3445/4000], Loss: 0.1808\n",
      "Epoch [3446/4000], Loss: 0.1957\n",
      "Epoch [3447/4000], Loss: 0.1523\n",
      "Epoch [3448/4000], Loss: 0.1640\n",
      "Epoch [3449/4000], Loss: 0.2278\n",
      "Epoch [3450/4000], Loss: 0.1797\n",
      "Epoch [3451/4000], Loss: 0.2331\n",
      "Epoch [3452/4000], Loss: 0.2795\n",
      "Epoch [3453/4000], Loss: 0.3487\n",
      "Epoch [3454/4000], Loss: 0.3242\n",
      "Epoch [3455/4000], Loss: 0.2724\n",
      "Epoch [3456/4000], Loss: 0.2076\n",
      "Epoch [3457/4000], Loss: 0.3166\n",
      "Epoch [3458/4000], Loss: 0.1729\n",
      "Epoch [3459/4000], Loss: 0.2884\n",
      "Epoch [3460/4000], Loss: 0.3457\n",
      "Epoch [3461/4000], Loss: 0.2781\n",
      "Epoch [3462/4000], Loss: 0.1771\n",
      "Epoch [3463/4000], Loss: 0.2508\n",
      "Epoch [3464/4000], Loss: 0.1986\n",
      "Epoch [3465/4000], Loss: 0.2661\n",
      "Epoch [3466/4000], Loss: 0.2174\n",
      "Epoch [3467/4000], Loss: 0.3965\n",
      "Epoch [3468/4000], Loss: 0.2217\n",
      "Epoch [3469/4000], Loss: 0.3810\n",
      "Epoch [3470/4000], Loss: 0.2788\n",
      "Epoch [3471/4000], Loss: 0.2308\n",
      "Epoch [3472/4000], Loss: 0.2815\n",
      "Epoch [3473/4000], Loss: 0.1484\n",
      "Epoch [3474/4000], Loss: 0.2992\n",
      "Epoch [3475/4000], Loss: 0.2088\n",
      "Epoch [3476/4000], Loss: 0.1406\n",
      "Epoch [3477/4000], Loss: 0.1835\n",
      "Epoch [3478/4000], Loss: 0.2046\n",
      "Epoch [3479/4000], Loss: 0.2219\n",
      "Epoch [3480/4000], Loss: 0.2559\n",
      "Epoch [3481/4000], Loss: 0.2498\n",
      "Epoch [3482/4000], Loss: 0.3570\n",
      "Epoch [3483/4000], Loss: 0.2373\n",
      "Epoch [3484/4000], Loss: 0.1972\n",
      "Epoch [3485/4000], Loss: 0.1414\n",
      "Epoch [3486/4000], Loss: 0.2027\n",
      "Epoch [3487/4000], Loss: 0.2906\n",
      "Epoch [3488/4000], Loss: 0.3092\n",
      "Epoch [3489/4000], Loss: 0.1661\n",
      "Epoch [3490/4000], Loss: 0.1641\n",
      "Epoch [3491/4000], Loss: 0.1256\n",
      "Epoch [3492/4000], Loss: 0.2955\n",
      "Epoch [3493/4000], Loss: 0.2017\n",
      "Epoch [3494/4000], Loss: 0.1651\n",
      "Epoch [3495/4000], Loss: 0.3343\n",
      "Epoch [3496/4000], Loss: 0.2867\n",
      "Epoch [3497/4000], Loss: 0.1502\n",
      "Epoch [3498/4000], Loss: 0.1878\n",
      "Epoch [3499/4000], Loss: 0.5110\n",
      "Epoch [3500/4000], Loss: 0.1394\n",
      "Epoch [3501/4000], Loss: 0.4593\n",
      "Epoch [3502/4000], Loss: 0.2493\n",
      "Epoch [3503/4000], Loss: 0.2932\n",
      "Epoch [3504/4000], Loss: 0.3531\n",
      "Epoch [3505/4000], Loss: 0.2641\n",
      "Epoch [3506/4000], Loss: 0.2268\n",
      "Epoch [3507/4000], Loss: 0.2462\n",
      "Epoch [3508/4000], Loss: 0.1830\n",
      "Epoch [3509/4000], Loss: 0.2470\n",
      "Epoch [3510/4000], Loss: 0.2667\n",
      "Epoch [3511/4000], Loss: 0.3084\n",
      "Epoch [3512/4000], Loss: 0.2359\n",
      "Epoch [3513/4000], Loss: 0.1919\n",
      "Epoch [3514/4000], Loss: 0.1872\n",
      "Epoch [3515/4000], Loss: 0.2027\n",
      "Epoch [3516/4000], Loss: 0.2062\n",
      "Epoch [3517/4000], Loss: 0.2952\n",
      "Epoch [3518/4000], Loss: 0.2651\n",
      "Epoch [3519/4000], Loss: 0.2020\n",
      "Epoch [3520/4000], Loss: 0.2443\n",
      "Epoch [3521/4000], Loss: 0.3037\n",
      "Epoch [3522/4000], Loss: 0.1042\n",
      "Epoch [3523/4000], Loss: 0.2218\n",
      "Epoch [3524/4000], Loss: 0.2578\n",
      "Epoch [3525/4000], Loss: 0.1333\n",
      "Epoch [3526/4000], Loss: 0.2371\n",
      "Epoch [3527/4000], Loss: 0.3165\n",
      "Epoch [3528/4000], Loss: 0.1695\n",
      "Epoch [3529/4000], Loss: 0.3107\n",
      "Epoch [3530/4000], Loss: 0.2226\n",
      "Epoch [3531/4000], Loss: 0.2617\n",
      "Epoch [3532/4000], Loss: 0.1504\n",
      "Epoch [3533/4000], Loss: 0.1280\n",
      "Epoch [3534/4000], Loss: 0.1916\n",
      "Epoch [3535/4000], Loss: 0.1538\n",
      "Epoch [3536/4000], Loss: 0.3180\n",
      "Epoch [3537/4000], Loss: 0.1993\n",
      "Epoch [3538/4000], Loss: 0.1135\n",
      "Epoch [3539/4000], Loss: 0.2011\n",
      "Epoch [3540/4000], Loss: 0.1425\n",
      "Epoch [3541/4000], Loss: 0.2448\n",
      "Epoch [3542/4000], Loss: 0.2013\n",
      "Epoch [3543/4000], Loss: 0.1295\n",
      "Epoch [3544/4000], Loss: 0.2115\n",
      "Epoch [3545/4000], Loss: 0.2250\n",
      "Epoch [3546/4000], Loss: 0.1496\n",
      "Epoch [3547/4000], Loss: 0.1694\n",
      "Epoch [3548/4000], Loss: 0.2177\n",
      "Epoch [3549/4000], Loss: 0.1644\n",
      "Epoch [3550/4000], Loss: 0.2366\n",
      "Epoch [3551/4000], Loss: 0.1686\n",
      "Epoch [3552/4000], Loss: 0.3714\n",
      "Epoch [3553/4000], Loss: 0.2688\n",
      "Epoch [3554/4000], Loss: 0.2522\n",
      "Epoch [3555/4000], Loss: 0.3480\n",
      "Epoch [3556/4000], Loss: 0.2035\n",
      "Epoch [3557/4000], Loss: 0.1920\n",
      "Epoch [3558/4000], Loss: 0.3093\n",
      "Epoch [3559/4000], Loss: 0.4107\n",
      "Epoch [3560/4000], Loss: 0.2610\n",
      "Epoch [3561/4000], Loss: 0.1178\n",
      "Epoch [3562/4000], Loss: 0.2065\n",
      "Epoch [3563/4000], Loss: 0.1462\n",
      "Epoch [3564/4000], Loss: 0.3669\n",
      "Epoch [3565/4000], Loss: 0.1959\n",
      "Epoch [3566/4000], Loss: 0.2461\n",
      "Epoch [3567/4000], Loss: 0.3328\n",
      "Epoch [3568/4000], Loss: 0.2908\n",
      "Epoch [3569/4000], Loss: 0.1825\n",
      "Epoch [3570/4000], Loss: 0.3555\n",
      "Epoch [3571/4000], Loss: 0.2042\n",
      "Epoch [3572/4000], Loss: 0.3104\n",
      "Epoch [3573/4000], Loss: 0.3218\n",
      "Epoch [3574/4000], Loss: 0.1129\n",
      "Epoch [3575/4000], Loss: 0.3460\n",
      "Epoch [3576/4000], Loss: 0.1711\n",
      "Epoch [3577/4000], Loss: 0.2838\n",
      "Epoch [3578/4000], Loss: 0.1484\n",
      "Epoch [3579/4000], Loss: 0.2244\n",
      "Epoch [3580/4000], Loss: 0.1392\n",
      "Epoch [3581/4000], Loss: 0.1303\n",
      "Epoch [3582/4000], Loss: 0.1521\n",
      "Epoch [3583/4000], Loss: 0.4803\n",
      "Epoch [3584/4000], Loss: 0.2357\n",
      "Epoch [3585/4000], Loss: 0.3002\n",
      "Epoch [3586/4000], Loss: 0.1890\n",
      "Epoch [3587/4000], Loss: 0.6010\n",
      "Epoch [3588/4000], Loss: 0.1516\n",
      "Epoch [3589/4000], Loss: 0.1315\n",
      "Epoch [3590/4000], Loss: 0.1583\n",
      "Epoch [3591/4000], Loss: 0.1499\n",
      "Epoch [3592/4000], Loss: 0.2166\n",
      "Epoch [3593/4000], Loss: 0.1487\n",
      "Epoch [3594/4000], Loss: 0.1566\n",
      "Epoch [3595/4000], Loss: 0.2437\n",
      "Epoch [3596/4000], Loss: 0.2564\n",
      "Epoch [3597/4000], Loss: 0.1952\n",
      "Epoch [3598/4000], Loss: 0.1561\n",
      "Epoch [3599/4000], Loss: 0.2007\n",
      "Epoch [3600/4000], Loss: 0.2431\n",
      "Epoch [3601/4000], Loss: 0.2283\n",
      "Epoch [3602/4000], Loss: 0.1903\n",
      "Epoch [3603/4000], Loss: 0.2130\n",
      "Epoch [3604/4000], Loss: 0.3341\n",
      "Epoch [3605/4000], Loss: 0.2062\n",
      "Epoch [3606/4000], Loss: 0.1830\n",
      "Epoch [3607/4000], Loss: 0.2067\n",
      "Epoch [3608/4000], Loss: 0.3107\n",
      "Epoch [3609/4000], Loss: 0.1564\n",
      "Epoch [3610/4000], Loss: 0.3401\n",
      "Epoch [3611/4000], Loss: 0.1550\n",
      "Epoch [3612/4000], Loss: 0.1980\n",
      "Epoch [3613/4000], Loss: 0.2573\n",
      "Epoch [3614/4000], Loss: 0.3076\n",
      "Epoch [3615/4000], Loss: 0.1575\n",
      "Epoch [3616/4000], Loss: 0.2816\n",
      "Epoch [3617/4000], Loss: 0.2104\n",
      "Epoch [3618/4000], Loss: 0.1412\n",
      "Epoch [3619/4000], Loss: 0.1493\n",
      "Epoch [3620/4000], Loss: 0.2327\n",
      "Epoch [3621/4000], Loss: 0.1963\n",
      "Epoch [3622/4000], Loss: 0.5103\n",
      "Epoch [3623/4000], Loss: 0.3026\n",
      "Epoch [3624/4000], Loss: 0.3112\n",
      "Epoch [3625/4000], Loss: 0.1850\n",
      "Epoch [3626/4000], Loss: 0.1751\n",
      "Epoch [3627/4000], Loss: 0.3197\n",
      "Epoch [3628/4000], Loss: 0.1828\n",
      "Epoch [3629/4000], Loss: 0.2816\n",
      "Epoch [3630/4000], Loss: 0.2185\n",
      "Epoch [3631/4000], Loss: 0.1944\n",
      "Epoch [3632/4000], Loss: 0.1901\n",
      "Epoch [3633/4000], Loss: 0.1396\n",
      "Epoch [3634/4000], Loss: 0.1877\n",
      "Epoch [3635/4000], Loss: 0.1975\n",
      "Epoch [3636/4000], Loss: 0.1403\n",
      "Epoch [3637/4000], Loss: 0.1390\n",
      "Epoch [3638/4000], Loss: 0.2081\n",
      "Epoch [3639/4000], Loss: 0.1532\n",
      "Epoch [3640/4000], Loss: 0.1910\n",
      "Epoch [3641/4000], Loss: 0.1699\n",
      "Epoch [3642/4000], Loss: 0.1932\n",
      "Epoch [3643/4000], Loss: 0.1685\n",
      "Epoch [3644/4000], Loss: 0.1664\n",
      "Epoch [3645/4000], Loss: 0.1992\n",
      "Epoch [3646/4000], Loss: 0.1790\n",
      "Epoch [3647/4000], Loss: 0.2329\n",
      "Epoch [3648/4000], Loss: 0.3147\n",
      "Epoch [3649/4000], Loss: 0.2351\n",
      "Epoch [3650/4000], Loss: 0.2540\n",
      "Epoch [3651/4000], Loss: 0.1655\n",
      "Epoch [3652/4000], Loss: 0.2225\n",
      "Epoch [3653/4000], Loss: 0.2562\n",
      "Epoch [3654/4000], Loss: 0.1622\n",
      "Epoch [3655/4000], Loss: 0.2440\n",
      "Epoch [3656/4000], Loss: 0.2141\n",
      "Epoch [3657/4000], Loss: 0.1794\n",
      "Epoch [3658/4000], Loss: 0.1641\n",
      "Epoch [3659/4000], Loss: 0.2567\n",
      "Epoch [3660/4000], Loss: 0.2776\n",
      "Epoch [3661/4000], Loss: 0.1895\n",
      "Epoch [3662/4000], Loss: 0.2079\n",
      "Epoch [3663/4000], Loss: 0.2926\n",
      "Epoch [3664/4000], Loss: 0.4135\n",
      "Epoch [3665/4000], Loss: 0.1880\n",
      "Epoch [3666/4000], Loss: 0.2178\n",
      "Epoch [3667/4000], Loss: 0.0839\n",
      "Epoch [3668/4000], Loss: 0.2600\n",
      "Epoch [3669/4000], Loss: 0.1419\n",
      "Epoch [3670/4000], Loss: 0.1348\n",
      "Epoch [3671/4000], Loss: 0.1997\n",
      "Epoch [3672/4000], Loss: 0.2325\n",
      "Epoch [3673/4000], Loss: 0.1492\n",
      "Epoch [3674/4000], Loss: 0.2413\n",
      "Epoch [3675/4000], Loss: 0.3117\n",
      "Epoch [3676/4000], Loss: 0.2278\n",
      "Epoch [3677/4000], Loss: 0.2264\n",
      "Epoch [3678/4000], Loss: 0.1650\n",
      "Epoch [3679/4000], Loss: 0.2044\n",
      "Epoch [3680/4000], Loss: 0.1748\n",
      "Epoch [3681/4000], Loss: 0.1346\n",
      "Epoch [3682/4000], Loss: 0.4245\n",
      "Epoch [3683/4000], Loss: 0.1894\n",
      "Epoch [3684/4000], Loss: 0.2298\n",
      "Epoch [3685/4000], Loss: 0.2828\n",
      "Epoch [3686/4000], Loss: 0.1525\n",
      "Epoch [3687/4000], Loss: 0.1021\n",
      "Epoch [3688/4000], Loss: 0.3731\n",
      "Epoch [3689/4000], Loss: 0.4355\n",
      "Epoch [3690/4000], Loss: 0.1697\n",
      "Epoch [3691/4000], Loss: 0.1992\n",
      "Epoch [3692/4000], Loss: 0.1233\n",
      "Epoch [3693/4000], Loss: 0.1082\n",
      "Epoch [3694/4000], Loss: 0.1608\n",
      "Epoch [3695/4000], Loss: 0.1873\n",
      "Epoch [3696/4000], Loss: 0.2630\n",
      "Epoch [3697/4000], Loss: 0.1654\n",
      "Epoch [3698/4000], Loss: 0.1803\n",
      "Epoch [3699/4000], Loss: 0.3027\n",
      "Epoch [3700/4000], Loss: 0.4122\n",
      "Epoch [3701/4000], Loss: 0.3343\n",
      "Epoch [3702/4000], Loss: 0.0724\n",
      "Epoch [3703/4000], Loss: 0.1724\n",
      "Epoch [3704/4000], Loss: 0.1812\n",
      "Epoch [3705/4000], Loss: 0.1733\n",
      "Epoch [3706/4000], Loss: 0.1842\n",
      "Epoch [3707/4000], Loss: 0.5102\n",
      "Epoch [3708/4000], Loss: 0.2515\n",
      "Epoch [3709/4000], Loss: 0.2922\n",
      "Epoch [3710/4000], Loss: 0.2551\n",
      "Epoch [3711/4000], Loss: 0.1177\n",
      "Epoch [3712/4000], Loss: 0.2882\n",
      "Epoch [3713/4000], Loss: 0.1926\n",
      "Epoch [3714/4000], Loss: 0.1840\n",
      "Epoch [3715/4000], Loss: 0.2743\n",
      "Epoch [3716/4000], Loss: 0.1949\n",
      "Epoch [3717/4000], Loss: 0.2005\n",
      "Epoch [3718/4000], Loss: 0.1497\n",
      "Epoch [3719/4000], Loss: 0.2142\n",
      "Epoch [3720/4000], Loss: 0.2640\n",
      "Epoch [3721/4000], Loss: 0.1471\n",
      "Epoch [3722/4000], Loss: 0.3951\n",
      "Epoch [3723/4000], Loss: 0.1728\n",
      "Epoch [3724/4000], Loss: 0.3368\n",
      "Epoch [3725/4000], Loss: 0.3325\n",
      "Epoch [3726/4000], Loss: 0.1439\n",
      "Epoch [3727/4000], Loss: 0.3785\n",
      "Epoch [3728/4000], Loss: 0.2906\n",
      "Epoch [3729/4000], Loss: 0.1481\n",
      "Epoch [3730/4000], Loss: 0.1729\n",
      "Epoch [3731/4000], Loss: 0.2301\n",
      "Epoch [3732/4000], Loss: 0.2218\n",
      "Epoch [3733/4000], Loss: 0.3625\n",
      "Epoch [3734/4000], Loss: 0.1220\n",
      "Epoch [3735/4000], Loss: 0.2977\n",
      "Epoch [3736/4000], Loss: 0.1705\n",
      "Epoch [3737/4000], Loss: 0.1748\n",
      "Epoch [3738/4000], Loss: 0.2101\n",
      "Epoch [3739/4000], Loss: 0.3027\n",
      "Epoch [3740/4000], Loss: 0.1313\n",
      "Epoch [3741/4000], Loss: 0.2406\n",
      "Epoch [3742/4000], Loss: 0.2266\n",
      "Epoch [3743/4000], Loss: 0.2300\n",
      "Epoch [3744/4000], Loss: 0.2398\n",
      "Epoch [3745/4000], Loss: 0.1935\n",
      "Epoch [3746/4000], Loss: 0.1772\n",
      "Epoch [3747/4000], Loss: 0.2869\n",
      "Epoch [3748/4000], Loss: 0.3370\n",
      "Epoch [3749/4000], Loss: 0.1700\n",
      "Epoch [3750/4000], Loss: 0.1523\n",
      "Epoch [3751/4000], Loss: 0.1548\n",
      "Epoch [3752/4000], Loss: 0.2245\n",
      "Epoch [3753/4000], Loss: 0.1955\n",
      "Epoch [3754/4000], Loss: 0.0761\n",
      "Epoch [3755/4000], Loss: 0.2252\n",
      "Epoch [3756/4000], Loss: 0.1447\n",
      "Epoch [3757/4000], Loss: 0.1284\n",
      "Epoch [3758/4000], Loss: 0.1693\n",
      "Epoch [3759/4000], Loss: 0.2008\n",
      "Epoch [3760/4000], Loss: 0.3408\n",
      "Epoch [3761/4000], Loss: 0.1627\n",
      "Epoch [3762/4000], Loss: 0.1187\n",
      "Epoch [3763/4000], Loss: 0.2272\n",
      "Epoch [3764/4000], Loss: 0.3434\n",
      "Epoch [3765/4000], Loss: 0.1582\n",
      "Epoch [3766/4000], Loss: 0.1781\n",
      "Epoch [3767/4000], Loss: 0.1622\n",
      "Epoch [3768/4000], Loss: 0.3137\n",
      "Epoch [3769/4000], Loss: 0.3102\n",
      "Epoch [3770/4000], Loss: 0.2026\n",
      "Epoch [3771/4000], Loss: 0.4269\n",
      "Epoch [3772/4000], Loss: 0.1461\n",
      "Epoch [3773/4000], Loss: 0.1377\n",
      "Epoch [3774/4000], Loss: 0.2107\n",
      "Epoch [3775/4000], Loss: 0.1308\n",
      "Epoch [3776/4000], Loss: 0.1990\n",
      "Epoch [3777/4000], Loss: 0.2290\n",
      "Epoch [3778/4000], Loss: 0.2474\n",
      "Epoch [3779/4000], Loss: 0.1116\n",
      "Epoch [3780/4000], Loss: 0.1571\n",
      "Epoch [3781/4000], Loss: 0.2234\n",
      "Epoch [3782/4000], Loss: 0.3036\n",
      "Epoch [3783/4000], Loss: 0.1390\n",
      "Epoch [3784/4000], Loss: 0.2628\n",
      "Epoch [3785/4000], Loss: 0.1175\n",
      "Epoch [3786/4000], Loss: 0.3605\n",
      "Epoch [3787/4000], Loss: 0.2062\n",
      "Epoch [3788/4000], Loss: 0.1888\n",
      "Epoch [3789/4000], Loss: 0.3040\n",
      "Epoch [3790/4000], Loss: 0.1842\n",
      "Epoch [3791/4000], Loss: 0.1726\n",
      "Epoch [3792/4000], Loss: 0.2835\n",
      "Epoch [3793/4000], Loss: 0.1908\n",
      "Epoch [3794/4000], Loss: 0.2022\n",
      "Epoch [3795/4000], Loss: 0.2116\n",
      "Epoch [3796/4000], Loss: 0.2249\n",
      "Epoch [3797/4000], Loss: 0.3215\n",
      "Epoch [3798/4000], Loss: 0.2064\n",
      "Epoch [3799/4000], Loss: 0.2316\n",
      "Epoch [3800/4000], Loss: 0.3230\n",
      "Epoch [3801/4000], Loss: 0.2139\n",
      "Epoch [3802/4000], Loss: 0.1829\n",
      "Epoch [3803/4000], Loss: 0.2470\n",
      "Epoch [3804/4000], Loss: 0.3324\n",
      "Epoch [3805/4000], Loss: 0.2582\n",
      "Epoch [3806/4000], Loss: 0.2223\n",
      "Epoch [3807/4000], Loss: 0.1307\n",
      "Epoch [3808/4000], Loss: 0.3248\n",
      "Epoch [3809/4000], Loss: 0.2656\n",
      "Epoch [3810/4000], Loss: 0.2774\n",
      "Epoch [3811/4000], Loss: 0.2056\n",
      "Epoch [3812/4000], Loss: 0.1389\n",
      "Epoch [3813/4000], Loss: 0.1982\n",
      "Epoch [3814/4000], Loss: 0.2745\n",
      "Epoch [3815/4000], Loss: 0.2249\n",
      "Epoch [3816/4000], Loss: 0.1989\n",
      "Epoch [3817/4000], Loss: 0.2255\n",
      "Epoch [3818/4000], Loss: 0.2738\n",
      "Epoch [3819/4000], Loss: 0.1829\n",
      "Epoch [3820/4000], Loss: 0.2317\n",
      "Epoch [3821/4000], Loss: 0.2144\n",
      "Epoch [3822/4000], Loss: 0.4122\n",
      "Epoch [3823/4000], Loss: 0.1031\n",
      "Epoch [3824/4000], Loss: 0.2408\n",
      "Epoch [3825/4000], Loss: 0.1651\n",
      "Epoch [3826/4000], Loss: 0.2444\n",
      "Epoch [3827/4000], Loss: 0.3572\n",
      "Epoch [3828/4000], Loss: 0.4490\n",
      "Epoch [3829/4000], Loss: 0.3048\n",
      "Epoch [3830/4000], Loss: 0.1837\n",
      "Epoch [3831/4000], Loss: 0.2904\n",
      "Epoch [3832/4000], Loss: 0.2255\n",
      "Epoch [3833/4000], Loss: 0.1758\n",
      "Epoch [3834/4000], Loss: 0.2629\n",
      "Epoch [3835/4000], Loss: 0.2241\n",
      "Epoch [3836/4000], Loss: 0.0785\n",
      "Epoch [3837/4000], Loss: 0.2046\n",
      "Epoch [3838/4000], Loss: 0.1176\n",
      "Epoch [3839/4000], Loss: 0.2120\n",
      "Epoch [3840/4000], Loss: 0.1283\n",
      "Epoch [3841/4000], Loss: 0.1002\n",
      "Epoch [3842/4000], Loss: 0.3093\n",
      "Epoch [3843/4000], Loss: 0.1975\n",
      "Epoch [3844/4000], Loss: 0.1192\n",
      "Epoch [3845/4000], Loss: 0.2597\n",
      "Epoch [3846/4000], Loss: 0.1533\n",
      "Epoch [3847/4000], Loss: 0.1574\n",
      "Epoch [3848/4000], Loss: 0.1721\n",
      "Epoch [3849/4000], Loss: 0.0723\n",
      "Epoch [3850/4000], Loss: 0.1147\n",
      "Epoch [3851/4000], Loss: 0.1663\n",
      "Epoch [3852/4000], Loss: 0.1567\n",
      "Epoch [3853/4000], Loss: 0.1602\n",
      "Epoch [3854/4000], Loss: 0.2581\n",
      "Epoch [3855/4000], Loss: 0.1895\n",
      "Epoch [3856/4000], Loss: 0.2445\n",
      "Epoch [3857/4000], Loss: 0.1657\n",
      "Epoch [3858/4000], Loss: 0.2234\n",
      "Epoch [3859/4000], Loss: 0.1320\n",
      "Epoch [3860/4000], Loss: 0.3294\n",
      "Epoch [3861/4000], Loss: 0.1478\n",
      "Epoch [3862/4000], Loss: 0.1656\n",
      "Epoch [3863/4000], Loss: 0.2059\n",
      "Epoch [3864/4000], Loss: 0.1467\n",
      "Epoch [3865/4000], Loss: 0.1224\n",
      "Epoch [3866/4000], Loss: 0.4419\n",
      "Epoch [3867/4000], Loss: 0.2816\n",
      "Epoch [3868/4000], Loss: 0.3053\n",
      "Epoch [3869/4000], Loss: 0.1207\n",
      "Epoch [3870/4000], Loss: 0.2013\n",
      "Epoch [3871/4000], Loss: 0.2239\n",
      "Epoch [3872/4000], Loss: 0.2823\n",
      "Epoch [3873/4000], Loss: 0.2435\n",
      "Epoch [3874/4000], Loss: 0.1648\n",
      "Epoch [3875/4000], Loss: 0.1369\n",
      "Epoch [3876/4000], Loss: 0.1821\n",
      "Epoch [3877/4000], Loss: 0.2120\n",
      "Epoch [3878/4000], Loss: 0.1540\n",
      "Epoch [3879/4000], Loss: 0.0780\n",
      "Epoch [3880/4000], Loss: 0.1582\n",
      "Epoch [3881/4000], Loss: 0.1475\n",
      "Epoch [3882/4000], Loss: 0.2074\n",
      "Epoch [3883/4000], Loss: 0.2467\n",
      "Epoch [3884/4000], Loss: 0.1620\n",
      "Epoch [3885/4000], Loss: 0.0983\n",
      "Epoch [3886/4000], Loss: 0.2086\n",
      "Epoch [3887/4000], Loss: 0.1938\n",
      "Epoch [3888/4000], Loss: 0.2317\n",
      "Epoch [3889/4000], Loss: 0.1459\n",
      "Epoch [3890/4000], Loss: 0.1462\n",
      "Epoch [3891/4000], Loss: 0.2945\n",
      "Epoch [3892/4000], Loss: 0.2604\n",
      "Epoch [3893/4000], Loss: 0.1488\n",
      "Epoch [3894/4000], Loss: 0.2131\n",
      "Epoch [3895/4000], Loss: 0.1735\n",
      "Epoch [3896/4000], Loss: 0.0997\n",
      "Epoch [3897/4000], Loss: 0.3135\n",
      "Epoch [3898/4000], Loss: 0.2601\n",
      "Epoch [3899/4000], Loss: 0.1939\n",
      "Epoch [3900/4000], Loss: 0.1439\n",
      "Epoch [3901/4000], Loss: 0.2298\n",
      "Epoch [3902/4000], Loss: 0.2704\n",
      "Epoch [3903/4000], Loss: 0.1435\n",
      "Epoch [3904/4000], Loss: 0.2408\n",
      "Epoch [3905/4000], Loss: 0.1303\n",
      "Epoch [3906/4000], Loss: 0.1948\n",
      "Epoch [3907/4000], Loss: 0.1522\n",
      "Epoch [3908/4000], Loss: 0.1698\n",
      "Epoch [3909/4000], Loss: 0.2688\n",
      "Epoch [3910/4000], Loss: 0.1917\n",
      "Epoch [3911/4000], Loss: 0.2927\n",
      "Epoch [3912/4000], Loss: 0.1770\n",
      "Epoch [3913/4000], Loss: 0.1265\n",
      "Epoch [3914/4000], Loss: 0.2207\n",
      "Epoch [3915/4000], Loss: 0.1448\n",
      "Epoch [3916/4000], Loss: 0.1798\n",
      "Epoch [3917/4000], Loss: 0.4202\n",
      "Epoch [3918/4000], Loss: 0.2716\n",
      "Epoch [3919/4000], Loss: 0.1860\n",
      "Epoch [3920/4000], Loss: 0.2553\n",
      "Epoch [3921/4000], Loss: 0.2339\n",
      "Epoch [3922/4000], Loss: 0.4214\n",
      "Epoch [3923/4000], Loss: 0.1943\n",
      "Epoch [3924/4000], Loss: 0.1391\n",
      "Epoch [3925/4000], Loss: 0.1531\n",
      "Epoch [3926/4000], Loss: 0.1487\n",
      "Epoch [3927/4000], Loss: 0.1559\n",
      "Epoch [3928/4000], Loss: 0.1910\n",
      "Epoch [3929/4000], Loss: 0.1866\n",
      "Epoch [3930/4000], Loss: 0.1493\n",
      "Epoch [3931/4000], Loss: 0.2528\n",
      "Epoch [3932/4000], Loss: 0.3921\n",
      "Epoch [3933/4000], Loss: 0.1951\n",
      "Epoch [3934/4000], Loss: 0.2331\n",
      "Epoch [3935/4000], Loss: 0.4690\n",
      "Epoch [3936/4000], Loss: 0.1176\n",
      "Epoch [3937/4000], Loss: 0.1213\n",
      "Epoch [3938/4000], Loss: 0.2664\n",
      "Epoch [3939/4000], Loss: 0.1787\n",
      "Epoch [3940/4000], Loss: 0.2381\n",
      "Epoch [3941/4000], Loss: 0.1780\n",
      "Epoch [3942/4000], Loss: 0.1752\n",
      "Epoch [3943/4000], Loss: 0.1647\n",
      "Epoch [3944/4000], Loss: 0.1388\n",
      "Epoch [3945/4000], Loss: 0.2290\n",
      "Epoch [3946/4000], Loss: 0.2269\n",
      "Epoch [3947/4000], Loss: 0.1820\n",
      "Epoch [3948/4000], Loss: 0.1050\n",
      "Epoch [3949/4000], Loss: 0.3851\n",
      "Epoch [3950/4000], Loss: 0.1958\n",
      "Epoch [3951/4000], Loss: 0.1334\n",
      "Epoch [3952/4000], Loss: 0.1643\n",
      "Epoch [3953/4000], Loss: 0.0908\n",
      "Epoch [3954/4000], Loss: 0.1833\n",
      "Epoch [3955/4000], Loss: 0.1483\n",
      "Epoch [3956/4000], Loss: 0.1988\n",
      "Epoch [3957/4000], Loss: 0.1825\n",
      "Epoch [3958/4000], Loss: 0.1638\n",
      "Epoch [3959/4000], Loss: 0.2538\n",
      "Epoch [3960/4000], Loss: 0.1756\n",
      "Epoch [3961/4000], Loss: 0.1401\n",
      "Epoch [3962/4000], Loss: 0.2222\n",
      "Epoch [3963/4000], Loss: 0.1738\n",
      "Epoch [3964/4000], Loss: 0.1917\n",
      "Epoch [3965/4000], Loss: 0.2182\n",
      "Epoch [3966/4000], Loss: 0.3344\n",
      "Epoch [3967/4000], Loss: 0.2094\n",
      "Epoch [3968/4000], Loss: 0.1205\n",
      "Epoch [3969/4000], Loss: 0.2294\n",
      "Epoch [3970/4000], Loss: 0.1948\n",
      "Epoch [3971/4000], Loss: 0.1457\n",
      "Epoch [3972/4000], Loss: 0.3030\n",
      "Epoch [3973/4000], Loss: 0.2575\n",
      "Epoch [3974/4000], Loss: 0.0988\n",
      "Epoch [3975/4000], Loss: 0.1593\n",
      "Epoch [3976/4000], Loss: 0.1441\n",
      "Epoch [3977/4000], Loss: 0.1826\n",
      "Epoch [3978/4000], Loss: 0.2221\n",
      "Epoch [3979/4000], Loss: 0.0834\n",
      "Epoch [3980/4000], Loss: 0.1912\n",
      "Epoch [3981/4000], Loss: 0.1875\n",
      "Epoch [3982/4000], Loss: 0.1871\n",
      "Epoch [3983/4000], Loss: 0.2210\n",
      "Epoch [3984/4000], Loss: 0.2079\n",
      "Epoch [3985/4000], Loss: 0.1866\n",
      "Epoch [3986/4000], Loss: 0.1309\n",
      "Epoch [3987/4000], Loss: 0.0943\n",
      "Epoch [3988/4000], Loss: 0.2173\n",
      "Epoch [3989/4000], Loss: 0.2463\n",
      "Epoch [3990/4000], Loss: 0.2808\n",
      "Epoch [3991/4000], Loss: 0.1489\n",
      "Epoch [3992/4000], Loss: 0.2282\n",
      "Epoch [3993/4000], Loss: 0.2080\n",
      "Epoch [3994/4000], Loss: 0.2642\n",
      "Epoch [3995/4000], Loss: 0.1453\n",
      "Epoch [3996/4000], Loss: 0.1759\n",
      "Epoch [3997/4000], Loss: 0.1363\n",
      "Epoch [3998/4000], Loss: 0.1521\n",
      "Epoch [3999/4000], Loss: 0.1686\n",
      "Epoch [4000/4000], Loss: 0.3179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the Neural Network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Model parameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 64  # Number of neurons in the hidden layer\n",
    "num_classes = len(y.unique())\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = NeuralNetwork(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 4000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "test_accuracy = correct / total\n",
    "\n",
    "test_accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
